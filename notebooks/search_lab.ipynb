{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test search utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:11:47:12,588 INFO     [model_agent.py:184] Agent name=`designer`, model_details={\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"temperature\": 0.3\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized dataset wikitext-2 from C:\\ChengJunyan1\\Research\\model_discovery\\data\\wikitext-2\\tokenized\\meta-llama/Llama-2-7b-hf\\2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:11:47:13,292 INFO     [model_agent.py:184] Agent name=`debugger`, model_details={\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-12:11:47:13,554 INFO     [model_agent.py:184] Agent name=`claude`, model_details={\n",
      "    \"model_name\": \"gpt-4o\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evoname': 'test_evo_000', 'scales': '14M,31M,70M', 'selection_ratio': '0.25', 'select_method': 'random', 'design_budget': '0'}\n",
      "Evolution system initialized with scales: ['14M', '31M', '70M']\n",
      "Current scale: 0\n",
      "Budgets remaining: {'70M': 1, '31M': 4, '14M': 16}\n",
      "Checkpoint directory: C:\\ChengJunyan1\\Research\\model_discovery\\ckpt\\test_evo_000\n",
      "gpt2 tree loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:11:47:14,495 INFO     [model_agent.py:184] Agent name=`designer`, model_details={\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"temperature\": 0.3\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized dataset wikitext-2 from C:\\ChengJunyan1\\Research\\model_discovery\\data\\wikitext-2\\tokenized\\meta-llama/Llama-2-7b-hf\\2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:11:47:15,211 INFO     [model_agent.py:184] Agent name=`debugger`, model_details={\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-12:11:47:15,486 INFO     [model_agent.py:184] Agent name=`claude`, model_details={\n",
      "    \"model_name\": \"gpt-4o\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "import importlib\n",
    "import arxiv\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.utils as U\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "headers={\"X-API-KEY\": os.environ['S2_API_KEY']}\n",
    "paper_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}'\n",
    "\n",
    "\n",
    "strparams=[\n",
    "    f\"evoname=test_evo_000\",\n",
    "    \"scales=14M,31M,70M\",\n",
    "    \"selection_ratio=0.25\",\n",
    "    \"select_method=random\",\n",
    "    \"design_budget=0\",\n",
    "]\n",
    "evo_system = BuildEvolution(\n",
    "    strparams=';'.join(strparams),\n",
    "    do_cache=False,\n",
    "    # cache_type='diskcache',\n",
    ")\n",
    "\n",
    "ptree = evo_system.ptree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get paper files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedbackmem\n",
      "kangpt\n",
      "ntk\n",
      "s4pp\n",
      "srt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "refs_s2ids={}\n",
    "REF_TYPES=['Reference','ReferenceCore','ReferenceWithCode','ReferenceCoreWithTree']\n",
    "\n",
    "for i in ptree.G.nodes:\n",
    "    node=ptree.G.nodes[i]['data']\n",
    "    if node.type in REF_TYPES:\n",
    "        s2id=node.s2id\n",
    "        if s2id is not None:\n",
    "            refs_s2ids[i]=s2id\n",
    "        else:\n",
    "            print(i)\n",
    "\n",
    "refs=ptree.filter_by_type(REF_TYPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rec and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 292/292 [02:03<00:00,  2.36it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_rec_papers(s2id):\n",
    "    url=f'https://api.semanticscholar.org/recommendations/v1/papers/forpaper/{s2id}'\n",
    "    rsp=requests.get(url,headers=headers)\n",
    "    results=rsp.json()\n",
    "    if 'error' in results:\n",
    "        print(s2id)\n",
    "        return None\n",
    "    return results['recommendedPapers']\n",
    "\n",
    "recs={}\n",
    "for i in tqdm(refs_s2ids):\n",
    "    s2id=refs_s2ids[i]\n",
    "    rec=get_rec_papers(s2id)\n",
    "    if rec is not None:\n",
    "        recs[i]=rec\n",
    "    time.sleep(0.1)\n",
    "\n",
    "all_recs={}\n",
    "for i in recs:\n",
    "    for j in recs[i]:\n",
    "        all_recs[j['paperId']]=j['title']\n",
    "\n",
    "# all_recs_filtered={}\n",
    "# for i in all_recs:\n",
    "#     if i in refs_s2ids.values():\n",
    "#         continue\n",
    "#     if i in refs2_detail:\n",
    "#         continue\n",
    "#     all_recs_filtered[i]=all_recs[i]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4409/4409 [00:41<00:00, 105.56it/s]\n"
     ]
    }
   ],
   "source": [
    " def get_paper_detail(s2id):\n",
    "    params={\n",
    "        \"fields\": \"title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,authors,embedding,tldr,openAccessPdf,externalIds,references\",\n",
    "    }\n",
    "    rsp=requests.get(\n",
    "        paper_detail.format(paper_id=s2id),\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    results = rsp.json()\n",
    "    return results\n",
    "\n",
    "save_dir=U.pjoin(ptree.lib_dir,'..','tree_ext','plus')\n",
    "refsp_detail={}\n",
    "U.mkdir(save_dir)\n",
    "for s2id in tqdm(all_recs_filtered):\n",
    "    save_path=U.pjoin(save_dir,f'{s2id}.json')\n",
    "    if U.pexists(save_path):\n",
    "        refsp_detail[s2id]=U.load_json(save_path)\n",
    "        continue\n",
    "    ret=get_paper_detail(s2id)\n",
    "    if 'error' not in ret:\n",
    "        U.save_json(ret,save_path)\n",
    "    refsp_detail[s2id]=ret\n",
    "    time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4409"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(refsp_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2803\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "len(all_recs_filtered)\n",
    "\n",
    "refp_arxiv={}\n",
    "refp_pdfs={}\n",
    "for i in refsp_detail:\n",
    "    if 'openAccessPdf' in refsp_detail[i] and refsp_detail[i]['openAccessPdf'] is not None and 'arxiv' not in refsp_detail[i]['openAccessPdf']['url']:\n",
    "        url=refsp_detail[i]['openAccessPdf']\n",
    "        refp_pdfs[i]=url['url']\n",
    "        continue\n",
    "    if 'externalIds' in refsp_detail[i]:\n",
    "        if 'ArXiv' in refsp_detail[i]['externalIds']:\n",
    "            refp_arxiv[i]=refsp_detail[i]['externalIds']['ArXiv']\n",
    "            continue\n",
    "    # print(i)\n",
    "print(len(refp_arxiv))\n",
    "print(len(refp_pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/214 [00:00<00:15, 13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 1edd8a9ce55c0696beeca75c5aa315b621a77359, https://dl.acm.org/doi/pdf/10.1145/3687475\n",
      "Failed to download c738952caeb142be1d755ac6d50e7fbf5721b73b, https://www.mdpi.com/1424-8220/24/14/4733/pdf?version=1721635306\n",
      "Failed to download 6ccd721263b09e13fdc93a919dd3e94de3fec783, https://www.mdpi.com/2079-9292/13/15/2928/pdf?version=1721897144\n",
      "Failed to download a88fe2922556b2236bb53cea0240a1577d4d18f6, https://www.mdpi.com/2072-4292/16/14/2642/pdf?version=1721438473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 10/214 [00:22<08:53,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 035f2dd4f219f67b6321c48931b00f2e3edf54ff, https://www.mdpi.com/2078-2489/15/9/517/pdf?version=1724574257\n",
      "Failed to download 5c229896a16fc7b3ae040f1147982d98ee899df6, https://www.mdpi.com/2076-3417/14/15/6578/pdf?version=1722411871\n",
      "Failed to download e763616e0bc236e8b23bb389dd837ebefbad32b7, https://dl.acm.org/doi/pdf/10.1145/3689632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 12/214 [00:23<06:16,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download e95bd7109cbb35ae26be4e4e39518babc0c07d6a, http://wepub.org/index.php/TCSISR/article/download/2445/2662\n",
      "Failed to download e09bfc955fbf66e0a042ca3f921108b823093b2e, https://dl.acm.org/doi/pdf/10.1145/3689827\n",
      "Failed to download aa9cb2964a215223ff613dd2040fd05698999bee, https://dl.acm.org/doi/pdf/10.1145/3689630\n",
      "Failed to download fcfbfdf158708dd6d1cdbdf339ee5a959bd75a59, https://www.mdpi.com/2075-1680/13/8/495/pdf?version=1721814859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 19/214 [00:26<02:40,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 52753936271044aaa7472f3e174e5ae734a06877, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/mice.13315\n",
      "Failed to download 65fe512110167bfa7ee3b7029ff8269ad1cd7278, https://dl.acm.org/doi/pdf/10.1145/3690656\n",
      "Failed to download b36dcb1e162f1014e6547cade62439146c7d6e20, https://www.mdpi.com/2079-9292/13/17/3408/pdf?version=1724751455\n",
      "Failed to download 1e2c92cc982bf07369f3d4a9ef027d9606762943, https://www.mdpi.com/2076-3417/14/15/6670/pdf?version=1722405364\n",
      "Failed to download c5434fb1c8e938e223081f0e7bc0f0c69fc4c6dd, https://www.mdpi.com/2073-4395/14/9/1940/pdf?version=1724849442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 27/214 [00:30<01:25,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 1eab122244e87d8befe6c64cfa7efc40d6d37614, https://ijsrem.com/download/enhancing-image-classification-using-few-shot-learning-prototypical-networks-with-resnet-18-detection-accuracy-enhancement-and-optimization/?wpdmdl=37385&refresh=66d85cd3759931725455571\n",
      "Failed to download 53c70efd4d8211517d56086aeb47b06f2ad306d7, https://www.mdpi.com/2227-7080/12/8/126/pdf?version=1722849488\n",
      "Failed to download 4a784ae1f89564d2fa143b5c4f90a5db20c32e9a, https://publications.eai.eu/index.php/phat/article/download/5912/3294\n",
      "Failed to download 9096997c866a4983d4b09b7ad3a72977986ab407, https://www.mdpi.com/2673-9585/4/3/22/pdf?version=1721376361\n",
      "Failed to download 0eba887e07fc90cd98ce4dd688fa7e9415a8d5ea, https://www.mdpi.com/1099-4300/26/8/664/pdf?version=1722837680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 34/214 [01:20<18:00,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 09aa961280494e357c2236fec89909ef379e9e9a, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/sjos.12747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 39/214 [01:32<09:03,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 2f6806ed1406fac5b1cd27b2cf932eb08cd87292, https://dl.acm.org/doi/pdf/10.1145/3690645\n",
      "Failed to download d890dbf3502df2b73e7403abca6b1585a2a7bdd4, https://dl.acm.org/doi/pdf/10.1145/3679017\n",
      "Failed to download a06eacdd4d9838ac582cf3fe56bdf260f62f0abb, https://dl.acm.org/doi/pdf/10.1145/3673038.3673124\n",
      "Failed to download aac5822ac79a755e794731da8cd49c58ca2f5736, https://dl.acm.org/doi/pdf/10.1145/3673038.3673070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 41/214 [01:33<06:25,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 632b3f650650d0f190d0c81356a35979545cb8d3, https://www.e3s-conferences.org/articles/e3sconf/pdf/2024/79/e3sconf_transsiberia2024_08016.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 42/214 [01:38<07:53,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 32aef4c8b3da240cbff6446ea206b3813eff0358, https://www.mdpi.com/1424-8220/24/14/4639/pdf?version=1721289848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 44/214 [01:51<11:38,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 0d13c8a337792729ebfe16500b4d55a365e191f8, https://dl.acm.org/doi/pdf/10.1145/3688804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 46/214 [01:54<08:44,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 1cbb94346e6508ea3a13416e654af43226bbbea2, https://www.mdpi.com/2079-9292/13/15/3046/pdf?version=1722510369\n",
      "Failed to download fe610f9d81ad1389fca5c3c7e7bd0ec9065c01aa, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/acm2.14483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 52/214 [01:56<03:26,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6960131d4b7b2e57ba2a2ed34af2a8f15b260c97, https://dl.acm.org/doi/pdf/10.1145/3687466\n",
      "Failed to download 873ba88109ee122f8cf896a2987572c26a3b9bed, https://research.vu.nl/files/338418464/phdthesis%2012%20-%2066a2a2b708320.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 58/214 [02:07<04:13,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download db3723be8b313459307304548eae46d8a27873f2, https://www.mdpi.com/1424-8220/24/15/4801/pdf?version=1721895428\n",
      "Failed to download 31fb768ab8026eb4a194932b7efcb26b3d49a41a, https://www.mdpi.com/2227-7390/12/15/2389/pdf?version=1722592644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 61/214 [02:08<02:11,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download e37e0c3bdb28ff1f41c64b74eaf4853739b564d3, http://wepub.org/index.php/TCSISR/article/download/2380/2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 68/214 [02:14<01:24,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 2f021c9c693ae7730c2926a731802810724c0299, https://www.mdpi.com/2076-3417/14/14/6375/pdf?version=1721720114\n",
      "Failed to download e4ddecce10221789d0059dfed4fdfa902964fc43, https://www.mdpi.com/2076-3417/14/14/6336/pdf?version=1721632443\n",
      "Failed to download 11d84028ae644acc2b89af8821bf0f4518203459, https://www.mdpi.com/2079-9292/13/15/3064/pdf?version=1722586382\n",
      "Failed to download 3cd9b1863afa95c4e59f4d2110aa49d908a2d7cd, https://www.mdpi.com/2313-0105/10/8/286/pdf?version=1723274277\n",
      "Failed to download 417faaee3deb596d0726ff4da06e7c264755bc07, https://www.mdpi.com/2076-3417/14/14/6143/pdf?version=1721092478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 76/214 [02:25<02:28,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c50217f86bd42f8dc058f69c5922746651c158de, https://www.mdpi.com/2072-4292/16/16/2896/pdf?version=1723104706\n",
      "Failed to download f25d65db222c9049172ffd1b0b684258cac8449d, https://www.mdpi.com/2073-8994/16/8/952/pdf?version=1721901631\n",
      "Failed to download 294208c05c233dbfcd8f0a5c45c8b8667232d18e, https://dl.acm.org/doi/pdf/10.1145/3637528.3671952\n",
      "Failed to download db973c40cc48f79c5115881fed9de55955830474, https://dl.acm.org/doi/pdf/10.1145/3673038.3673043\n",
      "Failed to download 89308a693e882c7415b9ff85e5c2ba1b47b1e18c, https://www.mdpi.com/2079-9292/13/16/3240/pdf?version=1723714034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 79/214 [02:25<01:30,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download db459cc7e3037d961ee4a86e37778b23fcd86a18, https://dl.acm.org/doi/pdf/10.1145/3689334\n",
      "Failed to download b200fdb766ad40041683c26f0e2f9a4722c2ec9b, https://dl.acm.org/doi/pdf/10.1145/3677178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 81/214 [02:27<01:37,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c94eaffe50d59799a4aed509237787e41672d919, https://dl.acm.org/doi/pdf/10.1145/3680549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 83/214 [02:31<02:29,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 391b54ad4f935fae5561ed1be18520c6b5541313, https://dl.acm.org/doi/pdf/10.1145/3687483\n",
      "Failed to download 9ae42e8903eb46a440bbac5af11ac4b36a1cc7cc, https://dl.acm.org/doi/pdf/10.1145/3687239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 90/214 [02:41<02:32,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 55ae666a39f3a812d79514a4ad22858fd505cf02, https://journals.sagepub.com/doi/pdf/10.1177/14759217241262972\n",
      "Failed to download 2e0e6ed6f54bb1c9a50518c9d0827eb6b3eeea90, https://www.mdpi.com/2079-9292/13/15/3047/pdf?version=1722590131\n",
      "Failed to download df8abef5d3a79df25da8495db7434b86114030b4, https://www.mdpi.com/2504-3110/8/7/433/pdf?version=1721968414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 92/214 [02:46<03:13,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 07cf34a811ecf5e9ff2ee4a0f50e8df5f3e50e5c, https://dl.acm.org/doi/pdf/10.1145/3689430\n",
      "Failed to download 1db76d3ff0c6debbb920306e05e231c348af4329, https://www.mdpi.com/2077-1312/12/9/1490/pdf?version=1724850047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 95/214 [02:47<01:53,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4dfde0c75449d36929f4fa76dd9cc6d68712fd4f, https://drpress.org/ojs/index.php/cpl/article/download/24595/24093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 100/214 [03:06<05:32,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 71c9ab948e887ef8cb0558ce1ffe6b4432e051ea, https://journal.uir.ac.id/index.php/ITJRD/article/download/14171/6736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 102/214 [03:08<03:55,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4700e8564e26182e1995739b15795dac375c20b7, https://www.science.org/doi/pdf/10.1126/sciadv.adm8470?download=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 106/214 [03:12<02:29,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 91e2f10eaa93381d61e56df2e108e41c757a14cd, https://everant.org/index.php/etj/article/download/1402/1000\n",
      "Failed to download 20cb00d45de55424fa130ccd5b359b99cefd335d, https://www.mdpi.com/2079-9292/13/16/3230/pdf?version=1723705300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 108/214 [03:14<02:15,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 320ea2670306445929e86c87319ef53a4ac0ce52, https://www.mdpi.com/2073-8994/16/7/921/pdf?version=1721473051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 113/214 [03:20<01:56,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 22abfac098c7060f01b447235f8d6fb0c1c7d363, https://www.science.org/doi/pdf/10.1126/sciadv.adk1256?download=true\n",
      "Failed to download ce00d113b476fe8c2de00d7a235a5f60f9bc2358, http://www.cell.com/article/S2405844024114892/pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 125/214 [03:49<03:41,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6204d2677cd4f31bcee25cb1add0162d8a3fdd74, https://www.mdpi.com/2227-7390/12/15/2391/pdf?version=1722498879\n",
      "Failed to download 94361cc09aff990194196b84a0c3c71ca714c590, https://dl.acm.org/doi/pdf/10.1145/3681785\n",
      "Failed to download 27f74ae5731862fb5f58f4396d161682e2dee1b1, https://dl.acm.org/doi/pdf/10.1145/3637528.3671691\n",
      "Failed to download bca174a47076708d066303dedb16f6b92da9d16c, https://dl.acm.org/doi/pdf/10.1145/3673038.3673117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 127/214 [03:50<02:32,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 860e82a1df27705b71cb850325c07df5f62b2382, https://ojs.boulibrary.com/index.php/JAIGS/article/download/200/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 128/214 [03:50<02:09,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c290c569ad441d673c3294af1729c51cf838e641, https://easychair.org/publications/open/7dmH4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 133/214 [04:00<01:42,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download df3844cde85414c134fe832e69e286887720bd6e, https://dl.acm.org/doi/pdf/10.1145/3689428\n",
      "Failed to download 588c69df5e7920db0037db76c41f933ee16c290d, https://dl.acm.org/doi/pdf/10.1145/3679018\n",
      "Failed to download 3c47c5c976507f6ebf9b32ebdb8d1e35f8983535, https://dl.acm.org/doi/pdf/10.1145/3637528.3671835\n",
      "Failed to download 3d77ee76f1d9c8c067946a8df0ee0536e60d36e1, https://dl.acm.org/doi/pdf/10.1145/3682067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 136/214 [04:05<01:56,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 363b1de707e95e0d352c656e549cec30e90fa4bd, https://www.mdpi.com/2073-8994/16/7/934/pdf?version=1721636315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 138/214 [04:07<01:36,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4995d8f1b672fa27de16b9ff8513271dc37cac7f, https://www.mdpi.com/2504-2289/8/8/87/pdf?version=1723083292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 140/214 [04:20<03:35,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 21b302436c50d25f936828092b266c1fdcad8914, https://dl.acm.org/doi/pdf/10.1145/3686803\n",
      "Failed to download 16483c084e5fecb54a152a999886ec6e2dec71b6, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cta.4192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 159/214 [04:56<01:21,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 38fc6f7f1b497517b687900ba0fd286f61770a36, https://www.mdpi.com/2071-1050/16/15/6316/pdf?version=1721811779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 161/214 [04:58<01:07,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b7524c9ba798777ccfb71b4bb17c76cfcc6b0738, https://www.mdpi.com/2227-9717/12/9/1806/pdf?version=1724655736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 163/214 [05:05<01:49,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a2af1f6e675b6b849e39c999d28c887e428a5ca7, https://www.mdpi.com/2076-3417/14/15/6648/pdf?version=1722335122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 165/214 [05:08<01:31,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download da17f7be8f1367495e32df3e1377fc2c61a0bd65, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/ese3.1823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 171/214 [05:18<00:57,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 265ba56302baba92895ad0aba80daf2abcf000fb, https://www.metrology-journal.org/articles/ijmqe/pdf/2024/01/ijmqe240002.pdf\n",
      "Failed to download b0d6d44bddb2b531eb002987b6437e99f3755d20, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/htj.23163\n",
      "Failed to download f2090890c9e7007a66c6dea284c818ab2d132391, https://www.mdpi.com/1996-1073/17/15/3668/pdf?version=1721985624\n",
      "Failed to download 2df83a73ede1ba31e6464be1f8d948270d948513, https://www.mdpi.com/2227-7390/12/15/2404/pdf?version=1722582111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 175/214 [05:22<00:38,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a35cd6399431d7c292646f062dfe47e56468e46b, https://www.mdpi.com/2076-3417/14/15/6765/pdf?version=1722595374\n",
      "Failed to download 64909b216912ef11d7f98e76c8914d2e54164e04, https://www.mdpi.com/2076-3417/14/14/6289/pdf?version=1721374752\n",
      "Failed to download 3cd7293b5e3d5af30b841f3626dcd8c18da505bd, https://dl.acm.org/doi/pdf/10.1145/3688400\n",
      "Failed to download 718978937411e4f28577f65562f3c31aa168ddf7, https://www.mdpi.com/2079-9292/13/14/2846/pdf?version=1721377608\n",
      "Failed to download 953b0c8cdfc6cf970b43e30c36350c6d04bb2915, https://dl.acm.org/doi/pdf/10.1145/3689628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 179/214 [05:27<00:37,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c15e8611f9f31b26157fe105e147bad9deb23e3f, https://ijor.co.uk/ijor/article/download/7569/3992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 180/214 [05:27<00:31,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 3390860712ec9dbbf0cc24ee237f7a722820f7b0, https://www.ijltemas.in/submission/index.php/online/article/download/787/183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 182/214 [05:28<00:22,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d551b63629d253c5778f66d263b750d6259422f1, https://ijor.co.uk/ijor/article/download/7588/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 184/214 [05:30<00:28,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 8caee2a585da51c3b547de2e99e591e9ac2c1c4a, https://dl.acm.org/doi/pdf/10.1145/3664190.3672518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 190/214 [05:45<00:56,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download de25087659142aed08137cda84ea502bb41ed7f6, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/exsy.13680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 196/214 [05:47<00:14,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ad8d006813094dda037a19d4eb0ce24165daf600, https://dl.acm.org/doi/pdf/10.1145/3687125\n",
      "Failed to download fa67d6b2fa7ccf184e5bb52b7254ee53deb6dcc1, https://www.mdpi.com/1424-8220/24/15/4940/pdf?version=1722413561\n",
      "Failed to download d14a1677e416bd7ac6fbe01672cd3152fc1f983d, https://www.mdpi.com/2076-3417/14/15/6471/pdf?version=1721890351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 200/214 [05:56<00:23,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b4eeebc419dc0394a64bd2991079cf978e5b2701, https://www.mdpi.com/2227-7390/12/16/2448/pdf?version=1723016818\n",
      "Failed to download 74f6ea4ac45345d80768d2e2973b523f98b33f33, https://dl.acm.org/doi/pdf/10.1145/3688571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 210/214 [06:14<00:08,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 516f48e222fdad35b92113ecdbf8a459d37c95aa, https://www.mdpi.com/2073-431X/13/7/176/pdf?version=1721618358\n",
      "Failed to download a63b70ba0e65b4b2916093e15078348e8a5ec490, https://www.mdpi.com/2673-2688/5/3/74/pdf?version=1724902338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [06:18<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 301aa759e431ae9c24b8deb274996c832bd20f0f, https://pubs.aip.org/aip/pop/article-pdf/doi/10.1063/5.0213625/20106149/082507_1_5.0213625.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ar5iv_url='https://ar5iv.labs.arxiv.org/html/{arxiv_id}'\n",
    "htmlsp_dir=U.pjoin(ptree.lib_dir,'..','files','htmlsp')\n",
    "U.mkdir(htmlsp_dir)\n",
    "pdfsp_dir=U.pjoin(ptree.lib_dir,'..','files','pdfsp')\n",
    "U.mkdir(pdfsp_dir)\n",
    "\n",
    "    \n",
    "# for i in tqdm(refp_arxiv):\n",
    "#     DIR=U.pjoin(htmlsp_dir,f'{i}.html')\n",
    "#     if os.path.exists(DIR):\n",
    "#         continue\n",
    "#     url=ar5iv_url.format(arxiv_id=refp_arxiv[i])\n",
    "\n",
    "#     # Send a GET request to the URL\n",
    "#     response = requests.get(url)\n",
    "\n",
    "#     # Check if the request was successful (status code 200)\n",
    "#     if response.status_code == 200:\n",
    "#         # Save the HTML content to a file\n",
    "#         with open(DIR, \"w\", encoding=\"utf-8\") as file:\n",
    "#             file.write(response.text)\n",
    "#     else:\n",
    "#         print(f\"Failed to retrieve {i}. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "\n",
    "failed_pdfs={}\n",
    "for i in tqdm(refp_pdfs):\n",
    "    url=refp_pdfs[i]\n",
    "    save_path=U.pjoin(pdfsp_dir,f'{i}.pdf')\n",
    "    if not os.path.exists(save_path):\n",
    "        try:\n",
    "            pdf=urllib.request.urlretrieve(url, save_path)\n",
    "        except:\n",
    "            failed_pdfs[i]=url\n",
    "            print(f'Failed to download {i}, {url}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get references and files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 292/292 [00:53<00:00,  5.42it/s]\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "    \"fields\": \"openAccessPdf,externalIds\",\n",
    "}\n",
    "\n",
    "pdf_urls={}\n",
    "external_ids={}\n",
    "for i in tqdm(refs_s2ids):\n",
    "    s2id=refs_s2ids[i]\n",
    "    rsp=requests.get(\n",
    "        paper_detail.format(paper_id=s2id),\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    results = rsp.json()\n",
    "    if 'openAccessPdf' in results and results['openAccessPdf'] is not None:\n",
    "        pdf_urls[i]=results['openAccessPdf']\n",
    "    if 'externalIds' in results and results['externalIds'] is not None:\n",
    "        external_ids[i]=results['externalIds']\n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "arxiv_ids={}\n",
    "for i,j in external_ids.items():\n",
    "    if 'ArXiv' in j:\n",
    "        arxiv_ids[i]=j['ArXiv']\n",
    "\n",
    "arxiv_ids_add={\n",
    "    'fire':'2310.04418',\n",
    "    'mogrifier':'1909.01792',\n",
    "    'hedgehog':'2402.04347',\n",
    "    'synthesizer':'2005.00743',\n",
    "    'settransformer':'1810.00825',\n",
    "    'infiniteformer':'2109.00301',\n",
    "    'etc':'2004.08483',\n",
    "    'feedbackmem':'2002.09402',\n",
    "    'kangpt':'2408.10205',\n",
    "}\n",
    "\n",
    "arxiv_ids.update(arxiv_ids_add)\n",
    "\n",
    "ar5iv_url='https://ar5iv.labs.arxiv.org/html/{arxiv_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 292/292 [00:33<00:00,  8.79it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_node(id):\n",
    "    return ptree.G.nodes[id]['data']\n",
    "\n",
    "\n",
    "def get_references(id):\n",
    "    params={\n",
    "        \"fields\": \"references\",\n",
    "    }\n",
    "    s2id=refs_s2ids[id]\n",
    "    rsp=requests.get(\n",
    "        paper_detail.format(paper_id=s2id),\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    results = rsp.json()\n",
    "    return results['references']\n",
    "\n",
    "\n",
    "all_refs={}\n",
    "for i in tqdm(refs_s2ids):\n",
    "    all_refs[i]=get_references(i)\n",
    "\n",
    "refs_set={}\n",
    "for i in all_refs:\n",
    "    for ref in all_refs[i]:\n",
    "        refs_set[ref['paperId']]=ref['title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4775/4775 [00:21<00:00, 218.05it/s] \n"
     ]
    }
   ],
   "source": [
    "def get_paper_detail(s2id):\n",
    "    params={\n",
    "        \"fields\": \"title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,authors,tldr,embedding,tldr,openAccessPdf,externalIds,references\",\n",
    "    }\n",
    "    rsp=requests.get(\n",
    "        paper_detail.format(paper_id=s2id),\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    results = rsp.json()\n",
    "    return results\n",
    "\n",
    "refs_detail={}\n",
    "save_dir=U.pjoin(ptree.lib_dir,'..','tree_ext','refs2')\n",
    "for i in tqdm(refs_set):\n",
    "    save_path=U.pjoin(save_dir,f'{i}.json')\n",
    "    if U.pexists(save_path):\n",
    "        refs_detail[i]=U.load_json(save_path)\n",
    "        continue\n",
    "    ret=get_paper_detail(i)\n",
    "    if 'error' not in ret:\n",
    "        U.save_json(ret,save_path)\n",
    "    refs_detail[i]=ret\n",
    "    time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs2_detail={}\n",
    "ref2_dir=U.pjoin(ptree.lib_dir,'..','tree_ext','secondary')\n",
    "for i in os.listdir(ref2_dir):\n",
    "    refs2_detail[i.split('.')[0]]=U.load_json(U.pjoin(ref2_dir,i))\n",
    "\n",
    "ref2_arxiv={}\n",
    "ref2_pdfs={}\n",
    "for i in refs_detail:\n",
    "    if 'externalIds' in refs2_detail[i]:\n",
    "        if 'ArXiv' in refs2_detail[i]['externalIds']:\n",
    "            ref2_arxiv[i]=refs2_detail[i]['externalIds']['ArXiv']\n",
    "            continue\n",
    "    if 'openAccessPdf' in refs2_detail[i] and refs2_detail[i]['openAccessPdf'] is not None and 'arxiv' not in refs2_detail[i]['openAccessPdf']['url']:\n",
    "        url=refs_detail[i]['openAccessPdf']\n",
    "        ref2_pdfs[i]=url['url']\n",
    "        continue\n",
    "    # print(i)\n",
    "\n",
    "print(len(ref2_arxiv))\n",
    "print(len(ref2_pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1108 [00:03<17:08,  1.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 0048b9b8fd2c49a141a8f91168b7f0a30161c780, https://www.mdpi.com/1424-8220/23/16/7222/pdf?version=1692262936\n",
      "Failed to download 006fdeff6e1a81c404317ee4056d6cc72f9c0e50, https://www.aclweb.org/anthology/P18-1208.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1108 [00:07<08:40,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 013eb12ce5468f79d58bf859653f4929c5a2bd14, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00542/2074871/tacl_a_00542.pdf\n",
      "Failed to download 0269e74017b160ef3a0a49dff6f6c27a8adf93cf, https://www.aclweb.org/anthology/D17-1014.pdf\n",
      "Failed to download 027beed800f7d5e20194caf6d689345045e8d0d4, https://dl.acm.org/doi/pdf/10.1145/3219819.3219944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 17/1108 [00:07<04:10,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 030d7d7ae48a9f81700b2c1f7cf835235777b8e7, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00405/1962458/tacl_a_00405.pdf\n",
      "Failed to download 04234cd1cad396f76b96042227041abc9e525b0a, https://www.aclweb.org/anthology/P19-1092.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/1108 [00:07<03:33,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 0427110f0e79f41e69a8eb00a3ec8868bac26a4f, https://www.aclweb.org/anthology/D19-1534.pdf\n",
      "Failed to download 04a7021fe6be6bddcfae476493fcc7571e7c613c, https://www.aclweb.org/anthology/D19-1382.pdf\n",
      "Failed to download 04d99c52291f101d77bc54cc6af34e807e207c4e, https://www.mdpi.com/1999-4893/15/6/184/pdf?version=1653544968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 24/1108 [00:07<02:32,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 04db9b694280134f09af5fa787a306907edba29d, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00567/2141008/tacl_a_00567.pdf\n",
      "Failed to download 0539535989147bc7033f4a34931c7b8e17f1c650, https://dl.acm.org/doi/pdf/10.1145/3183713.3196909\n",
      "Failed to download 054aa8d6d5ceb543d3f0d89b578744fd89ef3230, https://www.aclweb.org/anthology/D18-1491.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 30/1108 [00:08<01:55,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 05b1127ee39504516009b25384ca2bd7f2e1b9d9, https://www.aclweb.org/anthology/N18-1150.pdf\n",
      "Failed to download 06354570d5f6be803d4a79bf59ecbb097bca8755, https://www.aclweb.org/anthology/P18-2117.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 35/1108 [00:08<01:33, 11.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 07a64686ce8e43ac475a8d820a8a9f1d87989583, https://www.aclweb.org/anthology/P19-1580.pdf\n",
      "Failed to download 07a9f47885cae97efb7b4aa109392128532433da, https://www.aclweb.org/anthology/2020.acl-main.687.pdf\n",
      "Failed to download 09ebb5f26bbecbe3d604af2da3c65fc6f38833d8, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00663/2370911/tacl_a_00663.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 49/1108 [00:08<00:35, 29.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 0b14178e7d79ac426d0a39700e1ac8b2c6f2e752, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/9781119295952.app1\n",
      "Failed to download 0ba86604228b555475496e200f31878df3aabd6e, https://dl.acm.org/doi/pdf/10.1145/3191513\n",
      "Failed to download 0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38, https://www.aclweb.org/anthology/P17-1161.pdf\n",
      "Failed to download 0c3c4c88c7b07596221ac640c7b7102686e3eae3, https://www.aclweb.org/anthology/D19-1259.pdf\n",
      "Failed to download 0c47cad9729c38d9db1f75491b1ee4bd883a5d4e, https://www.aclweb.org/anthology/D18-1217.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 54/1108 [00:09<00:57, 18.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 0cbf97173391b0430140117027edcaf1a37968c7, https://www.aclweb.org/anthology/2020.findings-emnlp.372.pdf\n",
      "Failed to download 0cf535110808d33fdf4db3ffa1621dea16e29c0d, https://www.aclweb.org/anthology/D19-1599.pdf\n",
      "Failed to download 0d1d943787d8ee4609aa4bd4c51a57a3abb31378, https://www.mdpi.com/1099-4300/13/6/1170/pdf?version=1424784813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 61/1108 [00:09<01:09, 15.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 0d4b28a4a244a569d5a1345464ae2be48595fff0, https://ieeexplore.ieee.org/ielx7/6570655/8716764/08707065.pdf\n",
      "Failed to download 0de0a44b859a3719d11834479112314b4caba669, https://www.aclweb.org/anthology/P19-3007.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 64/1108 [00:10<01:15, 13.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 0e4cd6bae6ac1017e7b1b9bd644375aee65b8372, https://www.aclweb.org/anthology/D19-1224.pdf\n",
      "Failed to download 0f45215ddad707f955ab08e90397fbc5c2a2298c, https://www.aclweb.org/anthology/P19-1551.pdf\n",
      "Failed to download 1006d191e9eb5b4dbc35fc0bb389328ddc75cba7, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00461/2004058/tacl_a_00461.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 69/1108 [00:10<01:15, 13.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 104715e1097b7ebee436058bfd9f45540f269845, https://www.aclweb.org/anthology/P17-1171.pdf\n",
      "Failed to download 10c86505de83647c7b4157595ab10f64e97c94ef, https://www.aclweb.org/anthology/2020.emnlp-main.576.pdf\n",
      "Failed to download 10ed05aa4a0648649c0454193becca59cf5cc181, https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli_a_00459/2058271/coli_a_00459.pdf\n",
      "Failed to download 10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb, https://dl.acm.org/doi/pdf/10.3115/1119176.1119195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 75/1108 [00:10<01:11, 14.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 114745b95c7029c5163b745a37829d1e85fc3083, https://www.aclweb.org/anthology/D19-1139.pdf\n",
      "Failed to download 117e6bcf80958677ae16a9392005079bdf7529cc, https://www.pnas.org/content/pnas/115/25/6347.full.pdf\n",
      "Failed to download 11abce981e90585c142078b5c64b2cb8331b8794, https://www.aclweb.org/anthology/2020.acl-main.222.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 79/1108 [00:11<01:16, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 127ffe6d21b75bd41dd808e3313bc392b9428346, https://www.aclweb.org/anthology/D19-1588.pdf\n",
      "Failed to download 12ae4d4428963d98346e8b16d76c8e165e33a094, https://www.aclweb.org/anthology/P19-1636.pdf\n",
      "Failed to download 131c6f328c11706de2c43cd16e0b7c5d5e610b6a, https://dl.acm.org/doi/pdf/10.1145/3649506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 81/1108 [00:11<01:34, 10.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 13395213d47f78672ab4e81573f2b0fa0cfc8c6d, https://www.aclweb.org/anthology/D17-1239.pdf\n",
      "Failed to download 135112c7ba1762d65f39b1a61777f26ae4dfd8ad, https://www.aclweb.org/anthology/P19-1282.pdf\n",
      "Failed to download 13bc4e683075bdd6a3f0155241c276a772d4aa06, https://dl.acm.org/doi/pdf/10.1145/3422622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 87/1108 [00:11<01:13, 13.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 13fe71da009484f240c46f14d9330e932f8de210, https://www.aclweb.org/anthology/D16-1053.pdf\n",
      "Failed to download 1423fac786b0927a4e8dd1ed2999db4e8171d1c2, https://www.aclweb.org/anthology/D19-1521.pdf\n",
      "Failed to download 146b3649b693ae591c8953c0aae5264512c26ea3, http://dl.acm.org/ft_gateway.cfm?id=1610162&type=pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 89/1108 [00:11<01:17, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 1478075a10ea2e0a1b5bdc170468dcea81e6fcb2, https://www.aclweb.org/anthology/D17-1283.pdf\n",
      "Failed to download 1492ddfd4f4b152b83f11db8c9ecdfd0d2543294, https://www.aclweb.org/anthology/S15-1002.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 93/1108 [00:12<01:28, 11.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 14c454c27dffd655cea839a0684a2d855117cd58, https://www.aclweb.org/anthology/2020.emnlp-main.58.pdf\n",
      "Failed to download 1518039b5001f1836565215eb047526b3ac7f462, https://www.aclweb.org/anthology/P16-1162.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 95/1108 [00:12<01:23, 12.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 1536e8958697c5364f68b2e2448905dbbeb3a0ca, https://www.aclweb.org/anthology/D18-1260.pdf\n",
      "Failed to download 167e1359943b96b9e92ee73db1df69a1f65d731d, https://dl.acm.org/doi/pdf/10.3115/1218955.1218990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 109/1108 [00:14<02:11,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 179790fac38b1c35f6003f65eef7097f8413cd35, https://dl.acm.org/doi/pdf/10.1145/3637364\n",
      "Failed to download 17a43e798ede87cadf71793fd29bb12b92ca71d4, https://dl.acm.org/doi/pdf/10.1145/3340531.3411908\n",
      "Failed to download 17aa716dae728e994a2539bf4952c05ad513bd7a, https://www.aclweb.org/anthology/2020.coling-main.4.pdf\n",
      "Failed to download 17dbd7b72029181327732e4d11b52a08ed4630d0, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00276/1923288/tacl_a_00276.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 115/1108 [00:32<20:44,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 199ff73d2f728e997f860b62a2322823d3e3d9e8, https://www.aclweb.org/anthology/D19-1275.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 119/1108 [00:37<17:44,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 19da5e491b6d8ad3943d2ae595db611314fc7dff, https://downloads.hindawi.com/archive/2013/842981.pdf\n",
      "Failed to download 19f1ad5c233e8e17b8defa02dd9cc750af16509a, https://www.aclweb.org/anthology/2020.acl-main.603.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 121/1108 [00:43<27:13,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 1a327709cc53ff9e52454e50a643abf4a0ac92af, https://doi.org/10.18653/v1/w16-2301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 128/1108 [00:54<19:37,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 1bf3d761b701e58da088a476a37963610ea23a60, https://www.aclweb.org/anthology/W18-3002.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 131/1108 [01:06<37:46,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 1c3112ef8a346b9817382ed34a8c146c53d5bcf5, https://www.aclweb.org/anthology/D18-1269.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 133/1108 [01:06<20:16,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 1c4e9156ca07705531e45960b7a919dc473abb51, http://www.bmva.org/bmvc/2016/papers/paper087/abstract087.pdf\n",
      "Failed to download 1ccf25e9e59ec74c899b7182a619972a196fc138, https://www.aclweb.org/anthology/2020.findings-emnlp.370.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 137/1108 [01:08<09:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 1da30f044d5634b80a98e85994a16f19bbf207ad, http://www.jaad.org/article/S0190962220308215/pdf\n",
      "Failed to download 1e077413b25c4d34945cc2707e17e46ed4fe784a, https://www.aclweb.org/anthology/P18-1031.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 139/1108 [01:09<08:14,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 1ea75cdb7ce8c4f5f2599165e3698034b4142e08, https://www.aclweb.org/anthology/P16-1094.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 142/1108 [01:10<07:04,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 1ec5f3b55a90c1b32ad74dbe4423019d006b6bd3, https://www.aclweb.org/anthology/P19-1623.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 148/1108 [01:16<07:36,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 1fa4e1f1d117af2e37052eedb26990ccd4867925, https://ieeexplore.ieee.org/ielx7/76/9316347/09006806.pdf\n",
      "Failed to download 203b543bfa1e564bb80ff4229b43174d7c71b0c0, https://www.aclweb.org/anthology/P19-1499.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 150/1108 [01:16<05:27,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 207da6d2c07289bf72a2b5974bb3f011ebb5dd0d, https://www.aclweb.org/anthology/2020.acl-main.441.pdf\n",
      "Failed to download 216b05e812896e790d5b5a084614e2523daa198e, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00490/2037124/tacl_a_00490.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 153/1108 [01:17<04:56,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 22655979df781d222eaf812b0d325fa9adf11594, https://www.aclweb.org/anthology/D18-1259.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 159/1108 [01:22<07:52,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 23effc082f2582b39e277dfc99bbce198cf36451, https://epubs.siam.org/doi/pdf/10.1137/18M118709X\n",
      "Failed to download 242fd8725ec6d30a3c8648a4f9ffe9f2cf67ae3f, https://www.aclweb.org/anthology/W19-5201.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 160/1108 [01:23<10:22,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 245b03b60cb4bf0235109af4e48f958fbab03b34, https://www.aclweb.org/anthology/W18-3022.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 164/1108 [01:31<20:19,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 24ddc6d938c32766559d3e5c66de11ac1743fc21, https://www.aclweb.org/anthology/P18-1108.pdf\n",
      "Failed to download 252571243aa4c0b533aa7fc63f88d07fd844e7bb, https://www.aclweb.org/anthology/D17-1036.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 167/1108 [01:32<10:02,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 2573af4e13d9a5dddb257d22cd38a600528d9a8b, https://www.aclweb.org/anthology/2020.acl-main.195.pdf\n",
      "Failed to download 2621323502fc779c79bca7ba112bc4d0c1db1d3f, https://www.aclweb.org/anthology/P19-1381.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 168/1108 [01:34<16:40,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 269648d8760666eb0cee07d1636eb1dcf119c38b, https://direct.mit.edu/coli/article-pdf/26/2/280/1797537/coli.2000.26.2.280.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 171/1108 [01:37<16:11,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 26b47e35fe6e4260fdf7b7cc98f279a73c277494, https://www.aclweb.org/anthology/P18-1158.pdf\n",
      "Failed to download 26bc9195c6343e4d7f434dd65b4ad67efe2be27a, http://dl.acm.org/ft_gateway.cfm?id=2939785&type=pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 173/1108 [01:38<10:27,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 26e743d5bd465f49b9538deaf116c15e61b7951f, https://doi.org/10.18653/v1/n16-1162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 177/1108 [01:43<14:32,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 27725a2d2a8cee9bf9fffc6c2167017103aba0fa, https://doi.org/10.3115/v1/p14-1062\n",
      "Failed to download 2785e7e7f625630eeeedbc45124acf7931ba878d, https://www.aclweb.org/anthology/D19-1438.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 182/1108 [01:48<11:01,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 29219d826ead654f2b863de6eceb69811850b7d4, http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 184/1108 [01:49<08:31,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 2997b26ffb8c291ce478bd8a6e47979d5a55c466, https://www.aclweb.org/anthology/N18-2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 186/1108 [01:50<06:46,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 29de7c0fb3c09eaf55b20619bceaeafe72fd87a6, https://www.aclweb.org/anthology/P18-1082.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 188/1108 [01:51<05:44,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 2a7e68459d79fcfdc147781593a0da9b1e93ab00, https://www.aclweb.org/anthology/P19-2057.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 190/1108 [01:52<06:35,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 2ace8667f2b331001136391cae237d50c0db6383, https://ieeexplore.ieee.org/ielx7/5/4357935/10242251.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 192/1108 [01:59<28:20,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 2b38ddff8e24a07597c8d042ea7b8b85a678e9b2, https://dl.acm.org/doi/pdf/10.1145/3575693.3575747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 195/1108 [02:02<20:47,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 2b8da013966c0c5e020ebc842d49d8ed166c8783, http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00139\n",
      "Failed to download 2babc9ba9dd301d6e61117302bd2a200f7b422e2, https://dl.acm.org/doi/pdf/10.1145/3503222.3507738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 201/1108 [02:09<13:45,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 2c88d7486f9871cb741ba3c7076b8adbb7fd5b68, https://www.aclweb.org/anthology/2020.emnlp-main.710.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 204/1108 [02:10<07:50,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 2cd8e8f510c89c7c18268e8ad51c061e459ad321, https://www.aclweb.org/anthology/D16-1244.pdf\n",
      "Failed to download 2cf3bd0cc1382f35384e259d99e4f9744eeaed28, https://www.aclweb.org/anthology/2020.findings-emnlp.232.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 209/1108 [02:14<07:10,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22, http://dl.acm.org/ft_gateway.cfm?id=3080246&type=pdf\n",
      "Failed to download 2e347a977f14eca7cc5bbbb4c71145b75637340c, https://www.aclweb.org/anthology/2020.acl-main.653.pdf\n",
      "Failed to download 2e55ba6c97ce5eb55abd959909403fe8da7e9fe9, https://www.pnas.org/content/pnas/114/13/3521.full.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 212/1108 [02:15<05:58,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 2e96ca17f9515cf79c9f3530d2771f43ebf2ca88, https://ieeexplore.ieee.org/ielx7/6287639/9312710/09335579.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 213/1108 [02:15<06:32,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c, https://dl.acm.org/doi/pdf/10.1145/3446776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 218/1108 [02:19<09:24,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 305b2cf37e5dece81e95c92883d5a6e28ac93b22, https://www.aclweb.org/anthology/D18-1206.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 221/1108 [02:22<12:35,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 30ab4c9071b10e22717a6b385b220f40a363bc75, https://ieeexplore.ieee.org/ielx7/34/9940445/09658224.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 224/1108 [02:23<07:11,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 30e4c372514c568955826abd84676c9392dd7e71, https://dl.acm.org/doi/pdf/10.1145/76263.76288\n",
      "Failed to download 31212862430352b8434bfceb1d4de29867c7a491, https://www.aclweb.org/anthology/P19-1338.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 227/1108 [02:24<05:02,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 3148268debf6c445953ae3389328f8801394e1ce, https://www.aclweb.org/anthology/P19-1384.pdf\n",
      "Failed to download 316931b899014bd69c0f50f7e773f724f813748f, https://www.aclweb.org/anthology/D15-1288.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 231/1108 [02:31<14:42,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 321f91528af535cefa1b6971df31c609673f463f, https://www.aclweb.org/anthology/P18-1173.pdf\n",
      "Failed to download 32999b20f890bcc5effe80197045d6c147226fe4, https://www.aclweb.org/anthology/2020.findings-emnlp.0.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 235/1108 [02:35<10:35,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 33172567ab1dff9ca32c8d995d88bbff466f3236, https://www.aclweb.org/anthology/2020.acl-main.214.pdf\n",
      "Failed to download 335613303ebc5eac98de757ed02a56377d99e03a, https://www.aclweb.org/anthology/P19-1356.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 236/1108 [02:36<10:56,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 347b926e82fa8e635050a5c7781598642c115596, https://www.pnas.org/doi/pdf/10.1073/pnas.2008852117\n",
      "Failed to download 349eb17c5b61924db8ccc5816c863c6674c8b565, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00493/2038506/tacl_a_00493.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 243/1108 [02:38<05:41,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 34ca2c54b21d5ad088e3138fb844dbbd496b7e3c, https://www.aclweb.org/anthology/W18-0607.pdf\n",
      "Failed to download 35fbb4104b77b27cb6d7a35a7648b17ebd554a25, https://research.vu.nl/files/73602821/Scanjob%20198800058\n",
      "Failed to download 3692f4df9d11af68f9b9c9a526667db3f99e552c, https://dl.acm.org/doi/pdf/10.1145/3567955.3567959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 246/1108 [02:39<03:30,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 36c097a225a95735271960e2b63a2cb9e98bff83, https://www.aclweb.org/anthology/P16-1139.pdf\n",
      "Failed to download 37c9c4e7648f639c0b36f150fc6c6c90b3682f4a, https://dl.acm.org/doi/pdf/10.1145/3528233.3530757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 248/1108 [02:41<07:57,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 388e2fcdcefbe0834e153ab2a0be127092f9674d, https://www.aclweb.org/anthology/2020.acl-demos.30.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 249/1108 [02:42<09:41,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 3891cf3eaeddc964673d48380732210d17cbfb2d, https://link.springer.com/content/pdf/10.1038%2Fnpre.2012.6939.1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 251/1108 [02:47<17:27,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 395de0bd3837fdf4b4b5e5f04835bcc69c279481, https://www.aclweb.org/anthology/2020.acl-main.703.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 254/1108 [02:48<08:57,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 399e7d8129c60818ee208f236c8dda17e876d21f, https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf\n",
      "Failed to download 39e734da43eb8c72e9549b42e96760545036f8e5, https://www.aclweb.org/anthology/D18-1241.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 255/1108 [02:48<07:15,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 3a0986aeaba70facd56dd4ce7e255734cb86517a, https://academic.oup.com/nar/article-pdf/50/D1/D988/42058123/gkab1049.pdf\n",
      "Failed to download 3a0d81ff7917b16156e11fe812182af82edeaa85, https://www.annualreviews.org/doi/pdf/10.1146/annurev-statistics-030718-104938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 260/1108 [03:08<38:14,  2.71s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 3aa52436575cf6768a0a1a476601825f6a62e58f, http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00115\n",
      "Failed to download 3aba4051edc0174c37b88ea4ff6f061c4b5fdeb7, https://www.aclweb.org/anthology/2020.emnlp-main.73.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 261/1108 [03:10<34:06,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 3b76ac2fa92ce933d276ea324160de801ef75fe9, https://www.mdpi.com/2227-7390/9/15/1733/pdf?version=1626956806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 266/1108 [03:12<11:15,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 3c78c6df5eb1695b6a399e346dde880af27d1016, https://www.aclweb.org/anthology/P18-1078.pdf\n",
      "Failed to download 3caf34532597683c980134579b156cd0d7db2f40, https://www.aclweb.org/anthology/D19-1221.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 268/1108 [03:13<09:55,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 3d849136e0070f6d038dd96985ed67ead5aedb69, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00536/2067865/tacl_a_00536.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 273/1108 [03:18<10:34,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 3dd61d97827e3f380bf9304101149a3f865051fc, https://www.aclweb.org/anthology/2020.acl-main.89.pdf\n",
      "Failed to download 3df83a60f55c64b40e6dbcd99cf9f67894a0736e, https://www.aclweb.org/anthology/2020.acl-main.672.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 277/1108 [03:22<10:59,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 3eda43078ae1f4741f09be08c4ecab6229046a5c, https://www.aclweb.org/anthology/W17-2623.pdf\n",
      "Failed to download 3f1d72105060bebba68b672dd6197c0deddca26f, https://www.aclweb.org/anthology/2020.emnlp-main.448.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 281/1108 [03:26<08:45,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 3f9514630194a9fba9505b594ec921b247fecb48, https://www.aclweb.org/anthology/2020.findings-emnlp.117.pdf\n",
      "Failed to download 3fc5ed18c2294596af072df929c8ee12c71f96a2, https://www.aclweb.org/anthology/N18-1033.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 284/1108 [03:32<16:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 3febb2bed8865945e7fddc99efd791887bb7e14f, https://www.aclweb.org/anthology/N18-1202.pdf\n",
      "Failed to download 3ff8d265f4351e4b1fdac5b586466bee0b5d6fff, https://www.aclweb.org/anthology/2020.acl-main.270.pdf\n",
      "Failed to download 401dc39c2c8c910253d47980cfa3b4d2f7790d9b, https://dl.acm.org/doi/pdf/10.1145/3474381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 286/1108 [03:32<09:24,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 40345901fd28cbf65791c34671db6548b1089ed4, https://www.aclweb.org/anthology/P19-1212.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 289/1108 [03:35<09:18,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 413a03a146e6f7b16c11e73243d83e6f1a6627a3, https://www.aclweb.org/anthology/P18-2103.pdf\n",
      "Failed to download 41d49ec6f73ab5621ab8e8cb5ddb677a886ccc76, https://www.aclweb.org/anthology/D19-1018.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 290/1108 [03:35<07:10,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 422cbcd3bfa663cf8394aa8f71be3ede31bb3280, http://www.cell.com/article/S0896627317303653/pdf\n",
      "Failed to download 428b663772dba998f5dc6a24488fff1858a0899f, https://dl.acm.org/doi/pdf/10.1145/3503250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 293/1108 [03:36<05:02,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 42ed4a9994e6121a9f325f5b901c5b3d7ce104f5, https://www.aclweb.org/anthology/P19-1334.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 297/1108 [03:39<05:50,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4450493ecb806cb889b341ae8e430886f2549a61, https://www.aclweb.org/anthology/2020.acl-main.372.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 300/1108 [03:44<13:33,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c, https://www.aclweb.org/anthology/W18-5446.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 303/1108 [03:48<11:51,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4550a4c714920ef57d19878e31c9ebae37b049b2, https://www.aclweb.org/anthology/D17-1151.pdf\n",
      "Failed to download 455afd748e8834ef521e4b67c7c056d3c33429e2, https://www.aclweb.org/anthology/N16-1174.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 308/1108 [03:49<05:08,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 465471bb5bf1a945549d6291c2d23367966b4957, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf\n",
      "Failed to download 46857f0c98f223622ad530b6fc4e446fb9187082, http://www.columbia.edu/cu/biology/courses/w4070/Reading_List_Yuste/haas_04.pdf\n",
      "Failed to download 4698a7e950443e4ea05b6489b3b1218436b86ed7, https://www.aclweb.org/anthology/K19-1092.pdf\n",
      "Failed to download 46c585ee9abf76779ea4b863d2da4358efd0d1d3, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00371/1924150/tacl_a_00371.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 312/1108 [03:53<09:04,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 47c28615714d99e7213b4cd707b5e496debfe8ec, https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.2019.0637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 317/1108 [03:57<09:32,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 495da6f19baa09c6db3697d839e10432cdc25934, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00343/1923401/tacl_a_00343.pdf\n",
      "Failed to download 499556c6ed1daec62e5a57456213cf4f921460f1, https://www.aclweb.org/anthology/D19-1131.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 318/1108 [03:57<07:54,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 49982ddfe116f82abbd0cc5505fd70037057c542, https://www.aclweb.org/anthology/2020.emnlp-main.18.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 319/1108 [03:58<08:47,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 49bc7fb789fec84878440da374c11a7e936b6139, http://www.cell.com/article/S0092867419301096/pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 324/1108 [04:02<10:25,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4af09143735210777281b66997ec12994dbb43d4, https://www.aclweb.org/anthology/P19-1279.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 327/1108 [04:06<12:22,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4b52fd45a52aa84b2b1bb1c8cf36cc2884d7df7a, https://www.aclweb.org/anthology/2020.acl-main.66.pdf\n",
      "Failed to download 4b5979a56cf927d5531e069910e2f43c028e9dd4, https://ieeexplore.ieee.org/ielx7/34/4359286/10497845.pdf\n",
      "Failed to download 4b6f6669060367ae8f58e8a749bde085102f6298, https://direct.mit.edu/coli/article-pdf/46/2/487/1847554/coli_a_00379.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 329/1108 [04:09<16:25,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4c50b9d5794dbb3893f8fbfb42803c1cd27502f2, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1207/s15516709cog0704_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 333/1108 [04:10<08:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4cef3559ba4120ed064bc397d94da4a625feb5d5, http://dl.acm.org/ft_gateway.cfm?id=1075255&type=pdf\n",
      "Failed to download 4d00097433a538002b36cfd7a621daddde3e4c0d, https://www.aclweb.org/anthology/D18-1151.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 335/1108 [04:11<06:43,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4d1c856275744c0284312a3a50efb6ca9dc4cd4c, https://www.aclweb.org/anthology/P18-2124.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 339/1108 [04:14<07:45,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4dabd6182ce2681c758f654561d351739e8df7bf, https://www.aclweb.org/anthology/N16-1155.pdf\n",
      "Failed to download 4e0090afe628c66b0b15bdcdfc2e097c20c7e564, https://www.aclweb.org/anthology/2020.emnlp-main.577.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 343/1108 [04:26<31:40,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4ee2eab4c298c1824a9fb8799ad8eed21be38d21, https://dl.acm.org/doi/pdf/10.5555/1557769.1557821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 348/1108 [04:33<19:49,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4fb0a181676a5200bc6e53dea1b770613c164aab, https://www.aclweb.org/anthology/2020.acl-main.555.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 351/1108 [04:37<15:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 4fc1bf60275c419eeb85e28793305a789dca4717, https://epubs.siam.org/doi/pdf/10.1137/1.9781611972757.70\n",
      "Failed to download 5019dbe8d1da5f128f4f373d6849095cf18fd519, https://www.aclweb.org/anthology/D19-1339.pdf\n",
      "Failed to download 501e463beadc85020e01ba467197218c5b0eb502, https://dl.acm.org/doi/pdf/10.1145/3383455.3422553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 358/1108 [04:39<05:16,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 514e7fb769950dbe96eb519c88ca17e04dc829f6, https://www.aclweb.org/anthology/P18-2006.pdf\n",
      "Failed to download 51df674adabae323e468bad8757b5fbc95b5d362, https://www.aclweb.org/anthology/2020.findings-emnlp.369.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 360/1108 [04:40<03:36,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 520ddb38b59b8fae2209ddc7c6640462cf153eec, https://www.aclweb.org/anthology/P18-2059.pdf\n",
      "Failed to download 5237c5cefa83af4aef3251f518a74b598db964b2, https://www.aclweb.org/anthology/D18-1052.pdf\n",
      "Failed to download 523f420cb55d8070f565c87a50099a9a5b0b9206, http://dl.acm.org/ft_gateway.cfm?id=1599147&type=pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 364/1108 [04:42<04:58,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 5278a8eb2ba2429d4029745caf4e661080073c81, https://dl.acm.org/doi/pdf/10.1145/3586183.3606763\n",
      "Failed to download 528a97b59f3d26ffe4549051ca5cf77cfdd5c559, https://www.aclweb.org/anthology/D18-1492.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 365/1108 [04:42<04:32,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 52d44a7937d076ea82726911cb79ce76e6597e40, https://academic.oup.com/comjnl/article-pdf/6/2/163/1041527/6-2-163.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 369/1108 [04:45<05:31,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 530b5f57cc806f6ee93c66c3db94df8875693c73, https://academic.oup.com/bioinformatics/article-pdf/36/11/3594/33463940/btaa158.pdf\n",
      "Failed to download 530c881c024d53c27b4ec8bdeb277ec1c7a2fde2, https://www.annualreviews.org/doi/pdf/10.1146/annurev-psych-010419-051101\n",
      "Failed to download 531a7f2c659787165df4fd5b4580590b953448e4, https://www.aclweb.org/anthology/W17-5525.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 373/1108 [04:50<12:58,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 547334370d2b03d51a39a1509fe7e164cd30e550, https://www.aclweb.org/anthology/D19-1260.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 377/1108 [04:54<09:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 54f9434d862c1f161042d967589c6618c6f30eb6, https://epubs.siam.org/doi/pdf/10.1137/20M1366794\n",
      "Failed to download 5507dc32b368c8afd3b9507e9b5888da7bd7d7cd, https://www.aclweb.org/anthology/D16-1137.pdf\n",
      "Failed to download 55282e7901cdb58d2707c42a51bc0921a3cb6567, https://downloads.hindawi.com/journals/jam/2001/693821.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 385/1108 [05:04<10:10,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 563a451d34ac33edea5006cb20bd63b6aa64ef0a, https://www.aclweb.org/anthology/D18-1210.pdf\n",
      "Failed to download 5665805becad6c87b194b260f2270d86d560bd3f, https://www.aclweb.org/anthology/2020.emnlp-main.748.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 391/1108 [05:09<07:46,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 57a10537978600fd33dcdd48922c791609a4851a, https://www.aclweb.org/anthology/D16-1139.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 395/1108 [05:11<04:56,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 57fff1bd2265848f126f7f62b10d1c67cb6be0da, https://www.aclweb.org/anthology/2020.acl-main.588.pdf\n",
      "Failed to download 58dfeb0bc41429393bf27ff882b7b679031f106c, https://www.aclweb.org/anthology/P17-1061.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 399/1108 [05:13<04:49,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 59761abc736397539bdd01ad7f9d91c8607c0457, https://www.aclweb.org/anthology/K16-1006.pdf\n",
      "Failed to download 59ce9cdbde13affc05a6c1f48a51ee7b0fcb154b, https://dl.acm.org/doi/pdf/10.3115/1075812.1075835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 403/1108 [05:14<03:09,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 5a96a270cbc73e3b448c2404ff31670c21da7b49, https://www.aclweb.org/anthology/2020.nlp4convai-1.5.pdf\n",
      "Failed to download 5a96f2bfa2deae2bc35b250251d5fbe82ef4932b, https://www.aclweb.org/anthology/D17-1115.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 404/1108 [05:14<02:45,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 5ad6970ff332a20e45a7e71ec1dd85c44292cf5f, https://www.aclweb.org/anthology/W18-3020.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 407/1108 [05:15<02:29,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 5b65975d2c561d91e4d9806356f3c11d465192e2, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00641/2346090/tacl_a_00641.pdf\n",
      "Failed to download 5b721c8767f895f03e1316c5e7ce6d93f19e2937, https://www.aclweb.org/anthology/N18-1100.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 408/1108 [05:18<11:44,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 5bc875d65df812f9617d8ba508c1c85f4d219b19, https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2015.0202\n",
      "Failed to download 5bfd8d40bc071fffaf93685a46974b122ee4239d, https://dl.acm.org/doi/pdf/10.5555/1699571.1699621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 412/1108 [05:20<06:54,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 5c65d095600d6c647426fa3bc45031b208882d5f, http://ml.informatik.uni-freiburg.de/_media/publications/langegabelriedmiller2011chapter.pdf\n",
      "Failed to download 5cea23330c76994cb626df20bed31cc2588033df, http://vikas.sindhwani.org/lowRank.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 414/1108 [05:20<04:51,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 5d22b241836e30d5b0d852b463951ab7e3245ea4, https://www.aclweb.org/anthology/2020.findings-emnlp.7.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 416/1108 [05:24<11:41,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 5ded2b8c64491b4a67f6d39ce473d4b9347a672e, https://www.aclweb.org/anthology/N18-1101.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 421/1108 [05:38<18:48,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 5e53fe8e9ab1714ecbf4120f64fbfcb118d39045, http://www.scirp.org/journal/PaperDownload.aspx?paperID=4708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 423/1108 [05:42<21:27,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc, https://doi.org/10.3115/v1/w14-3302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 425/1108 [05:43<11:58,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 5ed791f810da580c78df6a052c6b9f2e258f6b0a, https://doi.org/10.18653/v1/p16-1144\n",
      "Failed to download 5efadc9019ce3378a0eb6c8f939cdde6c8918b1e, https://www.aclweb.org/anthology/D19-1633.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 428/1108 [05:46<10:29,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 5f3c2e87a02217f44ddce61c1ffdea8751fd0a4a, http://manuscript.elsevier.com/S0167947320302395/pdf/S0167947320302395.pdf\n",
      "Failed to download 5f7ffec5729c8d5aea7485f19a5f20b51811abab, https://www.mdpi.com/2313-0105/9/5/280/pdf?version=1684479800\n",
      "Failed to download 5f907c076930effafaae9e84fad98bdcde092b0f, https://www.mdpi.com/2075-1702/10/8/673/pdf?version=1660137755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 432/1108 [05:47<04:22,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 5f994dc8cae24ca9d1ed629e517fcc652660ddde, https://www.aclweb.org/anthology/P19-1139.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 434/1108 [05:47<03:58,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 604764133befe7a0aaa692919545846197e6e065, https://www.aclweb.org/anthology/D16-1128.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 437/1108 [05:51<07:14,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 623b1c61aa36048a38485a44551cb3fdcbcc827b, https://www.aclweb.org/anthology/P19-2031.pdf\n",
      "Failed to download 623ddcab7e3504ce829280b0255279f5dcd27c43, https://www.aclweb.org/anthology/E17-2041.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 440/1108 [05:55<11:28,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6346222f4d308dad8e716e0fd33be470f6e94cbb, https://www.aclweb.org/anthology/2020.emnlp-main.211.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 442/1108 [06:00<16:43,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 636a79420d838eabe4af7fb25d6437de45ab64e8, https://www.aclweb.org/anthology/D17-1082.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 444/1108 [06:02<14:07,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 63748e59f4e106cbda6b65939b77589f40e48fcb, https://www.aclweb.org/anthology/D19-1387.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 445/1108 [06:02<10:44,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6398cb8f2af1c988a097ed1e1cefb380195edfb8, https://s3.ca-central-1.amazonaws.com/assets.jmir.org/assets/preprints/preprint-12429-submitted.pdf\n",
      "Failed to download 63b049aa8ce112a59473d1174772e01610758ae3, https://www.mdpi.com/2227-9717/9/8/1412/pdf?version=1629110034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 448/1108 [06:05<09:27,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 63e39cdf1ad884da6bc69096bb3413b5b1100559, https://www.aclweb.org/anthology/E17-2025.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 449/1108 [06:06<09:51,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6439ef68667f13b99e822436bad5adaa16f4924a, https://dl.acm.org/doi/pdf/10.1145/3536221.3556630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 456/1108 [06:08<03:40,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 64a39028f5d70ff2f7c3861847adb1f995319c2f, http://manuscript.elsevier.com/S1077314218300079/pdf/S1077314218300079.pdf\n",
      "Failed to download 64b469ebd49deb4ef67ca69b2790a6ca938ba0fd, https://dl.acm.org/doi/pdf/10.1145/3292500.3330897\n",
      "Failed to download 651e5bcc14f14605a879303e97572a27ea8c7956, https://www.aclweb.org/anthology/N16-1014.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 458/1108 [06:09<02:40,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 657329c633709dd1ac34a30d57341b186b1a47c2, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00353/1923932/tacl_a_00353.pdf\n",
      "Failed to download 658721bc13b0fa97366d38c05a96bf0a9f4bb0ac, https://www.aclweb.org/anthology/P19-1441.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 462/1108 [06:13<06:04,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 668087f0ae7ce1de6e0bd0965dbb480c08103260, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1207/s15516709cog1402_1\n",
      "Failed to download 668db48c6a79826456341680ee1175dfc4cced71, https://www.aclweb.org/anthology/P17-1099.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 465/1108 [06:15<06:15,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 672a99813f52aed720d3508d6be7db461328b064, https://dl.acm.org/doi/pdf/10.1145/363707.363723\n",
      "Failed to download 675249e1a2098e53fb6436c0fd8897e9a942b79c, https://www.aclweb.org/anthology/K19-1026.pdf\n",
      "Failed to download 677d17e00fcdad5baffc4fcd3925442f62fc9307, https://dl.acm.org/doi/pdf/10.1145/235809.235811\n",
      "Failed to download 68d1ef141ab947ab2098a2d672bb2789eafa4dd4, http://dl.acm.org/ft_gateway.cfm?id=1073476&type=pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 471/1108 [06:21<07:19,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 693cce5d9764f9e9e0c9c583bf840ac019e2179f, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00338/1923658/tacl_a_00338.pdf\n",
      "Failed to download 697e110df76fe33e232f019d7e44097af3572abd, https://www.aclweb.org/anthology/P18-1041.pdf\n",
      "Failed to download 6a07515741593b34b9a60db857703b6255b76122, https://pubs.acs.org/doi/pdf/10.1021/cen-v039n019.p011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 473/1108 [06:21<05:15,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6a1c61a0da5f56a3fdfca5515767cfd74529524a, https://www.aclweb.org/anthology/D18-1044.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 476/1108 [06:21<03:47,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6af58c061f2e4f130c3b795c21ff0c7e3903278f, https://dl.acm.org/doi/pdf/10.3115/1219840.1219855\n",
      "Failed to download 6af974277664126d8e9e47ed2f06eb56724c0a37, https://www.tandfonline.com/doi/pdf/10.1080/21642583.2019.1708830?needAccess=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 482/1108 [06:27<08:36,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6c7046195f64cccac1ed3275d88d77655534b5a4, https://www.aclweb.org/anthology/P18-1205.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 484/1108 [06:27<05:22,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6c79a9bb8f885050cad70b4c69e016b186ffa538, https://pubs.aip.org/asa/jasa/article-pdf/65/S1/S132/11520317/s132_1_online.pdf\n",
      "Failed to download 6c8503803760c5c7790f72437d0f8b874334e6f0, https://www.aclweb.org/anthology/2020.acl-main.247.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 490/1108 [06:51<17:11,  1.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6e207bb41d6f88a97145df7e0283f2b760981f3b, https://www.mdpi.com/1996-1073/15/24/9654/pdf?version=1671681829\n",
      "Failed to download 6e279973506f42868590d88e4cc01e1c4febb034, https://www.aclweb.org/anthology/D18-1380.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 492/1108 [06:51<10:13,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6e45251b16cd423f3c025f004959c6d2b26efab0, https://www.aclweb.org/anthology/P18-1166.pdf\n",
      "Failed to download 6e6a2fe517b33e1f29d761ae31fb37ddccb9a213, https://www.aclweb.org/anthology/2020.acl-main.120.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 494/1108 [06:54<11:38,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6ed9417eaa7ee16f0563599829a061421a3e0563, https://dl.acm.org/doi/pdf/10.3115/1225403.1225421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 497/1108 [06:56<08:39,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6f8d0cf858a5c8a0ba0c710c53f4486d81a32371, https://journals.sagepub.com/doi/pdf/10.1177/0975156420150401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 501/1108 [07:00<07:44,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6fda494b2afa8fe9a0999372b558a72b9ea8cfee, http://www.jidonline.org/article/S0022202X19304051/pdf\n",
      "Failed to download 6fde81d8640d1d06789c13c145032d2b14721bcf, https://www.aclweb.org/anthology/D19-3019.pdf\n",
      "Failed to download 6fe8c5bf8dddaadf10c765133d38dfef5714347f, https://dl.acm.org/doi/pdf/10.1145/170036.170072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 503/1108 [07:00<04:55,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6, https://www.aclweb.org/anthology/2020.acl-main.747.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 508/1108 [07:08<09:06,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 70557ea6b65846fc30729ceed224acd4ac64ca5d, https://www.aclweb.org/anthology/2020.acl-main.233.pdf\n",
      "Failed to download 705dcc8eadba137834e4b0359e2d696d4b209f5b, https://www.aclweb.org/anthology/E17-1002.pdf\n",
      "Failed to download 7072db6eddb85ecd2c117365d91bd694760f726e, https://direct.mit.edu/coli/article-pdf/48/3/733/2040503/coli_a_00445.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 513/1108 [07:20<14:01,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 710d183174844da5b7f392667f3cc25d2b098dde, https://www.aclweb.org/anthology/D19-1282.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 514/1108 [07:20<11:05,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 71171f8d34d0eec5630a16fff239c978fe53c383, https://dash.harvard.edu/bitstream/1/8899722/1/MichelScience2011.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 519/1108 [07:28<09:30,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 71cd5145c3ec27838bf06a1a09980314f05a17ba, https://www.aclweb.org/anthology/2020.acl-demos.16.pdf\n",
      "Failed to download 71e5a76ea53877e6d295dad67759a3683636854f, https://dl.acm.org/doi/pdf/10.1145/3301275.3302288\n",
      "Failed to download 71f9740e6d41141ed37ab3ddc793263b6124ef7f, http://www.jaad.org/article/S0190962220301146/pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 522/1108 [07:31<07:46,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 722e01d5ba05083f7a091f3188cfdfcf183a325d, https://www.aclweb.org/anthology/P16-1125.pdf\n",
      "Failed to download 7238baa75e78838398a03aa705742004ab068d35, https://www.aclweb.org/anthology/S19-2146.pdf\n",
      "Failed to download 7250889c52660a4a77e02c76236b2443f33b9eae, https://research.vu.nl/files/272546402/The_Distributed_Nature_of_Working_Memory.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 524/1108 [07:31<04:44,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 72c2802851078ca911f89b61b9c7312fe77637c4, https://www.aclweb.org/anthology/2020.emnlp-main.79.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 529/1108 [07:32<03:01,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 730043364aed106241ef18ab3e3b5e316802a254, https://www.aclweb.org/anthology/D19-1251.pdf\n",
      "Failed to download 732e3faec4e5be4d144256f2c379b9dc49f0b227, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00547/2075722/tacl_a_00547.pdf\n",
      "Failed to download 7345843e87c81e24e42264859b214d26042f8d51, https://www.aclweb.org/anthology/N16-1024.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 533/1108 [07:33<02:12,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 757a4e5fd848885e9c8d85450fe15c85b86d5ffb, https://www.aclweb.org/anthology/D18-1130.pdf\n",
      "Failed to download 75895ce98904e8afaaa248f081a1da501bd2dbe2, https://dl.acm.org/doi/pdf/10.3115/1072133.1072221\n",
      "Failed to download 75fe6c3ffdea2608794b4f21119c5a4dec07663a, https://www.aclweb.org/anthology/D19-1437.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 534/1108 [07:34<02:02,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 763654e1d3b875d9f914b5171e765ef53e7cf271, https://www.aclweb.org/anthology/2020.coling-main.607.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 537/1108 [07:35<02:53,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 76a9f336481b39515d6cea2920696f11fb686451, https://www.aclweb.org/anthology/2020.acl-main.385.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 538/1108 [07:43<24:08,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 77021fb48704b860fa850dd103b79db4dcf920ee, https://dl.acm.org/doi/pdf/10.3115/1073083.1073106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 544/1108 [07:51<15:50,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 78a9513e70f596077179101f6cb6eadc51602039, http://dl.acm.org/ft_gateway.cfm?id=1220855&type=pdf\n",
      "Failed to download 79a502caa0b12573f56a7e8948459722aa891479, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00574/2143272/tacl_a_00574.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 548/1108 [07:52<06:53,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 79c93274429d6355959f1e4374c2147bb81ea649, https://www.aclweb.org/anthology/D19-1514.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 552/1108 [08:11<21:17,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 7a27cc0cc37931e85315ed41333f01cb6de18c02, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00436/1979279/tacl_a_00436.pdf\n",
      "Failed to download 7a67159fc7bc76d0b37930b55005a69b51241635, https://www.aclweb.org/anthology/N16-1012.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 554/1108 [08:12<13:41,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 7af89df3691d8c33aaf1858f7cc51da1bc9549a9, https://www.aclweb.org/anthology/D18-1443.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 557/1108 [08:36<44:29,  4.84s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 7ba9b6266569bd7b6a3c2ec64348c5b969a5ceb7, https://www.aclweb.org/anthology/D18-1477.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 558/1108 [08:36<32:23,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 7bd83b055702bc178aa26def5b6df463f8eab7b9, https://ieeexplore.ieee.org/ielx7/34/9703108/09178977.pdf\n",
      "Failed to download 7c25adf2ddb35df05a61c697da97efb8583d77df, https://dl.acm.org/doi/pdf/10.1145/3579371.3589350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 561/1108 [08:38<16:20,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 7c5eee24f6f8b8dafc5133c4ede470bdcfae7d40, https://www.aclweb.org/anthology/2020.acl-main.709.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 562/1108 [08:40<16:24,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 7cc9169ad1c5c22fecd93cedef06a6025d65c558, https://www.mdpi.com/2075-1702/10/7/512/pdf?version=1656067847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 569/1108 [08:59<20:01,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 7d5cf22c70484fe217936c66741fb73b2a278bde, http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00021\n",
      "Failed to download 7d67237398986a6088c696df0bf57646c714508f, https://www.aclweb.org/anthology/P19-1290.pdf\n",
      "Failed to download 7e5709d81558d3ef4265de29ea75931afeb1f2dd, https://dl.acm.org/doi/pdf/10.1145/3530811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 573/1108 [09:03<10:45,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 7ea59779ffb392f099d5304680126b4299f43750, https://www.aclweb.org/anthology/P19-1228.pdf\n",
      "Failed to download 7f6acbbeabcbd585ef52105749705ca9cc3854bc, https://science.sciencemag.org/content/sci/358/6362/486.full.pdf\n",
      "Failed to download 7fc3a6862f746c9988d82c023d2b7aaf0e089168, https://dl.acm.org/doi/pdf/10.1145/357980.357997\n",
      "Failed to download 7fdafb15fbc7af75ccd1613e2a7ac21ee1de53e3, https://www.mdpi.com/2227-9717/9/9/1685/pdf?version=1632361635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 579/1108 [09:09<09:28,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 80376bdec5f534be78ba82821f540590ebce5559, https://www.aclweb.org/anthology/2020.emnlp-main.437.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 582/1108 [09:12<07:37,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 806adbb35ed4a95f51518f5962fd59685ad4706b, https://www.aclweb.org/anthology/2020.findings-emnlp.379.pdf\n",
      "Failed to download 80a624b9327d9050244dfebac96f7f6cf806880f, https://www.aclweb.org/anthology/P15-1162.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 584/1108 [09:13<04:28,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 80c05cae76e05fab33ff5622157731d0a5549723, https://www.aclweb.org/anthology/W16-1610.pdf\n",
      "Failed to download 80cf2a6af4200ecfca1c18fc89de16148f1cd4bf, https://www.aclweb.org/anthology/D19-1441.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 585/1108 [09:16<11:31,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 81f5810fbbab9b7203b9556f4ce3c741875407bc, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00300/1923170/tacl_a_00300.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 588/1108 [09:37<36:27,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 8277d67853a3b4dd3060a7aab1ae1c4af6c81211, http://www.cns.nyu.edu/pub/eero/paninski03-reprint.pdf\n",
      "Failed to download 82bb306038446302cedd20fa986d20640ed88a2e, https://www.aclweb.org/anthology/D16-1058.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 591/1108 [09:41<20:18,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 82fae97673a353271b1d4c001afda1af6ef6dc23, http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/papers/habmm_iccv2011.pdf\n",
      "Failed to download 831cfd0c5120f84a857b90f17ac761339fad0dd9, https://www.aclweb.org/anthology/N16-1106.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 592/1108 [09:41<15:49,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 832fc9327695f7425d8759c6aaeec0fa2d7b0a90, https://www.aclweb.org/anthology/P16-1145.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 597/1108 [10:00<23:36,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00317/1923348/tacl_a_00317.pdf\n",
      "Failed to download 83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05, https://dl.acm.org/doi/pdf/10.1145/3600006.3613165\n",
      "Failed to download 83e7654d545fbbaaf2328df365a781fb67b841b4, https://www.aclweb.org/anthology/P17-1152.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 599/1108 [10:01<15:37,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 84476fdf6ead3553f4493dff8e02308439d6222b, https://www.aclweb.org/anthology/2020.findings-emnlp.298.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 602/1108 [10:05<11:22,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 853d4d94651c6d9f8ed4d114e1eb21f15f786daa, https://www.aclweb.org/anthology/N18-2097.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 604/1108 [10:09<13:15,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 85653b72209fbf6cb0b9d4f5da2be4d35678ec73, https://www.aclweb.org/anthology/P18-1209.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 606/1108 [10:12<12:03,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 85b68477a6e031d88b963833e15a4b4fc6855264, https://www.aclweb.org/anthology/N16-1098.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 608/1108 [10:13<06:53,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 85bdefa400e2eb8cb312baf02a068164c510c6ad, https://ieeexplore.ieee.org/ielx7/5962385/6828828/06814892.pdf\n",
      "Failed to download 85f94d8098322f8130512b4c6c4627548ce4a6cc, https://www.aclweb.org/anthology/D17-1039.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 610/1108 [10:13<04:07,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 8648dbfff9662fa9c62a95622712dd2951b5b3a3, https://dl.acm.org/doi/pdf/10.3115/1075527.1075614\n",
      "Failed to download 8659bf379ca8756755125a487c43cfe8611ce842, https://www.aclweb.org/anthology/W19-4302.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 613/1108 [10:19<08:21,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 867db5097ad6aaef098c60b0845785b440eca49a, https://www.aclweb.org/anthology/P19-3019.pdf\n",
      "Failed to download 869e7b3409d2fdb87aee7cd1a1f2246e5a271e3d, https://www.aclweb.org/anthology/2020.conll-1.41.pdf\n",
      "Failed to download 8723dc63469c56d6a3c038d54ea3407228da4c44, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/adma.202400904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 616/1108 [10:19<04:28,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 87fc28cbb193a3bc100e13a4a57a8dc9ce7e31a3, https://www.aclweb.org/anthology/D17-1040.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 618/1108 [10:22<06:36,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 889e57259a1d6017701fb2c2ceece82f9f4eff4c, https://www.aclweb.org/anthology/N16-1036.pdf\n",
      "Failed to download 8905f3dcd215fbc3d56839b6f52a43d77ac59fe8, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00356/1924032/tacl_a_00356.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 622/1108 [10:30<12:19,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 894c3d6fae932af44f7a52fc130c414c171c8bea, https://www.aclweb.org/anthology/2020.eval4nlp-1.2.pdf\n",
      "Failed to download 896e7e1306277f65b7f316698ca93b93b448b8d8, https://journals.sagepub.com/doi/pdf/10.1177/172460080201700213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 626/1108 [10:34<08:05,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 898ddbc6d747e4583a3e870d33cb228691efb87c, https://journals.physiology.org/doi/pdf/10.1152/jn.1973.36.1.61\n",
      "Failed to download 89e25b268de51bc58c626d14238d741901a99cdb, http://www.cell.com/article/S0092867420310850/pdf\n",
      "Failed to download 89e93daca5e76a909579037f51623cf63a190fe8, https://pubs.acs.org/doi/pdf/10.1021/cen-v049n043.p033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 628/1108 [10:34<05:38,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 8a1ea7b6e7e834d146ad782be5d63f57f806a9cc, https://ieeexplore.ieee.org/ielx7/34/4359286/09887996.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 632/1108 [10:36<04:31,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad, https://www.aclweb.org/anthology/P19-1472.pdf\n",
      "Failed to download 8b354d76813bd5375e7e5c8d17f630bec5936a01, https://www.aclweb.org/anthology/N18-4013.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 635/1108 [10:38<03:15,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 8b53d0d64a118ac80936750dc77c9f633fe7a1a5, https://www.aclweb.org/anthology/P19-1010.pdf\n",
      "Failed to download 8cef9900c04d7f661c08f4b5b1ed4337ace042a3, https://www.aclweb.org/anthology/D19-1443.pdf\n",
      "Failed to download 8cf62055fa0faab9c325f4b30415f5b0dc285434, https://dl.acm.org/doi/pdf/10.1145/3377930.3389847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 638/1108 [10:38<01:59,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 8d350f2d767a70d55275a17d0b3dfcc80b2e0fee, https://pubs.aip.org/asa/jasa/article-pdf/62/S1/S63/11558910/s63_5_online.pdf\n",
      "Failed to download 8d908042f139575d6688c745e94156c9df6eae07, https://www.aclweb.org/anthology/2020.emnlp-main.463.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 641/1108 [10:40<02:44,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 8e3a3ba309fe68f5b1d02266a03395332f7d945c, https://www.aclweb.org/anthology/D17-1047.pdf\n",
      "Failed to download 8ecb900b992a4d55cd1033f672a342dca63d3c05, http://shoup.net/ntb/ntb-b4.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 645/1108 [10:54<12:49,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 8ed9254d93f540fdc922718fa4df47108f6f9df2, https://www.aclweb.org/anthology/2020.findings-emnlp.65.pdf\n",
      "Failed to download 8ee4eda834e95124aca1e5ff05a1b8ce7d1487ec, https://epubs.siam.org/doi/pdf/10.1137/18M1183480\n",
      "Failed to download 8f34ee2ec88e8b19b2736de55eb170539d26e527, https://www.aclweb.org/anthology/2020.emnlp-main.365.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 648/1108 [10:54<06:04,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 8f46c21fef31a4cdf7b1808e67171466a9317882, http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00019\n",
      "Failed to download 8f7ef2aa3d84714c1bd7ca30e1f4370688134cf0, http://www.jidonline.org/article/S0022202X1830441X/pdf\n",
      "Failed to download 8fa2d2d562c0564383b3975477b97a337842276f, https://www.mdpi.com/1996-1073/16/1/313/pdf?version=1672828942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 651/1108 [10:55<04:28,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9001eb3c3d5a96ad3d804410c2437e6f60feade9, https://www.aclweb.org/anthology/2020.coling-main.580.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 654/1108 [10:58<05:41,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9146414fca384e73f11ccfd3db8ad6d2a1e8eda2, https://www.aclweb.org/anthology/2020.acl-main.164.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 659/1108 [11:20<17:19,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 928f9dccb806a3278d20d82cc53781c5f44e2bb1, https://www.aclweb.org/anthology/P18-1249.pdf\n",
      "Failed to download 932a5de79d8a8ebb75ea0c43493450fd9922e738, https://www.aclweb.org/anthology/W17-4413.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 661/1108 [11:20<09:50,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 93499a7c7f699b6630a86fad964536f9423bb6d0, https://www.aclweb.org/anthology/D15-1166.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 664/1108 [11:27<14:51,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 942de4b239ff9eff33f7624641f97deaf541df04, https://www.tandfonline.com/doi/pdf/10.1016/S0968-8080%2806%2928264-8?needAccess=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 667/1108 [11:27<06:41,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 942e1b9a15c4271fe3fde3151f839e8fc07ebcc7, https://academic.oup.com/bioinformatics/article-pdf/33/14/i252/25156878/btx257.pdf\n",
      "Failed to download 94551d326be51a57434659093904524c39b877cd, https://www.aclweb.org/anthology/2020.acl-main.225.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 670/1108 [11:29<04:08,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 949fef650da4c41afe6049a183b504b3cc91f4bd, https://www.aclweb.org/anthology/P19-1656.pdf\n",
      "Failed to download 952af139e6a49c5b6490663be967d312c438334d, https://www.aclweb.org/anthology/D18-1544.pdf\n",
      "Failed to download 955191363c3676f71766af3d14d1e6bbc0f040d6, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00562/2131191/tacl_a_00562.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 672/1108 [11:29<02:34,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 95a251513853c6032bdecebd4b74e15795662986, https://www.aclweb.org/anthology/W19-4828.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 677/1108 [11:40<11:40,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 969287b8a96e242793b11f0dbb99ec341228106f, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00448/1985933/tacl_a_00448.pdf\n",
      "Failed to download 96d654dbd6d77c8b3d4fe13ee4111feee4e4fa85, https://www.aclweb.org/anthology/2020.sustainlp-1.20.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 678/1108 [11:40<09:08,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 97394554eb5a74c3160c6bd743fcd3e4bd6cbe28, https://www.aclweb.org/anthology/W17-0906.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 680/1108 [11:43<09:04,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 97f08c1ae8ca5ddf5948c66bfbbc0546ac154807, https://www.aclweb.org/anthology/2020.acl-main.244.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 684/1108 [11:48<08:15,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 988f2bc5fccbea00a23ecea2da112982d397a3dd, https://www.aclweb.org/anthology/P19-1030.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 686/1108 [12:01<23:14,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9904a69eab0792859108eec6b0578d11264b8e83, https://www.aclweb.org/anthology/D16-1172.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 690/1108 [12:02<08:28,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 990a7b4eceedb6e053e6386269481bdfc42a1094, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00266/1923252/tacl_a_00266.pdf\n",
      "Failed to download 995538af8de9868713128b35c511810445e65916, https://dl.acm.org/doi/pdf/10.1145/322217.322232\n",
      "Failed to download 995b7affd684b910d5a1c520c3af00fd20cc39b0, https://www.aclweb.org/anthology/W18-2605.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 692/1108 [12:02<05:29,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7, https://www.aclweb.org/anthology/N18-2002.pdf\n",
      "Failed to download 997c55547aeca733dfc5dfebd12412612ecba022, https://www.aclweb.org/anthology/D18-1503.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 694/1108 [12:03<03:28,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 99ad0533f84c110da2d0713d5798e6e14080b159, https://www.aclweb.org/anthology/N18-1023.pdf\n",
      "Failed to download 9a21740d87976bf76f4a9668a9da631035302fb2, https://www.aclweb.org/anthology/2020.emnlp-main.574.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 695/1108 [12:03<02:47,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9ae116139ae77ca6d78e162e9639681adef5761c, https://www.aclweb.org/anthology/2020.acl-main.561.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 696/1108 [12:24<42:29,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9b1df5b85c7e13edc350b68173eeb2b7840dffb1, http://psiexp.ss.uci.edu/research/papers/memory/HowardKahana.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 698/1108 [12:25<23:21,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9b4a861151fabae1dfd61c917d031c86d26be704, https://www.aclweb.org/anthology/W18-2706.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 702/1108 [12:28<07:59,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9c5c89199114858eafbe50b46d77d38ffd03b28a, https://www.aclweb.org/anthology/D18-1149.pdf\n",
      "Failed to download 9cdb5fd856f6e277332f8e1ce2585fc2217b97d2, https://www.aclweb.org/anthology/S16-1005.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 703/1108 [12:28<06:14,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9d16e1b2168df2290019c1f7fe377159ec2b07f9, http://manuscript.elsevier.com/S0028393217300350/pdf/S0028393217300350.pdf\n",
      "Failed to download 9d6fe89150401d69a90ae10dc9c4135ab406903c, https://epubs.siam.org/doi/pdf/10.1137/16M1062296\n",
      "Failed to download 9d7871d20221ba7b2ebe2b34469d77787969dc21, https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.1912.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 706/1108 [12:28<03:01,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9d7902e834d5d1d35179962c7a5b9d16623b0d39, https://www.aclweb.org/anthology/D19-1006.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 708/1108 [12:29<03:05,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9d92905edc1a5d027b0827d1309bc6ac03dd94a0, https://www.aclweb.org/anthology/2020.acl-main.145.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 714/1108 [12:37<04:23,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9e17578e742024202156f27231b5c8ebbf3abc5f, http://www.ee.columbia.edu/ln/dvmm/publications/12/CVPR_LateFusion.pdf\n",
      "Failed to download 9e35cd34c87332796ed9d1480068ed8bb275bd45, https://www.aclweb.org/anthology/K18-1010.pdf\n",
      "Failed to download 9e4134d0fba86de14c31c8f9e9a237e2faaffc68, http://www.cell.com/article/S0896627317305937/pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 717/1108 [12:38<02:57,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9e67b9758520e49016ab66bafb974d2e1ed762d1, https://www.aclweb.org/anthology/2020.emnlp-main.213.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 720/1108 [12:41<04:07,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2, https://www.aclweb.org/anthology/2020.findings-emnlp.148.pdf\n",
      "Failed to download 9ef902f3c427d697f3579cd79844b44de99bc93c, https://www.aclweb.org/anthology/S17-2013.pdf\n",
      "Failed to download 9f1623d474ebd4b9f483f559e96e647934ed8158, https://goldbook.iupac.org/files/pdf/goldbook.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 723/1108 [12:44<05:32,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 9fa8d73e572c3ca824a04a5f551b602a17831bc5, https://dl.acm.org/doi/pdf/10.5555/1610075.1610094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 726/1108 [12:46<04:20,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a039ea239e37f53a2cb60c68e0a1967994353166, https://www.aclweb.org/anthology/W19-4808.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 728/1108 [12:46<03:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a0451bbe27749d112a093c4e11b4f80b71b7bcd7, https://ieeexplore.ieee.org/ielx7/6287639/8948470/08963659.pdf\n",
      "Failed to download a113053b624b599b204fbd6599284b726c17f916, https://www.aclweb.org/anthology/2020.emnlp-main.661.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 729/1108 [12:46<02:36,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a14d0102e05b34fb0caec09013f0f9ce0f0fa26d, https://ieeexplore.ieee.org/ielx7/5/9369414/09369420.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 733/1108 [12:50<03:33,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a1b35b15a548819cc133e3e0e4cf9b01af80e35d, https://www.aclweb.org/anthology/W19-3901.pdf\n",
      "Failed to download a1fbd97c8b36f312d526fc2904cc2869089240f4, https://www.aclweb.org/anthology/D19-1098.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 734/1108 [12:50<02:47,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096, https://www.aclweb.org/anthology/S17-2001.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 737/1108 [12:53<03:44,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a2aacbe253b344b8f3cdf3f6b44ae84169cfefde, http://www.cell.com/article/S0960982221004619/pdf\n",
      "Failed to download a308e67ad08f29c7bcfeef9e7a20f72453e31678, https://www.aclweb.org/anthology/W18-2903.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 738/1108 [12:56<06:59,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a3afb3795f7054fc5b334d2d6feb92f0999942c7, https://dl.acm.org/doi/pdf/10.1145/3372885.3373827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 741/1108 [12:56<03:39,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a3ec5030b6b0e711592e2b66cdbcfc1618da05d6, https://pubs.aip.org/aip/cha/article-pdf/doi/10.1063/1.3673238/13927455/013105_1_online.pdf\n",
      "Failed to download a494b23f70cec7f5e69b971e9837fcecae5d128d, http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00107\n",
      "Failed to download a4a41319d5805a29316f24ed9519f09db77d4c29, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00632/2325685/tacl_a_00632.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 744/1108 [12:57<02:28,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a4c40532e68728fbeab5d9415f6ad8e9530db360, https://www.aclweb.org/anthology/W17-3518.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 746/1108 [12:58<02:23,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a56ebc39b8c527774be705cccdcb5f66c7302e0c, https://www.aclweb.org/anthology/D18-1152.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 747/1108 [12:58<02:17,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a5cff23d7b569a8a2b102a0c2c9dd1d0d9e96984, https://ieeexplore.ieee.org/ielx7/8423754/9205179/09139355.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 750/1108 [13:01<04:06,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a6979a2520fa9342953c3c620d4030a4e44d2b7f, https://ieeexplore.ieee.org/ielx7/6221020/6363502/10237242.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 752/1108 [13:04<04:49,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a6ce051b85e7c21287de9dd53f81b4e72925e1a9, https://academic.oup.com/comjnl/article-pdf/7/2/149/959725/070149.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 754/1108 [13:05<04:30,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a75869d69cc86f501939c237ae4711aa2885f6a6, https://www.aclweb.org/anthology/D18-1398.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 756/1108 [13:06<03:20,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a81874b4a651a740fffbfc47ef96515e8c7f782f, https://www.aclweb.org/anthology/P19-1612.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 757/1108 [13:06<02:45,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a85e512d8845bd007b0866b4a97e8341463f8190, https://ieeexplore.ieee.org/ielx7/34/6914638/06809191.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 760/1108 [13:08<02:24,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a8e4580471908d17e279000d328f39654359bd6e, https://www.aclweb.org/anthology/W17-3207.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 761/1108 [13:08<02:08,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a9075f6332542e12b2bf3cdbdb3a6ed44733fb41, https://doi.org/10.18653/v1/p16-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 764/1108 [13:13<04:55,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download a929a3d92e305f4ef22ad9e52d264e7782394bd4, https://peerj.com/articles/cs-93.pdf\n",
      "Failed to download a93e6c7762125c465c467d4b07b2872369db4ce5, https://www.aclweb.org/anthology/2020.acl-main.604.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 765/1108 [13:13<03:41,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download aa0d9ad6adf19aa6bafedcb11fd4de92a5f2a3a8, http://dl.acm.org/ft_gateway.cfm?id=3055430&type=pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 767/1108 [13:13<02:54,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download aa52f05176380ac84294d7e941bc036d1665aaac, https://direct.mit.edu/neco/article-pdf/30/12/3151/1048104/neco_a_01143.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 773/1108 [13:17<02:42,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ab4d183bc40c963e69a3ac8fa2e9b8587e1a0dd0, https://www.mdpi.com/2227-7390/11/12/2738/pdf?version=1686920823\n",
      "Failed to download abd1c342495432171beb7ca8fd9551ef13cbd0ff, http://dl.acm.org/ft_gateway.cfm?id=3065386&type=pdf\n",
      "Failed to download ac11062f1f368d97f4c826c317bf50dcc13fdb59, https://www.aclweb.org/anthology/D18-1179.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 775/1108 [13:20<03:44,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download aca16f64ddbf187f8944118c8f72777c3d682521, https://www.aclweb.org/anthology/P19-1424.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 777/1108 [13:21<03:02,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download acf5a74ccb14b01430dab2d200d9aabc5ee9dc16, https://www.aclweb.org/anthology/D19-5801.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 781/1108 [13:30<10:51,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ad796bf779c8617d1e0d8111913ac3f8eaaf6532, https://www.aclweb.org/anthology/P18-1117.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 782/1108 [13:31<08:03,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ada6daf25c16779e49bdb59ccee26e87656a83aa, https://ieeexplore.ieee.org/ielx7/6287639/8600701/08747502.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 785/1108 [13:37<09:13,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ae28c9932e7d16d6b2a25aa14532f9fd0138ba3a, http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 787/1108 [13:41<08:23,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download af5c4b80fbf847f69a202ba5a780a3dd18c1a027, https://www.aclweb.org/anthology/D18-1009.pdf\n",
      "Failed to download afa72122ba06b6a694c21cf67d82620662e4917c, https://dl.acm.org/doi/pdf/10.1145/3503222.3507767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 789/1108 [13:41<05:19,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download afd799d8074c666b689022b65604a0151d886ee2, https://www.mdpi.com/1099-4300/15/6/2246/pdf?version=1424785022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 791/1108 [14:02<25:21,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b0130277677e5b915d5cd86b3afafd77fd08eb2e, http://l2r.cs.uiuc.edu/~danr/Teaching/CS598-05/Papers/Katz87.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 796/1108 [14:05<07:32,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b0b0dddb8310e01b9407a21674c2d33a23a6e967, https://www.aclweb.org/anthology/2020.findings-emnlp.414.pdf\n",
      "Failed to download b0ea633e0c22fbd8cbc531c7326376725d16ce25, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00304/1923427/tacl_a_00304.pdf\n",
      "Failed to download b122a828f5fee3c6afc54e70f41b00184d6383fc, https://www.aclweb.org/anthology/D15-1229.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 797/1108 [14:05<05:56,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b123a0d46ad917b79c43c5ae981e03ed2458ed11, https://www.aclweb.org/anthology/P17-1015.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 800/1108 [14:34<30:19,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b20ddcbd239f3fa9acc603736ac2e4416302d074, https://www.aclweb.org/anthology/2020.emnlp-main.731.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 804/1108 [14:47<14:55,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b26f2037f769d5ffc5f7bdcec2de8da28ec14bee, https://www.aclweb.org/anthology/2020.emnlp-main.550.pdf\n",
      "Failed to download b28f7e2996b6ee2784dd2dbb8212cfa0c79ba9e7, https://www.aclweb.org/anthology/D16-1021.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 808/1108 [14:48<05:11,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b2ca160a0bd758c4cafd45b77093ff8f7fb743f3, https://www.mdpi.com/2313-0105/9/6/325/pdf?version=1686655720\n",
      "Failed to download b3564be8b79f25585acb035f3deaf4ae93c26d8f, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00306/1923102/tacl_a_00306.pdf\n",
      "Failed to download b39efed2e73357db4691f66935cf62e7b51f30e1, https://www.aclweb.org/anthology/W19-4801.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 810/1108 [14:48<03:31,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b496a6d0519eb355122c28b1cd50de7b9a17709b, http://dl.acm.org/ft_gateway.cfm?id=3037716&type=pdf\n",
      "Failed to download b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb, https://www.aclweb.org/anthology/W18-6319.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 812/1108 [14:48<02:27,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b5246fa284f86b544a7c31f050b3bd0defd053fd, https://www.aclweb.org/anthology/D18-2012.pdf\n",
      "Failed to download b56494ae0df11d96d15f507b7dd18de544b058c5, https://www.aclweb.org/anthology/W19-5207.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 818/1108 [14:54<03:02,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b60abe57bc195616063be10638c6437358c81d1e, http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00105\n",
      "Failed to download b61c6405f4de381758e8b52a20313554d68a9d85, https://www.aclweb.org/anthology/2020.acl-main.645.pdf\n",
      "Failed to download b65dd52767c71c800d913ad5ae30be6125fd0f63, https://www.researchsquare.com/article/rs-65710/v1.pdf?c=1599095133000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 821/1108 [14:54<02:01,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b761d0afcf976606543007a528ae87034f663d8e, https://www.aclweb.org/anthology/2020.emnlp-main.42.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 824/1108 [14:57<02:58,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f, https://www.aclweb.org/anthology/P17-1018.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 825/1108 [14:58<02:32,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download b95184d5eb25b0fe66d8bd1ad1b7677a51c21702, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00361/1924225/tacl_a_00361.pdf\n",
      "Failed to download b97c3c370401dc34d2adbeb24f34de5180a14be6, https://dl.acm.org/doi/pdf/10.1145/3466752.3480125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 830/1108 [15:03<04:09,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ba1382a0574baa0345fd727f259bc86797fe1381, https://www.aclweb.org/anthology/P18-1161.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 835/1108 [15:05<01:59,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download bb567cf793a6c4df1a652489c5ce866fe044f0e5, https://www.aclweb.org/anthology/N18-1143.pdf\n",
      "Failed to download bb669de2fce407df2f5cb2f8c51dedee3f467e04, https://www.aclweb.org/anthology/P18-1008.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 837/1108 [15:05<01:19,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d, https://www.aclweb.org/anthology/2020.emnlp-main.703.pdf\n",
      "Failed to download bbb3a49edf69a1909c0cf453858b451ef23fcbaf, https://www.aclweb.org/anthology/P18-1049.pdf\n",
      "Failed to download bbf58df1856fd582817fe93f816f1e03bfe3b17b, https://dl.acm.org/doi/pdf/10.1145/48529.48535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 840/1108 [15:07<02:31,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download bce22675d77e1ef28e92f3793c02f8f5ccdb0ddd, https://epubs.siam.org/doi/pdf/10.1137/1.9781611975031.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 844/1108 [15:17<06:01,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download be8f5732e6f9576a1b6988b1f8a5ee2daf9779a6, https://ieeexplore.ieee.org/ielx7/5962385/9931397/09442205.pdf\n",
      "Failed to download beccf5bc709167e483e8ea0f58829c34a2bde2e7, https://www.aclweb.org/anthology/D19-1464.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 845/1108 [15:17<04:36,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download bed87e8fb3e7e9bc87e1c2ee459ae405a35d3267, https://www.aclweb.org/anthology/2020.acl-main.15.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 847/1108 [15:28<11:54,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download bf8fe437f779f2098f9af82b534aa51dc9edb06f, https://www.aclweb.org/anthology/W18-6301.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 852/1108 [15:29<03:15,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c09bc9f1fff9cb4731dc4f83924727fb3cc96e7e, https://dl.acm.org/doi/pdf/10.1145/3386569.3392377\n",
      "Failed to download c156b1b30e3dd9284615e5304f2fb2826c09d0ff, https://dl.acm.org/doi/pdf/10.1145/3534678.3539260\n",
      "Failed to download c17d6fbe63025c7d8bda7fe92756409acdccecc3, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/cogs.13343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 854/1108 [15:30<02:36,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c1e62b537f3d30018e7979a89b0e0f15e2b6eecc, https://academic.oup.com/nar/article-pdf/44/D1/D1075/16661270/gkv1075.pdf\n",
      "Failed to download c20196183fd7d1de3171432edf1d1a987e671678, https://downloads.hindawi.com/journals/aaa/2014/563787.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 856/1108 [15:31<02:15,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c26969f883e44dade1c24270af157482434be20c, http://www.cell.com/article/S0960982217312551/pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 859/1108 [15:35<03:41,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c35ebdef193ef00f36f6b345e181648acd568991, https://journals.sagepub.com/doi/pdf/10.1177/875647939000600106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 864/1108 [15:40<03:35,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c39e54d62140121395230f6f872f56400ea0929f, https://www.aclweb.org/anthology/D18-1390.pdf\n",
      "Failed to download c3b8367a80181e28c95630b9b63060d895de08ff, https://www.aclweb.org/anthology/N15-1092.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 870/1108 [16:01<06:26,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c45362793d10a8d56960b9cd43d1f27980ae5bd4, https://www.aclweb.org/anthology/W16-6109.pdf\n",
      "Failed to download c4744a7c2bb298e4a52289a1e085c71cc3d37bc6, https://www.aclweb.org/anthology/P19-1285.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 872/1108 [16:01<03:42,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c47d24ca17a72e15910dffc6fca5a8708d8ef773, http://www.cell.com/article/S2589004221012347/pdf\n",
      "Failed to download c485b7e7b3f3c7cac63133722b76744db88fa4a9, http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 874/1108 [16:06<06:10,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c620e6fc4a0cb23b744e2c7261cc7cda79431132, https://www.pnas.org/doi/pdf/10.1073/pnas.2018422118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 877/1108 [16:08<04:11,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c6c547e5ecbb8e26bae02fd6f37154145ae0053b, https://www.aclweb.org/anthology/2020.emnlp-main.711.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 879/1108 [16:09<02:57,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c70f7b2f88709755fa9c84443979cca6ea08a15b, http://www.dtic.mil/dtic/tr/fulltext/u2/a188112.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 884/1108 [16:13<02:06,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c8efcc854d97dfc2a42b83316a2109f9d166e43f, https://www.aclweb.org/anthology/N18-2074.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 885/1108 [16:14<02:48,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c9214ebe91454e6369720136ab7dd990d52a07d4, http://dl.acm.org/ft_gateway.cfm?id=1075274&type=pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 887/1108 [16:17<04:20,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c93fd79856450eba642e9bf848264b362980aab6, https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli_a_00462/2062688/coli_a_00462.pdf\n",
      "Failed to download c96ef1c40f509f3864f08fef6949b76f5309452b, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00332/1923190/tacl_a_00332.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 891/1108 [16:20<03:01,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download c9b56cb026a38e39bb0228faac57accd6f65e6f7, https://www.aclweb.org/anthology/2020.emnlp-demos.16.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 894/1108 [16:22<02:36,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ca2f1088d3e581b2c6c75cf0ebc96506d620f64d, https://dl.acm.org/doi/pdf/10.1145/3442188.3445922\n",
      "Failed to download ca352ea6ac66c03a0cc22759098713e4202c71c6, https://www.aclweb.org/anthology/D19-1569.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 898/1108 [16:23<01:17,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download cae24695391e7ef8e7d351a8c922b4016fbfbd02, https://www.aclweb.org/anthology/2020.acl-main.434.pdf\n",
      "Failed to download cb0f3ee1e98faf92429d601cdcd76c69c1e484eb, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00290/1923083/tacl_a_00290.pdf\n",
      "Failed to download cb86c76627d51d38e156ce15046513053f98f53b, https://www.aclweb.org/anthology/D19-5813.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 900/1108 [16:25<01:44,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download cd8ff1960eacf5024af65f8ae3eebc5f57078dcf, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/nav.22074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 905/1108 [16:28<02:02,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ce177672b00ddf46e4906157a7e997ca9338b8b9, https://www.aclweb.org/anthology/D19-1002.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 907/1108 [16:29<01:53,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download cf570e633e6c6929cc807445916d7e520f667554, https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118535561.fmatter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 913/1108 [16:43<06:22,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download cff79255a94b9b05a4ce893eb403a522e0923f04, https://www.aclweb.org/anthology/E17-1038.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 916/1108 [16:45<03:06,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d06e84ac9e912b415719f0e7f3163d59e0a329cd, https://www.aclweb.org/anthology/2020.emnlp-main.156.pdf\n",
      "Failed to download d098bdca6d0335a31fd164c1fac68028784a0737, https://www.aclweb.org/anthology/W18-6401.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 917/1108 [16:45<02:20,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d0cda85c030711aaa5383c80d5928a4d22f8d3bf, https://www.aclweb.org/anthology/2020.acl-main.465.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 919/1108 [16:48<03:26,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d0fbae81d870bbfb34430654f70fd6a21e8bd1cc, https://www.aclweb.org/anthology/N18-2007.pdf\n",
      "Failed to download d150330c1ddd0f09c62c3d705aa59c6f255d3dd6, https://www.mdpi.com/2075-1702/10/8/658/pdf?version=1660038903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 922/1108 [16:49<01:50,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d27669c82faf78ea08cceaa0a171b540cccc304d, https://www.aclweb.org/anthology/2020.emnlp-main.19.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 924/1108 [16:51<02:24,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d2ae43a84b50a9291858d431d42e4c00685289fc, https://www.aclweb.org/anthology/2020.emnlp-main.168.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 925/1108 [16:52<01:55,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d2c733e34d48784a37d717fe43d9e93277a8c53e, http://www.image-net.org/papers/imagenet_cvpr09.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 930/1108 [16:55<01:46,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d47a682723f710395454687319bb55635e653105, https://www.aclweb.org/anthology/2020.acl-main.485.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 931/1108 [16:56<01:34,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d4e8bed3b50a035e1eabad614fe4218a34b3b178, https://dash.harvard.edu/bitstream/1/25104739/1/tr-10-98.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 933/1108 [16:58<02:13,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d5aefe86b1ba8c773a6bd0e84812ace161b8c0db, https://dl.acm.org/doi/pdf/10.1145/3219819.3219947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 938/1108 [16:59<00:55,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d5ddb30bf421bdfdf728b636993dc48b1e879176, http://wexler.free.fr/library/files/elman (1993) learning and development in neural networks. the importance of starting small.pdf\n",
      "Failed to download d64a1588bc3dacfeacf39ff1fa543c6832e051e2, https://dl.acm.org/doi/pdf/10.1145/218380.218498\n",
      "Failed to download d67ec9bebffe787be4c3d04bcb6d8ac4c869406c, https://www.aclweb.org/anthology/N18-1168.pdf\n",
      "Failed to download d69c0ed04ecc852e8c921900d3e7967f74f81263, https://dl.acm.org/doi/pdf/10.1145/3404835.3463238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 940/1108 [16:59<00:41,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea, https://www.aclweb.org/anthology/P19-1355.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 942/1108 [16:59<00:37,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d6cb85f0d532fdf34faa070a41d80e4aa87a643a, http://manuscript.elsevier.com/S0306261922009266/pdf/S0306261922009266.pdf\n",
      "Failed to download d715b4a9282562b9d84fb66e04ee70e66b12e86d, https://www.aclweb.org/anthology/2020.acl-main.39.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 944/1108 [17:00<00:31,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d73561ab8318ce343f5cb15f96c74f210b6b24fa, https://www.aclweb.org/anthology/2020.emnlp-main.446.pdf\n",
      "Failed to download d78aed1dac6656affa4a04cbf225ced11a83d103, https://www.aclweb.org/anthology/D19-1445.pdf\n",
      "Failed to download d7da009f457917aa381619facfa5ffae9329a6e9, https://dl.acm.org/doi/pdf/10.3115/1073083.1073135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 951/1108 [17:07<02:29,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a, http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 955/1108 [17:11<01:51,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download d97cd476bef990352ae921e4c7c5ae1222e9b8da, https://www.aclweb.org/anthology/2020.acl-main.587.pdf\n",
      "Failed to download da43a455e65f8d1fec2ac72932ac2dd6c6ddc20d, https://www.aclweb.org/anthology/2020.findings-emnlp.322.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 956/1108 [17:11<01:24,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download da6ba9f19581bd7e28bc280c53385a1327eb09dd, https://www.aclweb.org/anthology/W19-3905.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 957/1108 [17:12<01:26,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download daae2cdf397ccdc6b4c86b31f61720a956ce23a4, https://dl.acm.org/doi/pdf/10.1145/3339185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 961/1108 [17:14<00:59,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download db9715d147baf3fdd04830cfb7c88e30c07658eb, https://www.aclweb.org/anthology/2020.emnlp-main.701.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 962/1108 [17:14<00:57,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download db9b452f34e5dd6522acaad64e1d0d11868aa5d3, https://ieeexplore.ieee.org/ielx7/6287639/8600701/08675939.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 965/1108 [17:15<00:43,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download dbdfd22ec8b71d48ff100819235eff56a4a374a3, https://www.aclweb.org/anthology/2020.acl-main.331.pdf\n",
      "Failed to download dbeeca8466e0c177ec67c60d529899232415ca87, https://www.aclweb.org/anthology/2020.acl-main.173.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 969/1108 [17:23<03:16,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download dd589899bc03fe2fba72502f5a34b14674a9180b, https://www.aclweb.org/anthology/D17-1289.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 973/1108 [17:49<07:27,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download dde6151c4e82cbfd573e3ec1d1a36af4a1121220, https://dl.acm.org/doi/pdf/10.1145/3394171.3413678\n",
      "Failed to download dde89e64a7f375b90e1cc594142940f4161e1592, https://www.aclweb.org/anthology/D18-1298.pdf\n",
      "Failed to download de794d50713ea5f91a7c9da3d72041e2f5ef8452, http://dl.acm.org/ft_gateway.cfm?id=1654538&type=pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 976/1108 [17:50<03:46,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download df77714269f1f88182092f8535b1bc290fcd835d, https://www.aclweb.org/anthology/D15-1278.pdf\n",
      "Failed to download e01eae8dea6fbaa1ae7fc83535053932268df430, https://dl.acm.org/doi/pdf/10.5555/1699750.1699759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 980/1108 [17:52<02:17,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download e0995bad59c8638ea8c319bb7220c0f0b1ed5dca, https://ieeexplore.ieee.org/ielx7/34/4359286/10496231.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 986/1108 [18:22<06:52,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download e1f037f47d96d10af5c1ab390a246238dd8e1057, https://www.aclweb.org/anthology/P19-1486.pdf\n",
      "Failed to download e2080a3ee52ce4d1239da1b67fe0405f2cfe8d6d, http://dl.acm.org/ft_gateway.cfm?id=1613771&type=pdf\n",
      "Failed to download e20ff55e87e2b3ef02ae0529880bb705f5efbcae, https://www.aclweb.org/anthology/D18-1325.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 987/1108 [18:23<05:14,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download e237ec4e4d60c2d5ec70ebb7ce7a00b2f2fd6f9d, https://www.aclweb.org/anthology/D18-1055.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 992/1108 [18:27<01:47,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download e323f3e86cd766b50ade5d722af810f1ce3d2664, https://www.aclweb.org/anthology/P18-2041.pdf\n",
      "Failed to download e3a477c510d92c6d1dc6903ca0983eea22145435, http://www.cell.com/article/S2211124721001704/pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 993/1108 [18:27<01:19,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1, https://www.aclweb.org/anthology/D18-1458.pdf\n",
      "Failed to download e543cab98e4ad1b5f9d654bf888db7621398bdeb, https://www.pnas.org/doi/pdf/10.1073/pnas.72.2.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 999/1108 [18:39<02:56,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download e6566ece21f6637c515fe9969f9d1ec6cca6d36c, https://www.aclweb.org/anthology/N18-2108.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1004/1108 [18:43<01:15,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e, https://www.aclweb.org/anthology/P18-1007.pdf\n",
      "Failed to download e7f48efb3f4a6ea4dd8117c20156c5660e6323c6, https://dl.acm.org/doi/pdf/10.1145/3186563\n",
      "Failed to download e816f788767eec6a8ef0ea9eddd0e902435d4271, https://www.aclweb.org/anthology/2020.acl-main.740.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1006/1108 [18:43<00:47,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download e850595c3c2e4cbc3d7c3dd9fa5277d4ca77323e, https://www.aclweb.org/anthology/D15-1137.pdf\n",
      "Failed to download e86e81ad3fa4ab0b736f7fef721689e293ee788e, https://www.aclweb.org/anthology/E17-2009.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1011/1108 [18:52<02:29,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download eb606d9ce65139754232cee62f6ab77f3e0c665f, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00313/1923422/tacl_a_00313.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1014/1108 [18:55<01:57,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ebf59587f8f170ff4241c42263bbfb9da5bd2135, https://www.aclweb.org/anthology/P19-1346.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1015/1108 [18:58<02:43,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ec307b17f193b14292206b65a1bcc95bfd8f02ed, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00475/2020694/tacl_a_00475.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1018/1108 [19:00<01:34,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ecced0e4b7d1a1751473637ebb8451bcb286acc2, https://dl.acm.org/doi/pdf/10.5555/1687878.1687909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1021/1108 [19:01<00:56,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ed2b7bfbbf5e48d750d7c3e0046e55341cecd335, https://doi.org/10.18653/v1/n18-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1022/1108 [19:02<01:10,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ed2f7914bf51168842f33618d50f1e5f21622548, http://www.cse.ust.hk/~twinsen/nystrom.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1028/1108 [19:07<01:02,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ee4204e5c0d58006735508e13762e7a6f3491138, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/9781119549833.app1\n",
      "Failed to download ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c, https://www.aclweb.org/anthology/D17-1070.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1030/1108 [19:08<00:47,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ef56afa4cefb90e263b1434dd3ca650904f54c6e, https://www.aclweb.org/anthology/D18-1482.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1032/1108 [19:27<04:58,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f010affab57b5fcf1cd6be23df79d8ec98c7289c, https://www.aclweb.org/anthology/P17-1147.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1034/1108 [19:27<02:35,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f04df4e20a18358ea2f689b4c129781628ef7fc1, https://doi.org/10.18653/v1/d15-1075\n",
      "Failed to download f0f77fe7fc62ec627db40aa8deb40de06cfe8be4, https://www.aclweb.org/anthology/2020.coling-main.1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1035/1108 [19:29<02:26,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f15d89e3127bad5aefb620eb9c5359cf515d7194, http://downloads.hindawi.com/journals/mpe/2018/7987691.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 1038/1108 [19:30<01:14,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f22ac2e3e45debb13d635fddaccfff7e997e7db9, https://www.aclweb.org/anthology/W17-5221.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1040/1108 [19:31<00:58,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f2c17758e74707d379b87372528221656d14b697, https://dl.acm.org/doi/pdf/10.1145/3531146.3533088\n",
      "Failed to download f2e67cff013f2bca28e709f44993acec3472bfdb, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/9781119626879.app8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1045/1108 [19:32<00:23,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f37076f426023241f19cdc2fb0a0fd733a6fa7fa, https://www.aclweb.org/anthology/K16-1028.pdf\n",
      "Failed to download f3b89e9a2b8ce1b6058e6984c3556bc2dded0938, https://www.aclweb.org/anthology/P19-1459.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1047/1108 [19:33<00:19,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f3f90b45f4d9916d65731a1b11e0c2483605c79f, https://www.aclweb.org/anthology/K19-1031.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 1049/1108 [19:34<00:17,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f4238bd2385a52413ccbacfd9e409a650235bd13, https://www.aclweb.org/anthology/P19-1032.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 1051/1108 [19:35<00:29,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f48ae425e2567be2d993efcaaf74c2274fc9d7c5, https://www.aclweb.org/anthology/P19-1470.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1053/1108 [19:41<01:39,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f4b7a72d4e34d767b0c20953c3a124e4b28f2544, https://dl.acm.org/doi/pdf/10.1145/7902.7903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1055/1108 [19:42<01:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f4dff66ba8f2338d118f379f2eff1410feb57ce6, https://dl.acm.org/doi/pdf/10.1145/113446.113449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1059/1108 [19:45<00:42,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f52de7242e574b70410ca6fb70b79c811919fc00, https://dl.acm.org/doi/pdf/10.3115/1220175.1220230\n",
      "Failed to download f53e2ae46470b89cd1ce6e3bf1d60d9c59722ce1, https://www.aclweb.org/anthology/D15-1237.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1061/1108 [19:45<00:27,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f5a7da72496e2ca8edcd9f9123773012c010cfc6, https://www.aclweb.org/anthology/N16-1030.pdf\n",
      "Failed to download f6127bbe33d7e5776d3c313304b35d27e1051459, https://www.aclweb.org/anthology/2020.emnlp-main.83.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1062/1108 [19:46<00:28,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f67afec4226aba674e786698b39b85b124945ddd, https://epubs.siam.org/doi/pdf/10.1137/1.9781611975673.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1064/1108 [19:46<00:18,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f6dac1c52d3b07c993fe52513b8964f86e8fe381, http://conference.scipy.org/proceedings/scipy2010/pdfs/mckinney.pdf\n",
      "Failed to download f72d3f58ff73353978e224af348448b34d27cf7b, https://epubs.siam.org/doi/pdf/10.1137/1.9781611974508.fm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 1067/1108 [19:47<00:14,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f797fd44b9ddd5845611eb7a705ca9464a8819d1, https://www.aclweb.org/anthology/E17-1104.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1074/1108 [19:55<00:22,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f839972c81c8ee454cc66d066785e755a5d42c46, https://www.mdpi.com/2071-1050/14/19/11865/pdf?version=1663900024\n",
      "Failed to download f895cbc2d4a2cd00b4a81cccabc6d9c94b8ddfe4, https://www.aclweb.org/anthology/D15-1106.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1076/1108 [19:58<00:31,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f8b1b43f284f1246ca015cc002ac949bb67c5645, http://www.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-134.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1077/1108 [20:02<00:52,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f8da8c55c0e7c4940a02347347dd232bc2bac0b5, https://dl.acm.org/doi/pdf/10.1145/3467017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 1081/1108 [20:05<00:26,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download f958d4921951e394057a1c4ec33bad9a34e5dad1, https://www.aclweb.org/anthology/P17-1012.pdf\n",
      "Failed to download f9700e31a1d0ae34d4571ab056dfb268c1543349, https://www.aclweb.org/anthology/D19-5409.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 1087/1108 [20:56<02:41,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download fb50f6f4f81f361bf1c6dbc93cc8fab5aee12fdf, https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00366/1924181/tacl_a_00366.pdf\n",
      "Failed to download fbba8629ff9633ca57be1f2209d53d9bcfc2273c, https://www.aclweb.org/anthology/D18-1185.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 1089/1108 [20:57<01:23,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download fc089a09074c84979d1f34e89341318a5bc26d3d, https://www.aclweb.org/anthology/S19-2145.pdf\n",
      "Failed to download fc097d528fd62fe76d73fafbf0c57473b58d1e84, https://www.aclweb.org/anthology/W18-6322.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 1091/1108 [20:57<00:40,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download fc18676ef52caf30004ba7a2ce884b84c2a5e552, https://www.aclweb.org/anthology/D19-6503.pdf\n",
      "Failed to download fc7788e75e2a060e4fd96a4e030eaac77ebebc48, https://www.aclweb.org/anthology/W18-3302.pdf\n",
      "Failed to download fc7c428f604d13604a1d62e8a3e1b393c730791a, https://www.mdpi.com/2313-433X/6/6/41/pdf?version=1592375970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1096/1108 [21:09<00:22,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download fc85469f9ff785f24212a50c58b497de563ae3da, https://www.aclweb.org/anthology/2020.acl-main.343.pdf\n",
      "Failed to download fd23c9168418324e81881365f297fb6a1caa3a07, https://dl.acm.org/doi/pdf/10.1145/214762.214771\n",
      "Failed to download fd4675526ee569196ad1698935b8f5a529b1f9ba, https://www.aclweb.org/anthology/P19-1262.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1097/1108 [21:09<00:16,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299, https://www.aclweb.org/anthology/P18-1027.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1099/1108 [21:10<00:08,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download fdbb252f29ee0b72fc5467c0ae11f7cb30149f46, https://doi.org/10.3115/v1/d14-1\n",
      "Failed to download fdbdd4e0461d23905104460a02a176907d945f44, https://www.aclweb.org/anthology/D18-1317.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1101/1108 [21:10<00:03,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download fe5b2664159211f7a1a6cd959ef20dacae7175d8, https://www.aclweb.org/anthology/D18-1404.pdf\n",
      "Failed to download fed1a48da3d35694da6a75952829fbce5ea232c9, https://www.aclweb.org/anthology/N15-1080.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1103/1108 [21:11<00:01,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download ffa3810fb02bafe60b706c5ee11c185f5519afab, https://ieeexplore.ieee.org/ielx7/6287639/8948470/09123901.pdf\n",
      "Failed to download ffb949d3493c3b2f3c9acf9c75cb03938933ddf0, https://www.aclweb.org/anthology/D17-1215.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1108/1108 [21:16<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download fff100f13441b922c32b43f5628cdca285ec72a1, https://www.aclweb.org/anthology/D18-1208.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib, urllib.request\n",
    "\n",
    "save_dir=U.pjoin(ptree.lib_dir,'..','pdfs2')\n",
    "arxiv_pdf='https://arxiv.org/pdf/{arxiv_id}'\n",
    "U.mkdir(save_dir)\n",
    "\n",
    "# bar=tqdm(ref2_arxiv)\n",
    "# for i in bar:\n",
    "#     bar.set_description(f'Downloading {i}')\n",
    "#     arxiv_id=ref2_arxiv[i]\n",
    "#     download_arxiv_pdf(arxiv_id,save_dir,f'{i}.pdf')\n",
    "\n",
    "failed_pdfs={}\n",
    "for i in tqdm(ref2_pdfs):\n",
    "    url=ref2_pdfs[i]\n",
    "    save_path=U.pjoin(save_dir,f'{i}.pdf')\n",
    "    if not os.path.exists(save_path):\n",
    "        try:\n",
    "            pdf=urllib.request.urlretrieve(url, save_path)\n",
    "        except:\n",
    "            failed_pdfs[i]=url\n",
    "            print(f'Failed to download {i}, {url}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3172/3172 [10:32<00:00,  5.02it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "save_dir=U.pjoin(ptree.lib_dir,'..','htmls2')\n",
    "U.mkdir(save_dir)\n",
    "ar5iv_url='https://ar5iv.labs.arxiv.org/html/{arxiv_id}'\n",
    "\n",
    "for i in tqdm(ref2_arxiv):\n",
    "    DIR=U.pjoin(save_dir,f'{i}.html')\n",
    "    if os.path.exists(DIR):\n",
    "        continue\n",
    "    url=ar5iv_url.format(arxiv_id=ref2_arxiv[i])\n",
    "        \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Save the HTML content to a file\n",
    "        with open(DIR, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response.text)\n",
    "        # print(f\"HTML content saved to {i}.html\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {i}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n",
      "297\n",
      "https://ar5iv.labs.arxiv.org/html/1911.04070 https://arxiv.org/abs/1911.04070 bptrans\n",
      "https://ar5iv.labs.arxiv.org/html/2302.04542 https://arxiv.org/abs/2302.04542 eva\n",
      "https://ar5iv.labs.arxiv.org/html/2312.06635 https://arxiv.org/abs/2312.06635 gla\n",
      "https://ar5iv.labs.arxiv.org/html/2407.09941 https://arxiv.org/abs/2407.09941 hydra\n",
      "https://ar5iv.labs.arxiv.org/html/2302.10866 https://arxiv.org/abs/2302.10866 hyena\n",
      "https://ar5iv.labs.arxiv.org/html/2310.18780 https://arxiv.org/abs/2310.18780 hyenadistill\n",
      "https://ar5iv.labs.arxiv.org/html/2405.21060 https://arxiv.org/abs/2405.21060 mamba2\n",
      "https://ar5iv.labs.arxiv.org/html/2407.10347 https://arxiv.org/abs/2407.10347 mamba4gcn\n",
      "https://ar5iv.labs.arxiv.org/html/2310.01655 https://arxiv.org/abs/2310.01655 polysketchformer\n"
     ]
    }
   ],
   "source": [
    "htmls_dir=U.pjoin(ptree.lib_dir,'..','htmls')\n",
    "pdfs_dir=U.pjoin(ptree.lib_dir,'..','pdfs')\n",
    "\n",
    "htmls=[i.split('.')[0] for i in os.listdir(htmls_dir)]\n",
    "pdfs=[i.split('.')[0] for i in os.listdir(pdfs_dir)]\n",
    "\n",
    "print(len(htmls))\n",
    "print(len(pdfs))\n",
    "\n",
    "arxiv_url='https://arxiv.org/abs/{arxiv_id}'\n",
    "\n",
    "for i in pdfs:\n",
    "    if i not in htmls:\n",
    "        if i in arxiv_ids:\n",
    "            arxiv_id=arxiv_ids[i]\n",
    "            url=ar5iv_url.format(arxiv_id=arxiv_id)\n",
    "            arxiv=arxiv_url.format(arxiv_id=arxiv_id)\n",
    "            print(url,arxiv,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\ChengJunyan1\\\\Research\\\\model_discovery\\\\notebooks\\\\..\\\\model_discovery\\\\model\\\\library\\\\tree\\\\pdfs'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3D%223D-RPE%3A%20Enhancing%20Long-Context%20Modeling%20Through%203D%20Rotary%20Position%20Encoding%22%26id_list%3D%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=\"3D-RPE: Enhancing Long-Context Modeling Through 3D Rotary Position Encoding\"&amp;id_list=&amp;start=0&amp;max_results=1</title>\n",
      "  <id>http://arxiv.org/api/WZ9KoGw6d+aii+09GWS4ZE43lX0</id>\n",
      "  <updated>2024-09-11T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\n",
      "</feed>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib, urllib.request\n",
    "url = 'http://export.arxiv.org/api/query?search_query=\"{query}\"&start=0&max_results=1'\n",
    "title = ptree.G.nodes[refs[0]]['data'].title\n",
    "encoded_title = urllib.parse.quote(title)\n",
    "url=url.format(query=encoded_title)\n",
    "data = urllib.request.urlopen(url)\n",
    "print(data.read().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agents.search_utils \n",
    "importlib.reload(agents.search_utils)\n",
    "\n",
    "from agents.search_utils import SuperScholarSearcher\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dev tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.search_utils\n",
    "importlib.reload(model_discovery.agents.search_utils)\n",
    "\n",
    "from model_discovery.agents.search_utils import SuperScholarSearcher, pwc_search_patched\n",
    "\n",
    "sss=SuperScholarSearcher(ptree,evo_system.stream)\n",
    "\n",
    "# sss._load_libs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Semantic Scholar for \"Transformer\" from NeurIPS, ICML, ICLR, ACL, EMNLP, NAACL after 2010...\n",
      "Searching arXiv for \"Transformer\" in cs.LG, cs.CL...\n",
      "Searching Papers with Code for \"Transformer\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:12:27:55,199 INFO     [_client.py:1038] HTTP Request: GET https://paperswithcode.com/api/v1/search/?page=1&items_per_page=5&q=Transformer \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 related papers\n",
      "*1. Spatial Transformer Networks*\n",
      "Authors: Max Jaderberg, K. Simonyan, Andrew Zisserman, K. Kavukcuoglu\n",
      "TL;DR: This work introduces a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network, and can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps.\n",
      "Abstract: Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.\n",
      "Venue: Neural Information Processing Systems\n",
      "Year: 2015\n",
      "Citations: 6879\n",
      "Influential Citations: 716\n",
      "Semantic Scholar ID: dbb6ded623159c867fbeca0772db7b2eb9489523\n",
      "ArXiv ID: 1506.02025\n",
      "Open Access PDF: None\n",
      "\n",
      "*2. Transformer-XL: Attentive Language Models beyond a Fixed-Length Context*\n",
      "Authors: Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov\n",
      "TL;DR: This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.\n",
      "Abstract: Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.\n",
      "Venue: Annual Meeting of the Association for Computational Linguistics\n",
      "Year: 2019\n",
      "Citations: 3327\n",
      "Influential Citations: 394\n",
      "Semantic Scholar ID: c4744a7c2bb298e4a52289a1e085c71cc3d37bc6\n",
      "ArXiv ID: 1901.02860\n",
      "Open Access PDF: https://www.aclweb.org/anthology/P19-1285.pdf\n",
      "\n",
      "*3. Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting*\n",
      "Authors: Yunhao Zhang, Junchi Yan\n",
      "TL;DR: In Crossformer, a Transformer-based model utilizing cross-dimension dependency for MTS forecasting, the input MTS is embedded into a 2D vector array through the Dimension-Segment-Wise embedding to preserve time and dimension information and the Two-Stage Attention layer is proposed to capture the cross-time and cross- dimension dependency.\n",
      "Abstract: Recently many deep models have been proposed for multivariate time series (MTS) forecasting. In particular, Transformer-based models have shown great potential because they can capture long-term dependency. However, existing Transformer-based models mainly focus on modeling the temporal dependency (cross-time dependency) yet often omit the dependency among different variables (cross-dimension dependency), which is critical for MTS forecasting. To ﬁll the gap, we propose Crossformer, a Transformer-based model utilizing cross-dimension dependency for MTS forecasting. In Crossformer, the input MTS is embedded into a 2D vector array through the Dimension-Segment-Wise (DSW) embedding to preserve time and dimension information. Then the Two-Stage Attention (TSA) layer is proposed to efﬁciently capture the cross-time and cross-dimension dependency. Utilizing DSW embedding and TSA layer, Crossformer establishes a Hierarchical Encoder-Decoder (HED) to use the information at different scales for the ﬁnal forecasting. Extensive experimental results on six real-world datasets show the effectiveness of Crossformer against previous state-of-the-arts.\n",
      "Venue: International Conference on Learning Representations\n",
      "Year: 2023\n",
      "Citations: 202\n",
      "Influential Citations: 39\n",
      "Semantic Scholar ID: fb45d31cc89207aec392dbac8908cc24db2df871\n",
      "ArXiv ID: None\n",
      "Open Access PDF: None\n",
      "\n",
      "*4. mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer*\n",
      "Authors: Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel\n",
      "TL;DR: None\n",
      "Abstract: The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.\n",
      "Venue: North American Chapter of the Association for Computational Linguistics\n",
      "Year: 2020\n",
      "Citations: 1996\n",
      "Influential Citations: 218\n",
      "Semantic Scholar ID: 74276a37bfa50f90dfae37f767b2b67784bd402a\n",
      "ArXiv ID: 2010.11934\n",
      "Open Access PDF: https://aclanthology.org/2021.naacl-main.41.pdf\n",
      "\n",
      "*5. ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision*\n",
      "Authors: Wonjae Kim, Bokyung Son, Ildoo Kim\n",
      "TL;DR: A minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that the authors process textual inputs, showing that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance.\n",
      "Abstract: Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.\n",
      "Venue: International Conference on Machine Learning\n",
      "Year: 2021\n",
      "Citations: 1344\n",
      "Influential Citations: 169\n",
      "Semantic Scholar ID: 0839722fb5369c0abaff8515bfc08299efc790a1\n",
      "ArXiv ID: 2102.03334\n",
      "Open Access PDF: None\n",
      "\n",
      "*6. Reformer: The Efficient Transformer*\n",
      "Authors: Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya\n",
      "TL;DR: This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.\n",
      "Abstract: Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n",
      "Venue: International Conference on Learning Representations\n",
      "Year: 2020\n",
      "Citations: 1944\n",
      "Influential Citations: 224\n",
      "Semantic Scholar ID: 055fd6a9f7293269f1b22c1470e63bd02d8d9500\n",
      "ArXiv ID: 2001.04451\n",
      "Open Access PDF: None\n",
      "\n",
      "*7. RWKV: Reinventing RNNs for the Transformer Era*\n",
      "Authors: Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, G. Kranthikiran, Xuming He, Haowen Hou, Przemyslaw Kazienko, Jan Kocoń, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, J. S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, P. Zhou, Jian Zhu, Rui Zhu\n",
      "TL;DR: This work proposes a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs, and presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n",
      "Abstract: Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.\n",
      "Venue: Conference on Empirical Methods in Natural Language Processing\n",
      "Year: 2023\n",
      "Citations: 291\n",
      "Influential Citations: 31\n",
      "Semantic Scholar ID: 026b3396a63ed5772329708b7580d633bb86bec9\n",
      "ArXiv ID: 2305.13048\n",
      "Open Access PDF: http://arxiv.org/pdf/2305.13048\n",
      "\n",
      "*8. PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis*\n",
      "Authors: Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, Zhenguo Li\n",
      "TL;DR: PIXART-$\\alpha$ is introduced, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards.\n",
      "Abstract: The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PIXART-$\\alpha$, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024px resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that separately optimize pixel dependency, text-image alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PIXART-$\\alpha$'s training speed markedly surpasses existing large-scale T2I models, e.g., PIXART-$\\alpha$ only takes 10.8% of Stable Diffusion v1.5's training time (675 vs. 6,250 A100 GPU days), saving nearly \\$300,000 (\\$26,000 vs. \\$320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%. Extensive experiments demonstrate that PIXART-$\\alpha$ excels in image quality, artistry, and semantic control. We hope PIXART-$\\alpha$ will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.\n",
      "Venue: International Conference on Learning Representations\n",
      "Year: 2023\n",
      "Citations: 123\n",
      "Influential Citations: 26\n",
      "Semantic Scholar ID: 8fafd95a6ffbecf9c1b5f4542ac4b78a00602551\n",
      "ArXiv ID: 2310.00426\n",
      "Open Access PDF: https://arxiv.org/pdf/2310.00426\n",
      "\n",
      "*9. Decision Transformer: Reinforcement Learning via Sequence Modeling*\n",
      "Authors: Lili Chen, Kevin Lu, A. Rajeswaran, Kimin Lee, Aditya Grover, M. Laskin, P. Abbeel, A. Srinivas, Igor Mordatch\n",
      "TL;DR: Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.\n",
      "Abstract: We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.\n",
      "Venue: Neural Information Processing Systems\n",
      "Year: 2021\n",
      "Citations: 1193\n",
      "Influential Citations: 217\n",
      "Semantic Scholar ID: c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500\n",
      "ArXiv ID: 2106.01345\n",
      "Open Access PDF: None\n",
      "\n",
      "*10. NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification*\n",
      "Authors: Qitian Wu, Wentao Zhao, Zenan Li, D. Wipf, Junchi Yan\n",
      "TL;DR: A novel all-pair message passing scheme for efficiently propagating node signals between arbitrary nodes, as an important building block for a pioneering Transformer-style network for node classification on large graphs, dubbed as NodeFormer.\n",
      "Abstract: Graph neural networks have been extensively studied for learning with inter-connected data. Despite this, recent evidence has revealed GNNs' deficiencies related to over-squashing, heterophily, handling long-range dependencies, edge incompleteness and particularly, the absence of graphs altogether. While a plausible solution is to learn new adaptive topology for message passing, issues concerning quadratic complexity hinder simultaneous guarantees for scalability and precision in large networks. In this paper, we introduce a novel all-pair message passing scheme for efficiently propagating node signals between arbitrary nodes, as an important building block for a pioneering Transformer-style network for node classification on large graphs, dubbed as \\textsc{NodeFormer}. Specifically, the efficient computation is enabled by a kernerlized Gumbel-Softmax operator that reduces the algorithmic complexity to linearity w.r.t. node numbers for learning latent graph structures from large, potentially fully-connected graphs in a differentiable manner. We also provide accompanying theory as justification for our design. Extensive experiments demonstrate the promising efficacy of the method in various tasks including node classification on graphs (with up to 2M nodes) and graph-enhanced applications (e.g., image classification) where input graphs are missing.\n",
      "Venue: Neural Information Processing Systems\n",
      "Year: 2023\n",
      "Citations: 124\n",
      "Influential Citations: 12\n",
      "Semantic Scholar ID: 01de6d0c00e7e77050a90945246b2b4acde497a2\n",
      "ArXiv ID: 2306.08385\n",
      "Open Access PDF: http://arxiv.org/pdf/2306.08385\n",
      "\n",
      "*11. WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence\n",
      "  Learning Ability*\n",
      "Authors: Yufan Zhuang, Zihan Wang, Fangbo Tao, Jingbo Shang\n",
      "Abstract: Transformer and its variants are fundamental neural architectures in deep\n",
      "learning. Recent works show that learning attention in the Fourier space can\n",
      "improve the long sequence learning capability of Transformers. We argue that\n",
      "wavelet transform shall be a better choice because it captures both position\n",
      "and frequency information with linear time complexity. Therefore, in this\n",
      "paper, we systematically study the synergy between wavelet transform and\n",
      "Transformers. We propose Wavelet Space Attention (WavSpA) that facilitates\n",
      "attention learning in a learnable wavelet coefficient space which replaces the\n",
      "attention in Transformers by (1) applying forward wavelet transform to project\n",
      "the input sequences to multi-resolution bases, (2) conducting attention\n",
      "learning in the wavelet coefficient space, and (3) reconstructing the\n",
      "representation in input space via backward wavelet transform. Extensive\n",
      "experiments on the Long Range Arena demonstrate that learning attention in the\n",
      "wavelet space using either fixed or adaptive wavelets can consistently improve\n",
      "Transformer's performance and also significantly outperform learning in Fourier\n",
      "space. We further show our method can enhance Transformer's reasoning\n",
      "extrapolation capability over distance on the LEGO chain-of-reasoning task.\n",
      "Published: 2022-10-05T02:37:59Z\n",
      "Updated: 2023-05-22T22:42:47Z\n",
      "ArXiv ID: 2210.01989\n",
      "\n",
      "*12. N-ODE Transformer: A Depth-Adaptive Variant of the Transformer Using\n",
      "  Neural Ordinary Differential Equations*\n",
      "Authors: Aaron Baier-Reinio, Hans De Sterck\n",
      "Abstract: We use neural ordinary differential equations to formulate a variant of the\n",
      "Transformer that is depth-adaptive in the sense that an input-dependent number\n",
      "of time steps is taken by the ordinary differential equation solver. Our goal\n",
      "in proposing the N-ODE Transformer is to investigate whether its\n",
      "depth-adaptivity may aid in overcoming some specific known theoretical\n",
      "limitations of the Transformer in handling nonlocal effects. Specifically, we\n",
      "consider the simple problem of determining the parity of a binary sequence, for\n",
      "which the standard Transformer has known limitations that can only be overcome\n",
      "by using a sufficiently large number of layers or attention heads. We find,\n",
      "however, that the depth-adaptivity of the N-ODE Transformer does not provide a\n",
      "remedy for the inherently nonlocal nature of the parity problem, and provide\n",
      "explanations for why this is so. Next, we pursue regularization of the N-ODE\n",
      "Transformer by penalizing the arclength of the ODE trajectories, but find that\n",
      "this fails to improve the accuracy or efficiency of the N-ODE Transformer on\n",
      "the challenging parity problem. We suggest future avenues of research for\n",
      "modifications and extensions of the N-ODE Transformer that may lead to improved\n",
      "accuracy and efficiency for sequence modelling tasks such as neural machine\n",
      "translation.\n",
      "Published: 2020-10-22T00:48:24Z\n",
      "Updated: 2020-10-22T00:48:24Z\n",
      "ArXiv ID: 2010.11358\n",
      "\n",
      "*13. Trees in transformers: a theoretical analysis of the Transformer's\n",
      "  ability to represent trees*\n",
      "Authors: Qi He, João Sedoc, Jordan Rodu\n",
      "Abstract: Transformer networks are the de facto standard architecture in natural\n",
      "language processing. To date, there are no theoretical analyses of the\n",
      "Transformer's ability to capture tree structures. We focus on the ability of\n",
      "Transformer networks to learn tree structures that are important for tree\n",
      "transduction problems. We first analyze the theoretical capability of the\n",
      "standard Transformer architecture to learn tree structures given enumeration of\n",
      "all possible tree backbones, which we define as trees without labels. We then\n",
      "prove that two linear layers with ReLU activation function can recover any tree\n",
      "backbone from any two nonzero, linearly independent starting backbones. This\n",
      "implies that a Transformer can learn tree structures well in theory. We conduct\n",
      "experiments with synthetic data and find that the standard Transformer achieves\n",
      "similar accuracy compared to a Transformer where tree position information is\n",
      "explicitly encoded, albeit with slower convergence. This confirms empirically\n",
      "that Transformers can learn tree structures.\n",
      "Published: 2021-12-16T00:02:02Z\n",
      "Updated: 2021-12-16T00:02:02Z\n",
      "ArXiv ID: 2112.11913\n",
      "\n",
      "*14. Transformer on a Diet*\n",
      "Authors: Chenguang Wang, Zihao Ye, Aston Zhang, Zheng Zhang, Alexander J. Smola\n",
      "Abstract: Transformer has been widely used thanks to its ability to capture sequence\n",
      "information in an efficient way. However, recent developments, such as BERT and\n",
      "GPT-2, deliver only heavy architectures with a focus on effectiveness. In this\n",
      "paper, we explore three carefully-designed light Transformer architectures to\n",
      "figure out whether the Transformer with less computations could produce\n",
      "competitive results. Experimental results on language model benchmark datasets\n",
      "hint that such trade-off is promising, and the light Transformer reduces 70%\n",
      "parameters at best, while obtains competitive perplexity compared to standard\n",
      "Transformer. The source code is publicly available.\n",
      "Published: 2020-02-14T18:41:58Z\n",
      "Updated: 2020-02-14T18:41:58Z\n",
      "ArXiv ID: 2002.06170\n",
      "\n",
      "*15. FNetAR: Mixing Tokens with Autoregressive Fourier Transforms*\n",
      "Authors: Tim Lou, Michael Park, Mohammad Ramezanali, Vincent Tang\n",
      "Abstract: In this note we examine the autoregressive generalization of the FNet\n",
      "algorithm, in which self-attention layers from the standard Transformer\n",
      "architecture are substituted with a trivial sparse-uniformsampling procedure\n",
      "based on Fourier transforms. Using the Wikitext-103 benchmark, we\n",
      "demonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the\n",
      "task of causal language modelingcompared to a Transformer-XL baseline (24.2\n",
      "ppl) with only half the number self-attention layers,thus providing further\n",
      "evidence for the superfluity of deep neural networks with heavily\n",
      "compoundedattention mechanisms. The autoregressive Fourier transform could\n",
      "likely be used for parameterreduction on most Transformer-based time-series\n",
      "prediction models.\n",
      "Published: 2021-07-22T21:24:02Z\n",
      "Updated: 2021-07-22T21:24:02Z\n",
      "ArXiv ID: 2107.10932\n",
      "\n",
      "*16. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows*\n",
      "Authors: Baining Guo, Stephen Lin, Zheng Zhang, Yixuan Wei, Han Hu, Yue Cao, Yutong Lin, Ze Liu\n",
      "Abstract: This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\\url{https://github.com/microsoft/Swin-Transformer}.\n",
      "Conference: None\n",
      "Published: 2021-03-25\n",
      "PaperswithCode ID: swin-transformer-hierarchical-vision\n",
      "ArXiv ID: 2103.14030\n",
      "Open Access PDF: https://arxiv.org/pdf/2103.14030v2.pdf\n",
      "Repository: \n",
      "  Owner: huggingface\n",
      "  Name: transformers\n",
      "  Description: 🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.\n",
      "  Stars: 131873\n",
      "\n",
      "*17. Neighborhood Attention Transformer*\n",
      "Authors: Humphrey Shi, Shen Li, Jiachen Li, Steven Walton, Ali Hassani\n",
      "Abstract: We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9% ImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size. To support more research based on sliding-window attention, we open source our project and release our checkpoints at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer .\n",
      "Conference: None\n",
      "Published: 2022-04-14\n",
      "PaperswithCode ID: neighborhood-attention-transformer\n",
      "ArXiv ID: 2204.07143\n",
      "Open Access PDF: https://arxiv.org/pdf/2204.07143v5.pdf\n",
      "Repository: \n",
      "  Owner: huggingface\n",
      "  Name: transformers\n",
      "  Description: 🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.\n",
      "  Stars: 131873\n",
      "\n",
      "*18. Dilated Neighborhood Attention Transformer*\n",
      "Authors: Humphrey Shi, Ali Hassani\n",
      "Abstract: Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\n",
      "Conference: None\n",
      "Published: 2022-09-29\n",
      "PaperswithCode ID: dilated-neighborhood-attention-transformer\n",
      "ArXiv ID: 2209.15001\n",
      "Open Access PDF: https://arxiv.org/pdf/2209.15001v3.pdf\n",
      "Repository: \n",
      "  Owner: huggingface\n",
      "  Name: transformers\n",
      "  Description: 🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.\n",
      "  Stars: 131873\n",
      "\n",
      "*19. Longformer: The Long-Document Transformer*\n",
      "Authors: Iz Beltagy, Matthew E. Peters, Arman Cohan\n",
      "Abstract: Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.\n",
      "Conference: None\n",
      "Published: 2020-04-10\n",
      "PaperswithCode ID: longformer-the-long-document-transformer\n",
      "ArXiv ID: 2004.05150\n",
      "Open Access PDF: https://arxiv.org/pdf/2004.05150v2.pdf\n",
      "Repository: \n",
      "  Owner: huggingface\n",
      "  Name: transformers\n",
      "  Description: 🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.\n",
      "  Stars: 131873\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ret=sss.search_external('Transformer')\n",
    "print(sss.pretty_print(ret))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paper': {'id': 'swin-transformer-hierarchical-vision',\n",
       "   'arxiv_id': '2103.14030',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2103.14030v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2103.14030v2.pdf',\n",
       "   'title': 'Swin Transformer: Hierarchical Vision Transformer using Shifted Windows',\n",
       "   'abstract': 'This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \\\\textbf{S}hifted \\\\textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\\\\url{https://github.com/microsoft/Swin-Transformer}.',\n",
       "   'authors': ['Baining Guo',\n",
       "    'Stephen Lin',\n",
       "    'Zheng Zhang',\n",
       "    'Yixuan Wei',\n",
       "    'Han Hu',\n",
       "    'Yue Cao',\n",
       "    'Yutong Lin',\n",
       "    'Ze Liu'],\n",
       "   'published': '2021-03-25',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'http://openaccess.thecvf.com//content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html',\n",
       "   'conference_url_pdf': 'http://openaccess.thecvf.com//content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf',\n",
       "   'proceeding': 'iccv-2021-1'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'neighborhood-attention-transformer',\n",
       "   'arxiv_id': '2204.07143',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2204.07143v5',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2204.07143v5.pdf',\n",
       "   'title': 'Neighborhood Attention Transformer',\n",
       "   'abstract': \"We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9% ImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size. To support more research based on sliding-window attention, we open source our project and release our checkpoints at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer .\",\n",
       "   'authors': ['Humphrey Shi',\n",
       "    'Shen Li',\n",
       "    'Jiachen Li',\n",
       "    'Steven Walton',\n",
       "    'Ali Hassani'],\n",
       "   'published': '2022-04-14',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2023/html/Hassani_Neighborhood_Attention_Transformer_CVPR_2023_paper.html',\n",
       "   'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2023/papers/Hassani_Neighborhood_Attention_Transformer_CVPR_2023_paper.pdf',\n",
       "   'proceeding': 'cvpr-2023-1'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'reformer-the-efficient-transformer-1',\n",
       "   'arxiv_id': '2001.04451',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2001.04451v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2001.04451v2.pdf',\n",
       "   'title': 'Reformer: The Efficient Transformer',\n",
       "   'abstract': 'Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.',\n",
       "   'authors': ['Łukasz Kaiser', 'Anselm Levskaya', 'Nikita Kitaev'],\n",
       "   'published': '2020-01-13',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'https://openreview.net/forum?id=rkgNKkHtvB',\n",
       "   'conference_url_pdf': 'https://openreview.net/pdf?id=rkgNKkHtvB',\n",
       "   'proceeding': 'iclr-2020-1'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'dilated-neighborhood-attention-transformer',\n",
       "   'arxiv_id': '2209.15001',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2209.15001v3',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2209.15001v3.pdf',\n",
       "   'title': 'Dilated Neighborhood Attention Transformer',\n",
       "   'abstract': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\",\n",
       "   'authors': ['Humphrey Shi', 'Ali Hassani'],\n",
       "   'published': '2022-09-29',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'longformer-the-long-document-transformer',\n",
       "   'arxiv_id': '2004.05150',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2004.05150v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2004.05150v2.pdf',\n",
       "   'title': 'Longformer: The Long-Document Transformer',\n",
       "   'abstract': \"Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.\",\n",
       "   'authors': ['Iz Beltagy', 'Matthew E. Peters', 'Arman Cohan'],\n",
       "   'published': '2020-04-10',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'vision-transformers-for-dense-prediction',\n",
       "   'arxiv_id': '2103.13413',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2103.13413v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2103.13413v1.pdf',\n",
       "   'title': 'Vision Transformers for Dense Prediction',\n",
       "   'abstract': 'We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.',\n",
       "   'authors': ['Vladlen Koltun', 'Alexey Bochkovskiy', 'René Ranftl'],\n",
       "   'published': '2021-03-24',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'http://openaccess.thecvf.com//content/ICCV2021/html/Ranftl_Vision_Transformers_for_Dense_Prediction_ICCV_2021_paper.html',\n",
       "   'conference_url_pdf': 'http://openaccess.thecvf.com//content/ICCV2021/papers/Ranftl_Vision_Transformers_for_Dense_Prediction_ICCV_2021_paper.pdf',\n",
       "   'proceeding': 'iccv-2021-1'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'end-to-end-object-detection-with-transformers',\n",
       "   'arxiv_id': '2005.12872',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2005.12872v3',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2005.12872v3.pdf',\n",
       "   'title': 'End-to-End Object Detection with Transformers',\n",
       "   'abstract': 'We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.',\n",
       "   'authors': ['Sergey Zagoruyko',\n",
       "    'Nicolas Usunier',\n",
       "    'Gabriel Synnaeve',\n",
       "    'Alexander Kirillov',\n",
       "    'Francisco Massa',\n",
       "    'Nicolas Carion'],\n",
       "   'published': '2020-05-26',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/832_ECCV_2020_paper.php',\n",
       "   'conference_url_pdf': 'https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf',\n",
       "   'proceeding': 'eccv-2020-8'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'big-bird-transformers-for-longer-sequences',\n",
       "   'arxiv_id': '2007.14062',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2007.14062v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2007.14062v2.pdf',\n",
       "   'title': 'Big Bird: Transformers for Longer Sequences',\n",
       "   'abstract': 'Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.',\n",
       "   'authors': ['Anirudh Ravula',\n",
       "    'Santiago Ontanon',\n",
       "    'Manzil Zaheer',\n",
       "    'Chris Alberti',\n",
       "    'Avinava Dubey',\n",
       "    'Philip Pham',\n",
       "    'Joshua Ainslie',\n",
       "    'Amr Ahmed',\n",
       "    'Qifan Wang',\n",
       "    'Li Yang',\n",
       "    'Guru Guruganesh'],\n",
       "   'published': '2020-07-28',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'http://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html',\n",
       "   'conference_url_pdf': 'http://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf',\n",
       "   'proceeding': 'neurips-2020-12'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'cvt-introducing-convolutions-to-vision',\n",
       "   'arxiv_id': '2103.15808',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2103.15808v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2103.15808v1.pdf',\n",
       "   'title': 'CvT: Introducing Convolutions to Vision Transformers',\n",
       "   'abstract': 'We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both designs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (\\\\ie shift, scale, and distortion invariance) while maintaining the merits of Transformers (\\\\ie dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (\\\\eg ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\\\\% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely removed in our model, simplifying the design for higher resolution vision tasks. Code will be released at \\\\url{https://github.com/leoxiaobin/CvT}.',\n",
       "   'authors': ['Lei Zhang',\n",
       "    'Lu Yuan',\n",
       "    'Xiyang Dai',\n",
       "    'Mengchen Liu',\n",
       "    'Noel Codella',\n",
       "    'Bin Xiao',\n",
       "    'Haiping Wu'],\n",
       "   'published': '2021-03-29',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'http://openaccess.thecvf.com//content/ICCV2021/html/Wu_CvT_Introducing_Convolutions_to_Vision_Transformers_ICCV_2021_paper.html',\n",
       "   'conference_url_pdf': 'http://openaccess.thecvf.com//content/ICCV2021/papers/Wu_CvT_Introducing_Convolutions_to_Vision_Transformers_ICCV_2021_paper.pdf',\n",
       "   'proceeding': 'iccv-2021-1'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'roformer-enhanced-transformer-with-rotary',\n",
       "   'arxiv_id': '2104.09864',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2104.09864v5',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2104.09864v5.pdf',\n",
       "   'title': 'RoFormer: Enhanced Transformer with Rotary Position Embedding',\n",
       "   'abstract': 'Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \\\\url{https://huggingface.co/docs/transformers/model_doc/roformer}.',\n",
       "   'authors': ['Ahmed Murtadha',\n",
       "    'Yunfeng Liu',\n",
       "    'Bo Wen',\n",
       "    'Shengfeng Pan',\n",
       "    'Yu Lu',\n",
       "    'Jianlin Su'],\n",
       "   'published': '2021-04-20',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'beit-bert-pre-training-of-image-transformers',\n",
       "   'arxiv_id': '2106.08254',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2106.08254v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2106.08254v2.pdf',\n",
       "   'title': 'BEiT: BERT Pre-Training of Image Transformers',\n",
       "   'abstract': 'We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first \"tokenize\" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit.',\n",
       "   'authors': ['Furu Wei', 'Songhao Piao', 'Li Dong', 'Hangbo Bao'],\n",
       "   'published': '2021-06-15',\n",
       "   'conference': 'beit-bert-pre-training-of-image-transformers-1',\n",
       "   'conference_url_abs': 'https://openreview.net/forum?id=p-BhZSz59o4',\n",
       "   'conference_url_pdf': 'https://openreview.net/pdf?id=p-BhZSz59o4',\n",
       "   'proceeding': 'iclr-2022-4'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'segformer-simple-and-efficient-design-for',\n",
       "   'arxiv_id': '2105.15203',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2105.15203v3',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2105.15203v3.pdf',\n",
       "   'title': 'SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers',\n",
       "   'abstract': 'We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.',\n",
       "   'authors': ['Ping Luo',\n",
       "    'Jose M. Alvarez',\n",
       "    'Anima Anandkumar',\n",
       "    'Zhiding Yu',\n",
       "    'Wenhai Wang',\n",
       "    'Enze Xie'],\n",
       "   'published': '2021-05-31',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'http://proceedings.neurips.cc/paper/2021/hash/64f1f27bf1b4ec22924fd0acb550c235-Abstract.html',\n",
       "   'conference_url_pdf': 'http://proceedings.neurips.cc/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf',\n",
       "   'proceeding': 'neurips-2021-12'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'levit-a-vision-transformer-in-convnet-s',\n",
       "   'arxiv_id': '2104.01136',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2104.01136v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2104.01136v2.pdf',\n",
       "   'title': \"LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\",\n",
       "   'abstract': 'We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT',\n",
       "   'authors': ['Matthijs Douze',\n",
       "    'Hervé Jégou',\n",
       "    'Armand Joulin',\n",
       "    'Pierre Stock',\n",
       "    'Hugo Touvron',\n",
       "    'Alaaeldin El-Nouby',\n",
       "    'Ben Graham'],\n",
       "   'published': '2021-04-02',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'http://openaccess.thecvf.com//content/ICCV2021/html/Graham_LeViT_A_Vision_Transformer_in_ConvNets_Clothing_for_Faster_Inference_ICCV_2021_paper.html',\n",
       "   'conference_url_pdf': 'http://openaccess.thecvf.com//content/ICCV2021/papers/Graham_LeViT_A_Vision_Transformer_in_ConvNets_Clothing_for_Faster_Inference_ICCV_2021_paper.pdf',\n",
       "   'proceeding': 'iccv-2021-1'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'masked-attention-mask-transformer-for',\n",
       "   'arxiv_id': '2112.01527',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2112.01527v3',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2112.01527v3.pdf',\n",
       "   'title': 'Masked-attention Mask Transformer for Universal Image Segmentation',\n",
       "   'abstract': 'Image segmentation is about grouping pixels with different semantics, e.g., category or instance membership, where each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).',\n",
       "   'authors': ['Rohit Girdhar',\n",
       "    'Alexander Kirillov',\n",
       "    'Alexander G. Schwing',\n",
       "    'Ishan Misra',\n",
       "    'Bowen Cheng'],\n",
       "   'published': '2021-12-02',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2022/html/Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.html',\n",
       "   'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2022/papers/Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.pdf',\n",
       "   'proceeding': 'cvpr-2022-1'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'mt5-a-massively-multilingual-pre-trained-text',\n",
       "   'arxiv_id': '2010.11934',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2010.11934v3',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2010.11934v3.pdf',\n",
       "   'title': 'mT5: A massively multilingual pre-trained text-to-text transformer',\n",
       "   'abstract': 'The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent \"accidental translation\" in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.',\n",
       "   'authors': ['Colin Raffel',\n",
       "    'Aditya Barua',\n",
       "    'Aditya Siddhant',\n",
       "    'Rami Al-Rfou',\n",
       "    'Mihir Kale',\n",
       "    'Adam Roberts',\n",
       "    'Noah Constant',\n",
       "    'Linting Xue'],\n",
       "   'published': '2020-10-22',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'https://aclanthology.org/2021.naacl-main.41',\n",
       "   'conference_url_pdf': 'https://aclanthology.org/2021.naacl-main.41.pdf',\n",
       "   'proceeding': 'naacl-2021-4'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'an-image-is-worth-16x16-words-transformers-1',\n",
       "   'arxiv_id': '2010.11929',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2010.11929v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2010.11929v2.pdf',\n",
       "   'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale',\n",
       "   'abstract': 'While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.',\n",
       "   'authors': ['Neil Houlsby',\n",
       "    'Jakob Uszkoreit',\n",
       "    'Sylvain Gelly',\n",
       "    'Georg Heigold',\n",
       "    'Matthias Minderer',\n",
       "    'Mostafa Dehghani',\n",
       "    'Thomas Unterthiner',\n",
       "    'Xiaohua Zhai',\n",
       "    'Dirk Weissenborn',\n",
       "    'Alexander Kolesnikov',\n",
       "    'Lucas Beyer',\n",
       "    'Alexey Dosovitskiy'],\n",
       "   'published': '2020-10-22',\n",
       "   'conference': 'an-image-is-worth-16x16-words-transformers',\n",
       "   'conference_url_abs': 'https://openreview.net/forum?id=YicbFdNTTy',\n",
       "   'conference_url_pdf': 'https://openreview.net/pdf?id=YicbFdNTTy',\n",
       "   'proceeding': 'iclr-2021-1'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'dit-self-supervised-pre-training-for-document',\n",
       "   'arxiv_id': '2203.02378',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2203.02378v3',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2203.02378v3.pdf',\n",
       "   'title': 'DiT: Self-supervised Pre-training for Document Image Transformer',\n",
       "   'abstract': 'Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose \\\\textbf{DiT}, a self-supervised pre-trained \\\\textbf{D}ocument \\\\textbf{I}mage \\\\textbf{T}ransformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human-labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, table detection as well as text detection for OCR. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 $\\\\rightarrow$ 92.69), document layout analysis (91.0 $\\\\rightarrow$ 94.9), table detection (94.23 $\\\\rightarrow$ 96.55) and text detection for OCR (93.07 $\\\\rightarrow$ 94.29). The code and pre-trained models are publicly available at \\\\url{https://aka.ms/msdit}.',\n",
       "   'authors': ['Furu Wei',\n",
       "    'Cha Zhang',\n",
       "    'Lei Cui',\n",
       "    'Tengchao Lv',\n",
       "    'Yiheng Xu',\n",
       "    'Junlong Li'],\n",
       "   'published': '2022-03-04',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'vilt-vision-and-language-transformer-without',\n",
       "   'arxiv_id': '2102.03334',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2102.03334v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2102.03334v2.pdf',\n",
       "   'title': 'ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision',\n",
       "   'abstract': 'Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.',\n",
       "   'authors': ['Ildoo Kim', 'Bokyung Son', 'Wonjae Kim'],\n",
       "   'published': '2021-02-05',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'lxmert-learning-cross-modality-encoder',\n",
       "   'arxiv_id': '1908.07490',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/1908.07490v3',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/1908.07490v3.pdf',\n",
       "   'title': 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers',\n",
       "   'abstract': 'Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results; and also present several attention visualizations for the different encoders. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert',\n",
       "   'authors': ['Mohit Bansal', 'Hao Tan'],\n",
       "   'published': '2019-08-20',\n",
       "   'conference': 'lxmert-learning-cross-modality-encoder-1',\n",
       "   'conference_url_abs': 'https://aclanthology.org/D19-1514',\n",
       "   'conference_url_pdf': 'https://aclanthology.org/D19-1514.pdf',\n",
       "   'proceeding': 'ijcnlp-2019-11'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'training-data-efficient-image-transformers',\n",
       "   'arxiv_id': '2012.12877',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2012.12877v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2012.12877v2.pdf',\n",
       "   'title': 'Training data-efficient image transformers & distillation through attention',\n",
       "   'abstract': 'Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.',\n",
       "   'authors': ['Hervé Jégou',\n",
       "    'Alexandre Sablayrolles',\n",
       "    'Francisco Massa',\n",
       "    'Matthijs Douze',\n",
       "    'Matthieu Cord',\n",
       "    'Hugo Touvron'],\n",
       "   'published': '2020-12-23',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'vitmatte-boosting-image-matting-with',\n",
       "   'arxiv_id': '2305.15272',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2305.15272v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2305.15272v2.pdf',\n",
       "   'title': 'ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers',\n",
       "   'abstract': 'Recently, plain vision Transformers (ViTs) have shown impressive performance on various computer vision tasks, thanks to their strong modeling capacity and large-scale pretraining. However, they have not yet conquered the problem of image matting. We hypothesize that image matting could also be boosted by ViTs and present a new efficient and robust ViT-based matting system, named ViTMatte. Our method utilizes (i) a hybrid attention mechanism combined with a convolution neck to help ViTs achieve an excellent performance-computation trade-off in matting tasks. (ii) Additionally, we introduce the detail capture module, which just consists of simple lightweight convolutions to complement the detailed information required by matting. To the best of our knowledge, ViTMatte is the first work to unleash the potential of ViT on image matting with concise adaptation. It inherits many superior properties from ViT to matting, including various pretraining strategies, concise architecture design, and flexible inference strategies. We evaluate ViTMatte on Composition-1k and Distinctions-646, the most commonly used benchmark for image matting, our method achieves state-of-the-art performance and outperforms prior matting works by a large margin.',\n",
       "   'authors': ['Baoyuan Wang',\n",
       "    'Shusheng Yang',\n",
       "    'Xinggang Wang',\n",
       "    'Jingfeng Yao'],\n",
       "   'published': '2023-05-24',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'bert-pre-training-of-deep-bidirectional',\n",
       "   'arxiv_id': '1810.04805',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/1810.04805v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/1810.04805v2.pdf',\n",
       "   'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "   'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).',\n",
       "   'authors': ['Ming-Wei Chang',\n",
       "    'Kenton Lee',\n",
       "    'Jacob Devlin',\n",
       "    'Kristina Toutanova'],\n",
       "   'published': '2018-10-11',\n",
       "   'conference': 'bert-pre-training-of-deep-bidirectional-1',\n",
       "   'conference_url_abs': 'https://aclanthology.org/N19-1423',\n",
       "   'conference_url_pdf': 'https://aclanthology.org/N19-1423.pdf',\n",
       "   'proceeding': 'naacl-2019-6'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'transformer-xl-attentive-language-models',\n",
       "   'arxiv_id': '1901.02860',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/1901.02860v3',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/1901.02860v3.pdf',\n",
       "   'title': 'Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context',\n",
       "   'abstract': 'Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.',\n",
       "   'authors': ['Jaime Carbonell',\n",
       "    'Quoc V. Le',\n",
       "    'Ruslan Salakhutdinov',\n",
       "    'Zihang Dai',\n",
       "    'Zhilin Yang',\n",
       "    'Yiming Yang'],\n",
       "   'published': '2019-01-09',\n",
       "   'conference': 'transformer-xl-attentive-language-models-1',\n",
       "   'conference_url_abs': 'https://aclanthology.org/P19-1285',\n",
       "   'conference_url_pdf': 'https://aclanthology.org/P19-1285.pdf',\n",
       "   'proceeding': 'acl-2019-7'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'funnel-transformer-filtering-out-sequential',\n",
       "   'arxiv_id': '2006.03236',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2006.03236v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2006.03236v1.pdf',\n",
       "   'title': 'Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing',\n",
       "   'abstract': 'With the success of language pretraining, it is highly desirable to develop more efficient architectures of good scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further improve the model capacity. In addition, to perform token-level predictions as required by common pretraining objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading comprehension. The code and pretrained checkpoints are available at https://github.com/laiguokun/Funnel-Transformer.',\n",
       "   'authors': ['Quoc V. Le', 'Zihang Dai', 'Yiming Yang', 'Guokun Lai'],\n",
       "   'published': '2020-06-05',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'http://proceedings.neurips.cc/paper/2020/hash/2cd2915e69546904e4e5d4a2ac9e1652-Abstract.html',\n",
       "   'conference_url_pdf': 'http://proceedings.neurips.cc/paper/2020/file/2cd2915e69546904e4e5d4a2ac9e1652-Paper.pdf',\n",
       "   'proceeding': 'neurips-2020-12'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'trocr-transformer-based-optical-character',\n",
       "   'arxiv_id': '2109.10282',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2109.10282v5',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2109.10282v5.pdf',\n",
       "   'title': 'TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models',\n",
       "   'abstract': 'Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at \\\\url{https://aka.ms/trocr}.',\n",
       "   'authors': ['Yijuan Lu',\n",
       "    'Lei Cui',\n",
       "    'Jingye Chen',\n",
       "    'Furu Wei',\n",
       "    'Zhoujun Li',\n",
       "    'Cha Zhang',\n",
       "    'Dinei Florencio',\n",
       "    'Tengchao Lv',\n",
       "    'Minghao Li'],\n",
       "   'published': '2021-09-21',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'leveraging-redundancy-in-attention-with-reuse-1',\n",
       "   'arxiv_id': '2110.06821',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2110.06821v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2110.06821v1.pdf',\n",
       "   'title': 'Leveraging redundancy in attention with Reuse Transformers',\n",
       "   'abstract': 'Pairwise dot product-based attention allows Transformers to exchange information between tokens in an input-dependent way, and is key to their success across diverse applications in language and vision. However, a typical Transformer model computes such pairwise attention scores repeatedly for the same sequence, in multiple heads in multiple layers. We systematically analyze the empirical similarity of these scores across heads and layers and find them to be considerably redundant, especially adjacent layers showing high similarity. Motivated by these findings, we propose a novel architecture that reuses attention scores computed in one layer in multiple subsequent layers. Experiments on a number of standard benchmarks show that reusing attention delivers performance equivalent to or better than standard transformers, while reducing both compute and memory usage.',\n",
       "   'authors': ['Sanjiv Kumar',\n",
       "    'Yin-Wen Chang',\n",
       "    'Frederick Liu',\n",
       "    'Himanshu Jain',\n",
       "    'Michal Lukasik',\n",
       "    'Andreas Veit',\n",
       "    'Ayan Chakrabarti',\n",
       "    'Srinadh Bhojanapalli'],\n",
       "   'published': '2021-10-13',\n",
       "   'conference': 'leveraging-redundancy-in-attention-with-reuse',\n",
       "   'conference_url_abs': 'https://openreview.net/forum?id=V37YFd_fFgN',\n",
       "   'conference_url_pdf': 'https://openreview.net/pdf?id=V37YFd_fFgN',\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/tensorflow/models/blob/master/official/nlp/modeling/layers/reuse_transformer.py',\n",
       "   'owner': 'tensorflow',\n",
       "   'name': 'models',\n",
       "   'description': 'Models and examples built with TensorFlow',\n",
       "   'stars': 76945,\n",
       "   'framework': 'tf'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'an-attention-free-transformer-1',\n",
       "   'arxiv_id': '2105.14103',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2105.14103v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2105.14103v2.pdf',\n",
       "   'title': 'An Attention Free Transformer',\n",
       "   'abstract': 'We introduce Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.',\n",
       "   'authors': ['Josh Susskind',\n",
       "    'Ruixiang Zhang',\n",
       "    'Hanlin Goh',\n",
       "    'Chen Huang',\n",
       "    'Nitish Srivastava',\n",
       "    'Walter Talbott',\n",
       "    'Shuangfei Zhai'],\n",
       "   'published': '2021-05-28',\n",
       "   'conference': 'an-attention-free-transformer',\n",
       "   'conference_url_abs': 'https://openreview.net/forum?id=pW--cu2FCHY',\n",
       "   'conference_url_pdf': 'https://openreview.net/pdf?id=pW--cu2FCHY',\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/labmlai/annotated_deep_learning_paper_implementations',\n",
       "   'owner': 'labmlai',\n",
       "   'name': 'annotated_deep_learning_paper_implementations',\n",
       "   'description': '🧑\\u200d🏫 60+ Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, ... 🧠',\n",
       "   'stars': 53703,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'you-only-look-at-one-sequence-rethinking',\n",
       "   'arxiv_id': '2106.00666',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2106.00666v3',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2106.00666v3.pdf',\n",
       "   'title': 'You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection',\n",
       "   'abstract': 'Can Transformer perform 2D object- and region-level recognition from a pure sequence-to-sequence perspective with minimal knowledge about the 2D spatial structure? To answer this question, we present You Only Look at One Sequence (YOLOS), a series of object detection models based on the vanilla Vision Transformer with the fewest possible modifications, region priors, as well as inductive biases of the target task. We find that YOLOS pre-trained on the mid-sized ImageNet-1k dataset only can already achieve quite competitive performance on the challenging COCO object detection benchmark, e.g., YOLOS-Base directly adopted from BERT-Base architecture can obtain 42.0 box AP on COCO val. We also discuss the impacts as well as limitations of current pre-train schemes and model scaling strategies for Transformer in vision through YOLOS. Code and pre-trained models are available at https://github.com/hustvl/YOLOS.',\n",
       "   'authors': ['Wenyu Liu',\n",
       "    'Jianwei Niu',\n",
       "    'Rui Wu',\n",
       "    'Jiyang Qi',\n",
       "    'Jiemin Fang',\n",
       "    'Xinggang Wang',\n",
       "    'Bencheng Liao',\n",
       "    'Yuxin Fang'],\n",
       "   'published': '2021-06-01',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'http://proceedings.neurips.cc/paper/2021/hash/dc912a253d1e9ba40e2c597ed2376640-Abstract.html',\n",
       "   'conference_url_pdf': 'http://proceedings.neurips.cc/paper/2021/file/dc912a253d1e9ba40e2c597ed2376640-Paper.pdf',\n",
       "   'proceeding': 'neurips-2021-12'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'lilt-a-simple-yet-effective-language',\n",
       "   'arxiv_id': '2202.13669',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2202.13669v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2202.13669v1.pdf',\n",
       "   'title': 'LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding',\n",
       "   'abstract': 'Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing. However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited. To address this issue, we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding. LiLT can be pre-trained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding off-the-shelf monolingual/multilingual pre-trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure. Code and model are publicly available at https://github.com/jpWang/LiLT.',\n",
       "   'authors': ['Kai Ding', 'Lianwen Jin', 'Jiapeng Wang'],\n",
       "   'published': '2022-02-28',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'https://aclanthology.org/2022.acl-long.534',\n",
       "   'conference_url_pdf': 'https://aclanthology.org/2022.acl-long.534.pdf',\n",
       "   'proceeding': 'acl-2022-5'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'glu-variants-improve-transformer',\n",
       "   'arxiv_id': '2002.05202',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2002.05202v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2002.05202v1.pdf',\n",
       "   'title': 'GLU Variants Improve Transformer',\n",
       "   'abstract': 'Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.',\n",
       "   'authors': ['Noam Shazeer'],\n",
       "   'published': '2020-02-12',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/labmlai/annotated_deep_learning_paper_implementations',\n",
       "   'owner': 'labmlai',\n",
       "   'name': 'annotated_deep_learning_paper_implementations',\n",
       "   'description': '🧑\\u200d🏫 60+ Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, ... 🧠',\n",
       "   'stars': 53703,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'deepnet-scaling-transformers-to-1000-layers',\n",
       "   'arxiv_id': '2203.00555',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2203.00555v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2203.00555v1.pdf',\n",
       "   'title': 'DeepNet: Scaling Transformers to 1,000 Layers',\n",
       "   'abstract': 'In this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a new normalization function (DeepNorm) to modify the residual connection in Transformer, accompanying with theoretically derived initialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making DeepNorm a preferred alternative. We successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is one order of magnitude deeper than previous deep Transformers. Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction.',\n",
       "   'authors': ['Furu Wei',\n",
       "    'Dongdong Zhang',\n",
       "    'Shaohan Huang',\n",
       "    'Li Dong',\n",
       "    'Shuming Ma',\n",
       "    'Hongyu Wang'],\n",
       "   'published': '2022-03-01',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/labmlai/annotated_deep_learning_paper_implementations',\n",
       "   'owner': 'labmlai',\n",
       "   'name': 'annotated_deep_learning_paper_implementations',\n",
       "   'description': '🧑\\u200d🏫 60+ Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, ... 🧠',\n",
       "   'stars': 53703,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'fnet-mixing-tokens-with-fourier-transforms',\n",
       "   'arxiv_id': '2105.03824',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2105.03824v4',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2105.03824v4.pdf',\n",
       "   'title': 'FNet: Mixing Tokens with Fourier Transforms',\n",
       "   'abstract': 'We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with standard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text classification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.',\n",
       "   'authors': ['Santiago Ontanon',\n",
       "    'Ilya Eckstein',\n",
       "    'Joshua Ainslie',\n",
       "    'James Lee-Thorp'],\n",
       "   'published': '2021-05-09',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'https://aclanthology.org/2022.naacl-main.319',\n",
       "   'conference_url_pdf': 'https://aclanthology.org/2022.naacl-main.319.pdf',\n",
       "   'proceeding': 'naacl-2022-7'},\n",
       "  'repository': {'url': 'https://github.com/labmlai/annotated_deep_learning_paper_implementations',\n",
       "   'owner': 'labmlai',\n",
       "   'name': 'annotated_deep_learning_paper_implementations',\n",
       "   'description': '🧑\\u200d🏫 60+ Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, ... 🧠',\n",
       "   'stars': 53703,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'biogpt-generative-pre-trained-transformer-for',\n",
       "   'arxiv_id': '2210.10341',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2210.10341v3',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2210.10341v3.pdf',\n",
       "   'title': 'BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining',\n",
       "   'abstract': 'Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms. Code is available at https://github.com/microsoft/BioGPT.',\n",
       "   'authors': ['Tie-Yan Liu',\n",
       "    'Hoifung Poon',\n",
       "    'Sheng Zhang',\n",
       "    'Tao Qin',\n",
       "    'Yingce Xia',\n",
       "    'Liai Sun',\n",
       "    'Renqian Luo'],\n",
       "   'published': '2022-10-19',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'compressive-transformers-for-long-range-1',\n",
       "   'arxiv_id': '1911.05507',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/1911.05507v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/1911.05507v1.pdf',\n",
       "   'title': 'Compressive Transformers for Long-Range Sequence Modelling',\n",
       "   'abstract': 'We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.',\n",
       "   'authors': ['Siddhant M. Jayakumar',\n",
       "    'Anna Potapenko',\n",
       "    'Jack W. Rae',\n",
       "    'Timothy P. Lillicrap'],\n",
       "   'published': '2019-11-13',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'https://openreview.net/forum?id=SylKikSYDH',\n",
       "   'conference_url_pdf': 'https://openreview.net/pdf?id=SylKikSYDH',\n",
       "   'proceeding': 'iclr-2020-1'},\n",
       "  'repository': {'url': 'https://github.com/labmlai/annotated_deep_learning_paper_implementations',\n",
       "   'owner': 'labmlai',\n",
       "   'name': 'annotated_deep_learning_paper_implementations',\n",
       "   'description': '🧑\\u200d🏫 60+ Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, ... 🧠',\n",
       "   'stars': 53703,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'a-simple-single-scale-vision-transformer-for',\n",
       "   'arxiv_id': '2112.09747',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2112.09747v3',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2112.09747v3.pdf',\n",
       "   'title': 'A Simple Single-Scale Vision Transformer for Object Localization and Instance Segmentation',\n",
       "   'abstract': \"This work presents a simple vision transformer design as a strong baseline for object localization and instance segmentation tasks. Transformers recently demonstrate competitive performance in image classification tasks. To adopt ViT to object detection and dense prediction tasks, many works inherit the multistage design from convolutional networks and highly customized ViT architectures. Behind this design, the goal is to pursue a better trade-off between computational cost and effective aggregation of multiscale global contexts. However, existing works adopt the multistage architectural design as a black-box solution without a clear understanding of its true benefits. In this paper, we comprehensively study three architecture design choices on ViT -- spatial reduction, doubled channels, and multiscale features -- and demonstrate that a vanilla ViT architecture can fulfill this goal without handcrafting multiscale features, maintaining the original ViT design philosophy. We further complete a scaling rule to optimize our model's trade-off on accuracy and computation cost / model size. By leveraging a constant feature resolution and hidden size throughout the encoder blocks, we propose a simple and compact ViT architecture called Universal Vision Transformer (UViT) that achieves strong performance on COCO object detection and instance segmentation tasks.\",\n",
       "   'authors': ['Denny Zhou',\n",
       "    'Zhangyang Wang',\n",
       "    'Xiaodan Song',\n",
       "    'Jing Li',\n",
       "    'Huizhong Chen',\n",
       "    'Tsung-Yi Lin',\n",
       "    'Xiaohua Zhai',\n",
       "    'Lucas Beyer',\n",
       "    'Fan Yang',\n",
       "    'Xianzhi Du',\n",
       "    'Wuyang Chen'],\n",
       "   'published': '2021-12-17',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/tensorflow/models',\n",
       "   'owner': 'tensorflow',\n",
       "   'name': 'models',\n",
       "   'description': 'Models and examples built with TensorFlow',\n",
       "   'stars': 76938,\n",
       "   'framework': 'tf'},\n",
       "  'is_official': True},\n",
       " {'paper': {'id': 'accessing-higher-level-representations-in',\n",
       "   'arxiv_id': '2002.09402',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2002.09402v3',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2002.09402v3.pdf',\n",
       "   'title': 'Addressing Some Limitations of Transformers with Feedback Memory',\n",
       "   'abstract': 'Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.',\n",
       "   'authors': ['Edouard Grave',\n",
       "    'Armand Joulin',\n",
       "    'Angela Fan',\n",
       "    'Thibaut Lavril',\n",
       "    'Sainbayar Sukhbaatar'],\n",
       "   'published': '2020-02-21',\n",
       "   'conference': 'addressing-some-limitations-of-transformers',\n",
       "   'conference_url_abs': 'https://openreview.net/forum?id=OCm0rwa1lx1',\n",
       "   'conference_url_pdf': 'https://openreview.net/pdf?id=OCm0rwa1lx1',\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/labmlai/annotated_deep_learning_paper_implementations',\n",
       "   'owner': 'labmlai',\n",
       "   'name': 'annotated_deep_learning_paper_implementations',\n",
       "   'description': '🧑\\u200d🏫 60+ Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, ... 🧠',\n",
       "   'stars': 53703,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'primer-searching-for-efficient-transformers',\n",
       "   'arxiv_id': '2109.08668',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2109.08668v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2109.08668v2.pdf',\n",
       "   'title': 'Primer: Searching for Efficient Transformers for Language Modeling',\n",
       "   'abstract': \"Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer TensorFlow program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer's improvements can be mostly attributed to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention. Experiments show Primer's gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility.\",\n",
       "   'authors': ['Quoc V. Le',\n",
       "    'Noam Shazeer',\n",
       "    'Zihang Dai',\n",
       "    'Hanxiao Liu',\n",
       "    'Wojciech Mańke',\n",
       "    'David R. So'],\n",
       "   'published': '2021-09-17',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/labmlai/annotated_deep_learning_paper_implementations',\n",
       "   'owner': 'labmlai',\n",
       "   'name': 'annotated_deep_learning_paper_implementations',\n",
       "   'description': '🧑\\u200d🏫 60+ Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, ... 🧠',\n",
       "   'stars': 53703,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'hierarchical-transformers-are-more-efficient',\n",
       "   'arxiv_id': '2110.13711',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2110.13711v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2110.13711v2.pdf',\n",
       "   'title': 'Hierarchical Transformers Are More Efficient Language Models',\n",
       "   'abstract': 'Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences which allows them to produce long coherent outputs: full paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.',\n",
       "   'authors': ['Henryk Michalewski',\n",
       "    'Christian Szegedy',\n",
       "    'Yuhuai Wu',\n",
       "    'Łukasz Kaiser',\n",
       "    'Michał Tyrolski',\n",
       "    'Szymon Tworkowski',\n",
       "    'Piotr Nawrot'],\n",
       "   'published': '2021-10-26',\n",
       "   'conference': 'hierarchical-transformers-are-more-efficient-1',\n",
       "   'conference_url_abs': 'https://aclanthology.org/2022.findings-naacl.117',\n",
       "   'conference_url_pdf': 'https://aclanthology.org/2022.findings-naacl.117.pdf',\n",
       "   'proceeding': 'findings-naacl-2022-7'},\n",
       "  'repository': {'url': 'https://github.com/labmlai/annotated_deep_learning_paper_implementations',\n",
       "   'owner': 'labmlai',\n",
       "   'name': 'annotated_deep_learning_paper_implementations',\n",
       "   'description': '🧑\\u200d🏫 60+ Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, ... 🧠',\n",
       "   'stars': 53703,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'transformers-state-of-the-art-natural-1',\n",
       "   'arxiv_id': None,\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://aclanthology.org/2020.emnlp-demos.6',\n",
       "   'url_pdf': 'https://aclanthology.org/2020.emnlp-demos.6.pdf',\n",
       "   'title': 'Transformers: State-of-the-Art Natural Language Processing',\n",
       "   'abstract': 'Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.',\n",
       "   'authors': ['Alexander Rush',\n",
       "    'Quentin Lhoest',\n",
       "    'Mariama Drame',\n",
       "    'Sylvain Gugger',\n",
       "    'Teven Le Scao',\n",
       "    'Canwen Xu',\n",
       "    'Julien Plu',\n",
       "    'Yacine Jernite',\n",
       "    'Clara Ma',\n",
       "    'Patrick von Platen',\n",
       "    'Sam Shleifer',\n",
       "    'Joe Davison',\n",
       "    'Morgan Funtowicz',\n",
       "    'Remi Louf',\n",
       "    'Tim Rault',\n",
       "    'Pierric Cistac',\n",
       "    'Anthony Moi',\n",
       "    'Clement Delangue',\n",
       "    'Julien Chaumond',\n",
       "    'Victor Sanh',\n",
       "    'Lysandre Debut',\n",
       "    'Thomas Wolf'],\n",
       "   'published': '2020-10-01',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': 'emnlp-2020-11'},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': True},\n",
       " {'paper': {'id': 'transformers-state-of-the-art-natural',\n",
       "   'arxiv_id': '1910.03771',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/1910.03771v5',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/1910.03771v5.pdf',\n",
       "   'title': \"HuggingFace's Transformers: State-of-the-art Natural Language Processing\",\n",
       "   'abstract': 'Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\\\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\\\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\\\url{https://github.com/huggingface/transformers}.',\n",
       "   'authors': ['Alexander M. Rush',\n",
       "    'Julien Plu',\n",
       "    'Joe Davison',\n",
       "    'Rémi Louf',\n",
       "    'Clement Delangue',\n",
       "    'Yacine Jernite',\n",
       "    'Sam Shleifer',\n",
       "    'Morgan Funtowicz',\n",
       "    'Mariama Drame',\n",
       "    'Lysandre Debut',\n",
       "    'Julien Chaumond',\n",
       "    'Teven Le Scao',\n",
       "    'Clara Ma',\n",
       "    'Canwen Xu',\n",
       "    'Anthony Moi',\n",
       "    'Victor Sanh',\n",
       "    'Tim Rault',\n",
       "    'Thomas Wolf',\n",
       "    'Quentin Lhoest',\n",
       "    'Sylvain Gugger',\n",
       "    'Pierric Cistac',\n",
       "    'Patrick von Platen'],\n",
       "   'published': '2019-10-09',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': {'url': 'https://github.com/huggingface/transformers',\n",
       "   'owner': 'huggingface',\n",
       "   'name': 'transformers',\n",
       "   'description': '🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.',\n",
       "   'stars': 131873,\n",
       "   'framework': 'pytorch'},\n",
       "  'is_official': True},\n",
       " {'paper': {'id': 'transformerfusion-monocular-rgb-scene',\n",
       "   'arxiv_id': '2107.02191',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2107.02191v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2107.02191v1.pdf',\n",
       "   'title': 'TransformerFusion: Monocular RGB Scene Reconstruction using Transformers',\n",
       "   'abstract': 'We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation. Key to our approach is the transformer architecture that enables the network to learn to attend to the most relevant image frames for each 3D location in the scene, supervised only by the scene reconstruction task. Features are fused in a coarse-to-fine fashion, storing fine-level features only where needed, requiring lower memory storage and enabling fusion at interactive rates. The feature grid is then decoded to a higher-resolution scene reconstruction, using an MLP-based surface occupancy prediction from interpolated coarse-to-fine 3D features. Our approach results in an accurate surface reconstruction, outperforming state-of-the-art multi-view stereo depth estimation methods, fully-convolutional 3D reconstruction approaches, and approaches using LSTM- or GRU-based recurrent networks for video sequence fusion.',\n",
       "   'authors': ['Matthias Nießner',\n",
       "    'Angela Dai',\n",
       "    'Justus Thies',\n",
       "    'Pablo Palafox',\n",
       "    'Aljaž Božič'],\n",
       "   'published': '2021-07-05',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'http://proceedings.neurips.cc/paper/2021/hash/0a87257e5308197df43230edf4ad1dae-Abstract.html',\n",
       "   'conference_url_pdf': 'http://proceedings.neurips.cc/paper/2021/file/0a87257e5308197df43230edf4ad1dae-Paper.pdf',\n",
       "   'proceeding': 'neurips-2021-12'},\n",
       "  'repository': None,\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'transformerg2g-adaptive-time-stepping-for',\n",
       "   'arxiv_id': '2307.02588',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2307.02588v2',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2307.02588v2.pdf',\n",
       "   'title': 'TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers',\n",
       "   'abstract': 'Dynamic graph embedding has emerged as a very effective technique for addressing diverse temporal graph analytic tasks (i.e., link prediction, node classification, recommender systems, anomaly detection, and graph generation) in various applications. Such temporal graphs exhibit heterogeneous transient dynamics, varying time intervals, and highly evolving node features throughout their evolution. Hence, incorporating long-range dependencies from the historical graph context plays a crucial role in accurately learning their temporal dynamics. In this paper, we develop a graph embedding model with uncertainty quantification, TransformerG2G, by exploiting the advanced transformer encoder to first learn intermediate node representations from its current state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ is the length of context). Moreover, we employ two projection layers to generate lower-dimensional multivariate Gaussian distributions as each node\\'s latent embedding at timestamp $t$. We consider diverse benchmarks with varying levels of ``novelty\" as measured by the TEA (Temporal Edge Appearance) plots. Our experiments demonstrate that the proposed TransformerG2G model outperforms conventional multi-step methods and our prior work (DynG2G) in terms of both link prediction accuracy and computational efficiency, especially for high degree of novelty. Furthermore, the learned time-dependent attention weights across multiple graph snapshots reveal the development of an automatic adaptive time stepping enabled by the transformer. Importantly, by examining the attention weights, we can uncover temporal dependencies, identify influential elements, and gain insights into the complex interactions within the graph structure. For example, we identified a strong correlation between attention weights and node degree at the various stages of the graph topology evolution.',\n",
       "   'authors': ['George Em Karniadakis',\n",
       "    'Mengjia Xu',\n",
       "    'Aniruddha Bora',\n",
       "    'Alan John Varghese'],\n",
       "   'published': '2023-07-05',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': None,\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'transformer-dissection-an-unified',\n",
       "   'arxiv_id': '1908.11775',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/1908.11775v4',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/1908.11775v4.pdf',\n",
       "   'title': \"Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel\",\n",
       "   'abstract': \"Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction. At the core of the Transformer is the attention mechanism, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of attention via the lens of the kernel. To be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer's attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer's attention. As an example, we propose a new variant of Transformer's attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.\",\n",
       "   'authors': ['Louis-Philippe Morency',\n",
       "    'Makoto Yamada',\n",
       "    'Yao-Hung Hubert Tsai',\n",
       "    'Shaojie Bai',\n",
       "    'Ruslan Salakhutdinov'],\n",
       "   'published': '2019-08-30',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': 'emnlp-2019-11'},\n",
       "  'repository': None,\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'trees-in-transformers-a-theoretical-analysis',\n",
       "   'arxiv_id': '2112.11913',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2112.11913v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2112.11913v1.pdf',\n",
       "   'title': \"Trees in transformers: a theoretical analysis of the Transformer's ability to represent trees\",\n",
       "   'abstract': \"Transformer networks are the de facto standard architecture in natural language processing. To date, there are no theoretical analyses of the Transformer's ability to capture tree structures. We focus on the ability of Transformer networks to learn tree structures that are important for tree transduction problems. We first analyze the theoretical capability of the standard Transformer architecture to learn tree structures given enumeration of all possible tree backbones, which we define as trees without labels. We then prove that two linear layers with ReLU activation function can recover any tree backbone from any two nonzero, linearly independent starting backbones. This implies that a Transformer can learn tree structures well in theory. We conduct experiments with synthetic data and find that the standard Transformer achieves similar accuracy compared to a Transformer where tree position information is explicitly encoded, albeit with slower convergence. This confirms empirically that Transformers can learn tree structures.\",\n",
       "   'authors': ['Jordan Rodu', 'João Sedoc', 'Qi He'],\n",
       "   'published': '2021-12-16',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': None,\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'transformer-dissection-an-unified-1',\n",
       "   'arxiv_id': None,\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://aclanthology.org/D19-1443',\n",
       "   'url_pdf': 'https://aclanthology.org/D19-1443.pdf',\n",
       "   'title': \"Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel\",\n",
       "   'abstract': \"Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction. At the core of the Transformer is the attention mechanism, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of attention via the lens of the kernel. To be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer{'}s attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer{'}s attention. As an example, we propose a new variant of Transformer{'}s attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.\",\n",
       "   'authors': ['Louis-Philippe Morency',\n",
       "    'Yao-Hung Hubert Tsai',\n",
       "    'Makoto Yamada',\n",
       "    'Shaojie Bai',\n",
       "    'Ruslan Salakhutdinov'],\n",
       "   'published': '2019-11-01',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': 'ijcnlp-2019-11'},\n",
       "  'repository': None,\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'transformerfam-feedback-attention-is-working',\n",
       "   'arxiv_id': '2404.09173',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2404.09173v3',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2404.09173v3.pdf',\n",
       "   'title': 'TransformerFAM: Feedback attention is working memory',\n",
       "   'abstract': 'While Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length.',\n",
       "   'authors': ['Pedro Moreno Mengibar',\n",
       "    'Khe Chai Sim',\n",
       "    'Zhuoyuan Huo',\n",
       "    'Weiran Wang',\n",
       "    'Dongseong Hwang'],\n",
       "   'published': '2024-04-14',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': None,\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'scratching-visual-transformer-s-back-with',\n",
       "   'arxiv_id': '2210.08457',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2210.08457v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2210.08457v1.pdf',\n",
       "   'title': \"Scratching Visual Transformer's Back with Uniform Attention\",\n",
       "   'abstract': 'The favorable performance of Vision Transformers (ViTs) is often attributed to the multi-head self-attention (MSA). The MSA enables global interactions at each layer of a ViT model, which is a contrasting feature against Convolutional Neural Networks (CNNs) that gradually increase the range of interaction across multiple layers. We study the role of the density of the attention. Our preliminary analyses suggest that the spatial interactions of attention maps are close to dense interactions rather than sparse ones. This is a curious phenomenon, as dense attention maps are harder for the model to learn due to steeper softmax gradients around them. We interpret this as a strong preference for ViT models to include dense interaction. We thus manually insert the uniform attention to each layer of ViT models to supply the much needed dense interactions. We call this method Context Broadcasting, CB. We observe that the inclusion of CB reduces the degree of density in the original attention maps and increases both the capacity and generalizability of the ViT models. CB incurs negligible costs: 1 line in your model code, no additional parameters, and minimal extra operations.',\n",
       "   'authors': ['Tae-Hyun Oh',\n",
       "    'Seong Joon Oh',\n",
       "    'Doonyoon Han',\n",
       "    'Byeongho Heo',\n",
       "    'Kim Yu-Ji',\n",
       "    'Nam Hyeon-Woo'],\n",
       "   'published': '2022-10-16',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': 'http://openaccess.thecvf.com//content/ICCV2023/html/Hyeon-Woo_Scratching_Visual_Transformers_Back_with_Uniform_Attention_ICCV_2023_paper.html',\n",
       "   'conference_url_pdf': 'http://openaccess.thecvf.com//content/ICCV2023/papers/Hyeon-Woo_Scratching_Visual_Transformers_Back_with_Uniform_Attention_ICCV_2023_paper.pdf',\n",
       "   'proceeding': 'iccv-2023-1'},\n",
       "  'repository': None,\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'advancing-transformer-s-capabilities-in',\n",
       "   'arxiv_id': '2310.06803',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2310.06803v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2310.06803v1.pdf',\n",
       "   'title': \"Advancing Transformer's Capabilities in Commonsense Reasoning\",\n",
       "   'abstract': 'Recent advances in general purpose pre-trained language models have shown great potential in commonsense reasoning. However, current works still perform poorly on standard commonsense reasoning benchmarks including the Com2Sense Dataset. We argue that this is due to a disconnect with current cutting-edge machine learning methods. In this work, we aim to bridge the gap by introducing current ML-based methods to improve general purpose pre-trained language models in the task of commonsense reasoning. Specifically, we experiment with and systematically evaluate methods including knowledge transfer, model ensemble, and introducing an additional pairwise contrastive objective. Our best model outperforms the strongest previous works by ~15\\\\% absolute gains in Pairwise Accuracy and ~8.7\\\\% absolute gains in Standard Accuracy.',\n",
       "   'authors': ['Yulun Wu', 'Hanyu Zhou', 'Yunqiu Han', 'Yu Zhou'],\n",
       "   'published': '2023-10-10',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': None,\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'transformer-tricks-removing-weights-for',\n",
       "   'arxiv_id': '2404.12362',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2404.12362v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2404.12362v1.pdf',\n",
       "   'title': 'Transformer tricks: Removing weights for skipless transformers',\n",
       "   'abstract': 'He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the V and P (post-attention projection) linear layers, which reduces the total number of weights. However, this scheme is only applicable to MHA (multi-head attention), but not for MQA (multi-query attention) and GQA (grouped-query attention). The latter schemes are used by many popular LLMs such as Llama 2, Mistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes mathematically equivalent versions that are suitable for MQA and GQA. For example, removing Q and P from a skipless version of Mistral-7B would remove 15% of its weights (and thus reduce its compute and memory complexity). See arXiv:2402.13388 and https://github.com/OpenMachine-ai/transformer-tricks for code and more transformer tricks.',\n",
       "   'authors': ['Nils Graef'],\n",
       "   'published': '2024-04-18',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': None,\n",
       "  'is_official': False},\n",
       " {'paper': {'id': 'exploring-transformer-s-potential-on',\n",
       "   'arxiv_id': '2204.03898',\n",
       "   'nips_id': None,\n",
       "   'url_abs': 'https://arxiv.org/abs/2204.03898v1',\n",
       "   'url_pdf': 'https://arxiv.org/pdf/2204.03898v1.pdf',\n",
       "   'title': \"Exploring Transformer's potential on automatic piano transcription\",\n",
       "   'abstract': \"Most recent research about automatic music transcription (AMT) uses convolutional neural networks and recurrent neural networks to model the mapping from music signals to symbolic notation. Based on a high-resolution piano transcription system, we explore the possibility of incorporating another powerful sequence transformation tool -- the Transformer -- to deal with the AMT problem. We argue that the properties of the Transformer make it more suitable for certain AMT subtasks. We confirm the Transformer's superiority on the velocity detection task by experiments on the MAESTRO dataset and a cross-dataset evaluation on the MAPS dataset. We observe a performance improvement on both frame-level and note-level metrics after introducing the Transformer network.\",\n",
       "   'authors': ['Ye Wang',\n",
       "    'Jiqing Han',\n",
       "    'Emmanouil Benetos',\n",
       "    'Ziyi Guo',\n",
       "    'Longshen Ou'],\n",
       "   'published': '2022-04-08',\n",
       "   'conference': None,\n",
       "   'conference_url_abs': None,\n",
       "   'conference_url_pdf': None,\n",
       "   'proceeding': None},\n",
       "  'repository': None,\n",
       "  'is_official': False}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
