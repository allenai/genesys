{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test search utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\prompts\\prompts.py:1473: SyntaxWarning: invalid escape sequence '\\('\n",
      "  GUM_DESIGN_PROPOSER_SYSTEM_prompt = \"\"\"\n",
      "c:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\prompts\\prompts.py:1698: SyntaxWarning: invalid escape sequence '\\('\n",
      "  GUM_PROPOSAL_REVIEWER_SYSTEM_prompt = \"\"\"\n",
      "c:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\prompts\\prompts.py:2325: SyntaxWarning: invalid escape sequence '\\('\n",
      "  GUM_DESIGNER_SYSTEM_prompt_part4 = \"\"\"\n",
      "c:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\prompts\\prompts.py:2631: SyntaxWarning: invalid escape sequence '\\('\n",
      "  GUM_IMPLEMENTATION_REVIEWER_SYSTEM_prompt = \"\"\"\n",
      "2024-09-13:14:26:36,627 INFO     [model_agent.py:184] Agent name=`designer`, model_details={\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"temperature\": 0.3\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized dataset wikitext-2 from C:\\ChengJunyan1\\Research\\model_discovery\\data\\wikitext-2\\tokenized\\meta-llama/Llama-2-7b-hf\\2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13:14:26:37,678 INFO     [model_agent.py:184] Agent name=`debugger`, model_details={\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-13:14:26:37,936 INFO     [model_agent.py:184] Agent name=`claude`, model_details={\n",
      "    \"model_name\": \"gpt-4o\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evoname': 'test_evo_000', 'scales': '14M,31M,70M', 'selection_ratio': '0.25', 'select_method': 'random', 'design_budget': '0'}\n",
      "Evolution system initialized with scales: ['14M', '31M', '70M']\n",
      "Current scale: 0\n",
      "Budgets remaining: {'70M': 1, '31M': 4, '14M': 16}\n",
      "Checkpoint directory: C:\\ChengJunyan1\\Research\\model_discovery\\ckpt\\test_evo_000\n",
      "gpt2 tree loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13:14:26:39,252 INFO     [model_agent.py:184] Agent name=`designer`, model_details={\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"temperature\": 0.3\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized dataset wikitext-2 from C:\\ChengJunyan1\\Research\\model_discovery\\data\\wikitext-2\\tokenized\\meta-llama/Llama-2-7b-hf\\2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13:14:26:39,928 INFO     [model_agent.py:184] Agent name=`debugger`, model_details={\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-13:14:26:40,172 INFO     [model_agent.py:184] Agent name=`claude`, model_details={\n",
      "    \"model_name\": \"gpt-4o\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SuperScholarSearcher' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     17\u001b[0m paper_detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://api.semanticscholar.org/graph/v1/paper/\u001b[39m\u001b[38;5;132;01m{paper_id}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     20\u001b[0m strparams\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevoname=test_evo_000\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscales=14M,31M,70M\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesign_budget=0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     26\u001b[0m ]\n\u001b[1;32m---> 27\u001b[0m evo_system \u001b[38;5;241m=\u001b[39m \u001b[43mBuildEvolution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cache_type='diskcache',\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m ptree \u001b[38;5;241m=\u001b[39m evo_system\u001b[38;5;241m.\u001b[39mptree\n",
      "File \u001b[1;32mc:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\evolution.py:1064\u001b[0m, in \u001b[0;36mBuildEvolution\u001b[1;34m(config, stream, **kwargs)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Factory for loading evolution system \u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \n\u001b[0;32m   1059\u001b[0m \u001b[38;5;124;03m:param config: \u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;124;03m    Configuration object (optional) \u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \n\u001b[0;32m   1062\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevolution\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1064\u001b[0m evolution \u001b[38;5;241m=\u001b[39m \u001b[43mNativeBuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m   1066\u001b[0m     evolution\u001b[38;5;241m.\u001b[39mlink_stream(stream)\n",
      "File \u001b[1;32mc:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\exec_utils\\factory.py:122\u001b[0m, in \u001b[0;36mBuildSystem\u001b[1;34m(config, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Factory method for building groups. It will build\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03mthe configuration on the fly if not provided.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m \n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m config,updated_kwargs \u001b[38;5;241m=\u001b[39m _check_config(config,kwargs) \n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRegistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mupdated_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\exec_utils\\register.py:233\u001b[0m, in \u001b[0;36mRegistry.build_model\u001b[1;34m(cls, resource_type, config, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_cache \u001b[38;5;129;01mand\u001b[39;00m cache_method:\n\u001b[0;32m    231\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_caching\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 233\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mresource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mno_caching \u001b[38;5;129;01mand\u001b[39;00m cache_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     cache_info \u001b[38;5;241m=\u001b[39m create_cache(\n\u001b[0;32m    238\u001b[0m         obj\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    239\u001b[0m         cached_method\u001b[38;5;241m=\u001b[39mcache_method,\n\u001b[0;32m    240\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    241\u001b[0m         manual_cache\u001b[38;5;241m=\u001b[39mdo_cache,\n\u001b[0;32m    242\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\evolution.py:1050\u001b[0m, in \u001b[0;36mEvolutionSystem.from_config\u001b[1;34m(cls, config, **kwargs)\u001b[0m\n\u001b[0;32m   1045\u001b[0m config\u001b[38;5;241m.\u001b[39msystem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_discovery_system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1046\u001b[0m agent \u001b[38;5;241m=\u001b[39m BuildSystem(\n\u001b[0;32m   1047\u001b[0m     config,\n\u001b[0;32m   1048\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1049\u001b[0m )\n\u001b[1;32m-> 1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\evolution.py:777\u001b[0m, in \u001b[0;36mEvolutionSystem.__init__\u001b[1;34m(self, agent_system, config, **kwargs)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m PrintSystem(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config)\n\u001b[1;32m--> 777\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\evolution.py:833\u001b[0m, in \u001b[0;36mEvolutionSystem.load\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptree\u001b[38;5;241m=\u001b[39mPhylogeneticTree(U\u001b[38;5;241m.\u001b[39mpjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevo_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnd_agent \u001b[38;5;241m=\u001b[39m BuildSystem(\n\u001b[0;32m    825\u001b[0m     debug_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# True for debugging, but very long\u001b[39;00m\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;66;03m# cache_type=\"diskcache\", #<-- agent caching method \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    832\u001b[0m )\n\u001b[1;32m--> 833\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnd_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_ptree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptree\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\system.py:405\u001b[0m, in \u001b[0;36mModelDiscoverySystem.bind_ptree\u001b[1;34m(self, ptree)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_ptree\u001b[39m(\u001b[38;5;28mself\u001b[39m,ptree): \u001b[38;5;66;03m# need to bind a tree before start working, should be done immediately\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptree \u001b[38;5;241m=\u001b[39m ptree\n\u001b[1;32m--> 405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msss \u001b[38;5;241m=\u001b[39m \u001b[43mSuperScholarSearcher\u001b[49m(ptree,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SuperScholarSearcher' is not defined"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "import importlib\n",
    "import arxiv\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.utils as U\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "headers={\"X-API-KEY\": os.environ['S2_API_KEY']}\n",
    "paper_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}'\n",
    "\n",
    "\n",
    "strparams=[\n",
    "    f\"evoname=test_evo_000\",\n",
    "    \"scales=14M,31M,70M\",\n",
    "    \"selection_ratio=0.25\",\n",
    "    \"select_method=random\",\n",
    "    \"design_budget=0\",\n",
    "]\n",
    "evo_system = BuildEvolution(\n",
    "    strparams=';'.join(strparams),\n",
    "    do_cache=False,\n",
    "    # cache_type='diskcache',\n",
    ")\n",
    "\n",
    "ptree = evo_system.ptree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get paper files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "refs_s2ids={}\n",
    "REF_TYPES=['Reference','ReferenceCore','ReferenceWithCode','ReferenceCoreWithTree']\n",
    "\n",
    "for i in ptree.G.nodes:\n",
    "    node=ptree.G.nodes[i]['data']\n",
    "    if node.type in REF_TYPES:\n",
    "        s2id=node.s2id\n",
    "        if s2id is not None:\n",
    "            refs_s2ids[i]=s2id\n",
    "        else:\n",
    "            print(i)\n",
    "\n",
    "refs=ptree.filter_by_type(REF_TYPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rec and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_rec_papers(s2id):\n",
    "    url=f'https://api.semanticscholar.org/recommendations/v1/papers/forpaper/{s2id}'\n",
    "    rsp=requests.get(url,headers=headers)\n",
    "    results=rsp.json()\n",
    "    if 'error' in results:\n",
    "        print(s2id)\n",
    "        return None\n",
    "    return results['recommendedPapers']\n",
    "\n",
    "recs={}\n",
    "for i in tqdm(refs_s2ids):\n",
    "    s2id=refs_s2ids[i]\n",
    "    rec=get_rec_papers(s2id)\n",
    "    if rec is not None:\n",
    "        recs[i]=rec\n",
    "    time.sleep(0.1)\n",
    "\n",
    "all_recs={}\n",
    "for i in recs:\n",
    "    for j in recs[i]:\n",
    "        all_recs[j['paperId']]=j['title']\n",
    "\n",
    "# all_recs_filtered={}\n",
    "# for i in all_recs:\n",
    "#     if i in refs_s2ids.values():\n",
    "#         continue\n",
    "#     if i in refs2_detail:\n",
    "#         continue\n",
    "#     all_recs_filtered[i]=all_recs[i]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_paper_detail(s2id):\n",
    "    params={\n",
    "        \"fields\": \"title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,authors,embedding,tldr,openAccessPdf,externalIds,references\",\n",
    "    }\n",
    "    rsp=requests.get(\n",
    "        paper_detail.format(paper_id=s2id),\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    results = rsp.json()\n",
    "    return results\n",
    "\n",
    "save_dir=U.pjoin(ptree.lib_dir,'..','tree_ext','plus')\n",
    "refsp_detail={}\n",
    "U.mkdir(save_dir)\n",
    "for s2id in tqdm(all_recs_filtered):\n",
    "    save_path=U.pjoin(save_dir,f'{s2id}.json')\n",
    "    if U.pexists(save_path):\n",
    "        refsp_detail[s2id]=U.load_json(save_path)\n",
    "        continue\n",
    "    ret=get_paper_detail(s2id)\n",
    "    if 'error' not in ret:\n",
    "        U.save_json(ret,save_path)\n",
    "    refsp_detail[s2id]=ret\n",
    "    time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(refsp_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_recs_filtered)\n",
    "\n",
    "refp_arxiv={}\n",
    "refp_pdfs={}\n",
    "for i in refsp_detail:\n",
    "    if 'openAccessPdf' in refsp_detail[i] and refsp_detail[i]['openAccessPdf'] is not None and 'arxiv' not in refsp_detail[i]['openAccessPdf']['url']:\n",
    "        url=refsp_detail[i]['openAccessPdf']\n",
    "        refp_pdfs[i]=url['url']\n",
    "        continue\n",
    "    if 'externalIds' in refsp_detail[i]:\n",
    "        if 'ArXiv' in refsp_detail[i]['externalIds']:\n",
    "            refp_arxiv[i]=refsp_detail[i]['externalIds']['ArXiv']\n",
    "            continue\n",
    "    # print(i)\n",
    "print(len(refp_arxiv))\n",
    "print(len(refp_pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar5iv_url='https://ar5iv.labs.arxiv.org/html/{arxiv_id}'\n",
    "htmlsp_dir=U.pjoin(ptree.lib_dir,'..','files','htmlsp')\n",
    "U.mkdir(htmlsp_dir)\n",
    "pdfsp_dir=U.pjoin(ptree.lib_dir,'..','files','pdfsp')\n",
    "U.mkdir(pdfsp_dir)\n",
    "\n",
    "    \n",
    "# for i in tqdm(refp_arxiv):\n",
    "#     DIR=U.pjoin(htmlsp_dir,f'{i}.html')\n",
    "#     if os.path.exists(DIR):\n",
    "#         continue\n",
    "#     url=ar5iv_url.format(arxiv_id=refp_arxiv[i])\n",
    "\n",
    "#     # Send a GET request to the URL\n",
    "#     response = requests.get(url)\n",
    "\n",
    "#     # Check if the request was successful (status code 200)\n",
    "#     if response.status_code == 200:\n",
    "#         # Save the HTML content to a file\n",
    "#         with open(DIR, \"w\", encoding=\"utf-8\") as file:\n",
    "#             file.write(response.text)\n",
    "#     else:\n",
    "#         print(f\"Failed to retrieve {i}. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "\n",
    "failed_pdfs={}\n",
    "for i in tqdm(refp_pdfs):\n",
    "    url=refp_pdfs[i]\n",
    "    save_path=U.pjoin(pdfsp_dir,f'{i}.pdf')\n",
    "    if not os.path.exists(save_path):\n",
    "        try:\n",
    "            pdf=urllib.request.urlretrieve(url, save_path)\n",
    "        except:\n",
    "            failed_pdfs[i]=url\n",
    "            print(f'Failed to download {i}, {url}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get references and files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"fields\": \"openAccessPdf,externalIds\",\n",
    "}\n",
    "\n",
    "pdf_urls={}\n",
    "external_ids={}\n",
    "for i in tqdm(refs_s2ids):\n",
    "    s2id=refs_s2ids[i]\n",
    "    rsp=requests.get(\n",
    "        paper_detail.format(paper_id=s2id),\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    results = rsp.json()\n",
    "    if 'openAccessPdf' in results and results['openAccessPdf'] is not None:\n",
    "        pdf_urls[i]=results['openAccessPdf']\n",
    "    if 'externalIds' in results and results['externalIds'] is not None:\n",
    "        external_ids[i]=results['externalIds']\n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "arxiv_ids={}\n",
    "for i,j in external_ids.items():\n",
    "    if 'ArXiv' in j:\n",
    "        arxiv_ids[i]=j['ArXiv']\n",
    "\n",
    "arxiv_ids_add={\n",
    "    'fire':'2310.04418',\n",
    "    'mogrifier':'1909.01792',\n",
    "    'hedgehog':'2402.04347',\n",
    "    'synthesizer':'2005.00743',\n",
    "    'settransformer':'1810.00825',\n",
    "    'infiniteformer':'2109.00301',\n",
    "    'etc':'2004.08483',\n",
    "    'feedbackmem':'2002.09402',\n",
    "    'kangpt':'2408.10205',\n",
    "}\n",
    "\n",
    "arxiv_ids.update(arxiv_ids_add)\n",
    "\n",
    "ar5iv_url='https://ar5iv.labs.arxiv.org/html/{arxiv_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node(id):\n",
    "    return ptree.G.nodes[id]['data']\n",
    "\n",
    "\n",
    "def get_references(id):\n",
    "    params={\n",
    "        \"fields\": \"references\",\n",
    "    }\n",
    "    s2id=refs_s2ids[id]\n",
    "    rsp=requests.get(\n",
    "        paper_detail.format(paper_id=s2id),\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    results = rsp.json()\n",
    "    return results['references']\n",
    "\n",
    "\n",
    "all_refs={}\n",
    "for i in tqdm(refs_s2ids):\n",
    "    all_refs[i]=get_references(i)\n",
    "\n",
    "refs_set={}\n",
    "for i in all_refs:\n",
    "    for ref in all_refs[i]:\n",
    "        refs_set[ref['paperId']]=ref['title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_detail(s2id):\n",
    "    params={\n",
    "        \"fields\": \"title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,authors,tldr,embedding,tldr,openAccessPdf,externalIds,references\",\n",
    "    }\n",
    "    rsp=requests.get(\n",
    "        paper_detail.format(paper_id=s2id),\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    results = rsp.json()\n",
    "    return results\n",
    "\n",
    "refs_detail={}\n",
    "save_dir=U.pjoin(ptree.lib_dir,'..','tree_ext','refs2')\n",
    "for i in tqdm(refs_set):\n",
    "    save_path=U.pjoin(save_dir,f'{i}.json')\n",
    "    if U.pexists(save_path):\n",
    "        refs_detail[i]=U.load_json(save_path)\n",
    "        continue\n",
    "    ret=get_paper_detail(i)\n",
    "    if 'error' not in ret:\n",
    "        U.save_json(ret,save_path)\n",
    "    refs_detail[i]=ret\n",
    "    time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs2_detail={}\n",
    "ref2_dir=U.pjoin(ptree.lib_dir,'..','tree_ext','secondary')\n",
    "for i in os.listdir(ref2_dir):\n",
    "    refs2_detail[i.split('.')[0]]=U.load_json(U.pjoin(ref2_dir,i))\n",
    "\n",
    "ref2_arxiv={}\n",
    "ref2_pdfs={}\n",
    "for i in refs_detail:\n",
    "    if 'externalIds' in refs2_detail[i]:\n",
    "        if 'ArXiv' in refs2_detail[i]['externalIds']:\n",
    "            ref2_arxiv[i]=refs2_detail[i]['externalIds']['ArXiv']\n",
    "            continue\n",
    "    if 'openAccessPdf' in refs2_detail[i] and refs2_detail[i]['openAccessPdf'] is not None and 'arxiv' not in refs2_detail[i]['openAccessPdf']['url']:\n",
    "        url=refs_detail[i]['openAccessPdf']\n",
    "        ref2_pdfs[i]=url['url']\n",
    "        continue\n",
    "    # print(i)\n",
    "\n",
    "print(len(ref2_arxiv))\n",
    "print(len(ref2_pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, urllib.request\n",
    "\n",
    "save_dir=U.pjoin(ptree.lib_dir,'..','pdfs2')\n",
    "arxiv_pdf='https://arxiv.org/pdf/{arxiv_id}'\n",
    "U.mkdir(save_dir)\n",
    "\n",
    "# bar=tqdm(ref2_arxiv)\n",
    "# for i in bar:\n",
    "#     bar.set_description(f'Downloading {i}')\n",
    "#     arxiv_id=ref2_arxiv[i]\n",
    "#     download_arxiv_pdf(arxiv_id,save_dir,f'{i}.pdf')\n",
    "\n",
    "failed_pdfs={}\n",
    "for i in tqdm(ref2_pdfs):\n",
    "    url=ref2_pdfs[i]\n",
    "    save_path=U.pjoin(save_dir,f'{i}.pdf')\n",
    "    if not os.path.exists(save_path):\n",
    "        try:\n",
    "            pdf=urllib.request.urlretrieve(url, save_path)\n",
    "        except:\n",
    "            failed_pdfs[i]=url\n",
    "            print(f'Failed to download {i}, {url}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "save_dir=U.pjoin(ptree.lib_dir,'..','htmls2')\n",
    "U.mkdir(save_dir)\n",
    "ar5iv_url='https://ar5iv.labs.arxiv.org/html/{arxiv_id}'\n",
    "\n",
    "for i in tqdm(ref2_arxiv):\n",
    "    DIR=U.pjoin(save_dir,f'{i}.html')\n",
    "    if os.path.exists(DIR):\n",
    "        continue\n",
    "    url=ar5iv_url.format(arxiv_id=ref2_arxiv[i])\n",
    "        \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Save the HTML content to a file\n",
    "        with open(DIR, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response.text)\n",
    "        # print(f\"HTML content saved to {i}.html\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {i}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmls_dir=U.pjoin(ptree.lib_dir,'..','htmls')\n",
    "pdfs_dir=U.pjoin(ptree.lib_dir,'..','pdfs')\n",
    "\n",
    "htmls=[i.split('.')[0] for i in os.listdir(htmls_dir)]\n",
    "pdfs=[i.split('.')[0] for i in os.listdir(pdfs_dir)]\n",
    "\n",
    "print(len(htmls))\n",
    "print(len(pdfs))\n",
    "\n",
    "arxiv_url='https://arxiv.org/abs/{arxiv_id}'\n",
    "\n",
    "for i in pdfs:\n",
    "    if i not in htmls:\n",
    "        if i in arxiv_ids:\n",
    "            arxiv_id=arxiv_ids[i]\n",
    "            url=ar5iv_url.format(arxiv_id=arxiv_id)\n",
    "            arxiv=arxiv_url.format(arxiv_id=arxiv_id)\n",
    "            print(url,arxiv,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, urllib.request\n",
    "url = 'http://export.arxiv.org/api/query?search_query=\"{query}\"&start=0&max_results=1'\n",
    "title = ptree.G.nodes[refs[0]]['data'].title\n",
    "encoded_title = urllib.parse.quote(title)\n",
    "url=url.format(query=encoded_title)\n",
    "data = urllib.request.urlopen(url)\n",
    "print(data.read().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agents.search_utils \n",
    "importlib.reload(agents.search_utils)\n",
    "\n",
    "from agents.search_utils import SuperScholarSearcher\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dev tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13:13:48:55,052 INFO     [discover_namespace_packages.py:12] Discovering subpackages in _NamespacePath(['c:\\\\Users\\\\ChengJunyan1\\\\anaconda3\\\\envs\\\\modis\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "2024-09-13:13:48:55,055 INFO     [discover_plugins.py:9] Looking for plugins in pinecone_plugins.inference\n",
      "2024-09-13:13:48:55,056 INFO     [installation.py:10] Installing plugin inference into PineconeGRPC\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.search_utils\n",
    "importlib.reload(model_discovery.agents.search_utils)\n",
    "\n",
    "from model_discovery.agents.search_utils import SuperScholarSearcher, pwc_search_patched\n",
    "\n",
    "sss=SuperScholarSearcher(ptree,evo_system.stream)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vector Store for internal Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading splits Primary: 100%|██████████| 294/294 [00:31<00:00,  9.36it/s]\n",
      "Loading splits Secondary:  64%|██████▎   | 1807/2836 [02:03<01:22, 12.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw_texts in internal library...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading splits Secondary:  64%|██████▎   | 1807/2836 [02:15<01:22, 12.54it/s]2024-09-13:13:52:40,951 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:52:45,295 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1808/2836 [03:16<3:25:09, 11.97s/it]2024-09-13:13:52:48,078 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:52:51,520 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1809/2836 [03:22<3:04:59, 10.81s/it]2024-09-13:13:52:52,851 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:52:54,227 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:52:56,311 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:52:58,585 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1811/2836 [03:29<2:21:05,  8.26s/it]2024-09-13:13:52:59,664 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:00,895 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1812/2836 [03:32<2:01:51,  7.14s/it]2024-09-13:13:53:02,417 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:03,747 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1813/2836 [03:34<1:43:53,  6.09s/it]2024-09-13:13:53:05,593 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:07,619 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1814/2836 [03:38<1:35:10,  5.59s/it]2024-09-13:13:53:10,088 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:12,451 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1815/2836 [03:43<1:32:19,  5.43s/it]2024-09-13:13:53:14,532 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:17,682 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1816/2836 [03:48<1:32:11,  5.42s/it]2024-09-13:13:53:19,549 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:21,426 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1817/2836 [03:52<1:22:28,  4.86s/it]2024-09-13:13:53:23,911 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:26,209 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:29,855 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1818/2836 [04:01<1:44:20,  6.15s/it]2024-09-13:13:53:32,894 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:35,582 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1819/2836 [04:07<1:40:05,  5.90s/it]2024-09-13:13:53:38,572 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:40,818 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:43,567 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1820/2836 [04:16<1:56:34,  6.88s/it]2024-09-13:13:53:47,109 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:48,587 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1821/2836 [04:19<1:38:07,  5.80s/it]2024-09-13:13:53:50,788 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:53,619 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1822/2836 [04:24<1:33:48,  5.55s/it]2024-09-13:13:53:55,807 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:53:58,809 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:02,097 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:09,821 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1823/2836 [04:43<2:39:12,  9.43s/it]2024-09-13:13:54:13,285 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:14,560 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1824/2836 [04:45<2:04:23,  7.38s/it]2024-09-13:13:54:16,054 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:18,599 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1825/2836 [04:49<1:48:12,  6.42s/it]2024-09-13:13:54:20,638 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:23,038 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1826/2836 [04:53<1:36:17,  5.72s/it]2024-09-13:13:54:23,904 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:25,017 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1827/2836 [04:56<1:20:40,  4.80s/it]2024-09-13:13:54:27,111 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:30,056 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1828/2836 [05:01<1:20:23,  4.79s/it]2024-09-13:13:54:31,594 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:33,597 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  64%|██████▍   | 1829/2836 [05:04<1:13:30,  4.38s/it]2024-09-13:13:54:35,335 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:38,645 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1830/2836 [05:09<1:16:39,  4.57s/it]2024-09-13:13:54:40,623 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:43,706 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1831/2836 [05:14<1:18:19,  4.68s/it]2024-09-13:13:54:44,614 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:45,331 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1832/2836 [05:16<1:01:39,  3.69s/it]2024-09-13:13:54:46,344 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:47,865 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1833/2836 [05:18<56:34,  3.38s/it]  2024-09-13:13:54:51,062 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:54,055 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1834/2836 [05:24<1:11:10,  4.26s/it]2024-09-13:13:54:56,022 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:54:59,611 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1835/2836 [05:31<1:21:08,  4.86s/it]2024-09-13:13:55:02,262 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:55:05,556 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1836/2836 [05:37<1:28:15,  5.30s/it]2024-09-13:13:55:08,114 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:55:09,820 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1837/2836 [05:41<1:19:02,  4.75s/it]2024-09-13:13:55:12,223 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:55:14,715 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1838/2836 [05:45<1:18:37,  4.73s/it]2024-09-13:13:55:16,033 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:55:17,352 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1839/2836 [05:48<1:07:12,  4.04s/it]2024-09-13:13:55:19,095 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:55:22,475 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1840/2836 [05:53<1:14:55,  4.51s/it]2024-09-13:13:55:24,599 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:55:26,686 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1841/2836 [05:57<1:11:08,  4.29s/it]2024-09-13:13:55:27,916 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:55:30,054 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1842/2836 [06:00<1:06:36,  4.02s/it]2024-09-13:13:55:31,714 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:55:34,439 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▍   | 1843/2836 [06:05<1:10:06,  4.24s/it]2024-09-13:13:55:36,103 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:55:38,483 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1844/2836 [06:09<1:09:09,  4.18s/it]2024-09-13:13:55:40,531 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:55:42,612 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1845/2836 [06:13<1:06:43,  4.04s/it]2024-09-13:13:55:44,288 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:55:47,045 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1846/2836 [06:17<1:09:05,  4.19s/it]2024-09-13:13:55:48,349 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:55:51,018 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1847/2836 [06:21<1:06:57,  4.06s/it]2024-09-13:13:55:52,383 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:55:54,932 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1848/2836 [06:25<1:07:20,  4.09s/it]2024-09-13:13:55:57,806 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:00,432 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1849/2836 [06:31<1:16:52,  4.67s/it]2024-09-13:13:56:02,781 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:05,738 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1850/2836 [06:37<1:22:25,  5.02s/it]2024-09-13:13:56:08,613 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:12,419 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1851/2836 [06:44<1:31:50,  5.59s/it]2024-09-13:13:56:15,428 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:17,217 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1852/2836 [06:48<1:21:17,  4.96s/it]2024-09-13:13:56:18,659 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:19,478 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1853/2836 [06:50<1:07:45,  4.14s/it]2024-09-13:13:56:21,038 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:23,285 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1854/2836 [06:54<1:07:14,  4.11s/it]2024-09-13:13:56:25,275 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:29,305 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1855/2836 [07:01<1:20:36,  4.93s/it]2024-09-13:13:56:32,049 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:34,618 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1856/2836 [07:06<1:22:51,  5.07s/it]2024-09-13:13:56:37,267 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:38,975 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  65%|██████▌   | 1857/2836 [07:10<1:14:22,  4.56s/it]2024-09-13:13:56:40,732 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:43,917 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1858/2836 [07:14<1:16:15,  4.68s/it]2024-09-13:13:56:45,401 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:47,179 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1859/2836 [07:18<1:08:15,  4.19s/it]2024-09-13:13:56:48,724 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:50,076 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1860/2836 [07:21<1:05:48,  4.05s/it]2024-09-13:13:56:52,589 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:54,742 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1861/2836 [07:25<1:06:40,  4.10s/it]2024-09-13:13:56:56,843 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:56:58,769 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1862/2836 [07:30<1:09:45,  4.30s/it]2024-09-13:13:57:02,930 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:57:07,550 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:57:10,450 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1863/2836 [07:41<1:43:11,  6.36s/it]2024-09-13:13:57:13,173 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:57:17,468 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1864/2836 [07:48<1:45:22,  6.50s/it]2024-09-13:13:57:19,678 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:57:22,472 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1865/2836 [07:53<1:37:59,  6.06s/it]2024-09-13:13:57:24,101 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:57:25,401 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1866/2836 [07:56<1:21:37,  5.05s/it]2024-09-13:13:57:28,690 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:57:31,596 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:57:35,847 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1867/2836 [08:07<1:50:12,  6.82s/it]2024-09-13:13:57:37,865 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:57:39,584 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1868/2836 [08:10<1:31:33,  5.67s/it]2024-09-13:13:57:40,602 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:57:41,722 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1869/2836 [08:12<1:15:28,  4.68s/it]2024-09-13:13:57:42,916 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:57:44,642 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1870/2836 [08:15<1:06:54,  4.16s/it]2024-09-13:13:57:46,451 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:57:48,362 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1871/2836 [08:20<1:08:52,  4.28s/it]2024-09-13:13:57:50,642 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:57:52,838 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1872/2836 [08:23<1:05:51,  4.10s/it]2024-09-13:13:57:55,632 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:57:58,402 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1873/2836 [08:29<1:14:24,  4.64s/it]2024-09-13:13:58:00,213 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:02,469 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1874/2836 [08:33<1:08:03,  4.25s/it]2024-09-13:13:58:04,132 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:08,067 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1875/2836 [08:39<1:17:11,  4.82s/it]2024-09-13:13:58:10,028 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:11,682 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1876/2836 [08:42<1:10:07,  4.38s/it]2024-09-13:13:58:13,174 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:14,723 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1877/2836 [08:45<1:04:36,  4.04s/it]2024-09-13:13:58:16,884 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:19,308 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▌   | 1878/2836 [08:50<1:05:17,  4.09s/it]2024-09-13:13:58:20,542 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:22,386 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▋   | 1879/2836 [08:53<1:00:14,  3.78s/it]2024-09-13:13:58:24,243 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:26,519 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▋   | 1880/2836 [08:57<1:03:07,  3.96s/it]2024-09-13:13:58:28,492 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:32,921 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▋   | 1881/2836 [09:03<1:13:46,  4.63s/it]2024-09-13:13:58:35,888 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:38,343 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▋   | 1882/2836 [09:09<1:18:18,  4.93s/it]2024-09-13:13:58:41,008 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:42,850 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:44,923 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▋   | 1883/2836 [09:16<1:28:03,  5.54s/it]2024-09-13:13:58:47,811 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:50,980 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▋   | 1884/2836 [09:21<1:27:36,  5.52s/it]2024-09-13:13:58:52,234 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:54,127 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  66%|██████▋   | 1885/2836 [09:25<1:16:58,  4.86s/it]2024-09-13:13:58:55,623 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:57,111 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1886/2836 [09:28<1:08:38,  4.34s/it]2024-09-13:13:58:58,607 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:58:59,917 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1887/2836 [09:31<1:02:04,  3.92s/it]2024-09-13:13:59:01,563 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:03,873 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1888/2836 [09:34<1:01:02,  3.86s/it]2024-09-13:13:59:06,004 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:08,462 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1889/2836 [09:39<1:03:33,  4.03s/it]2024-09-13:13:59:09,778 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:11,349 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1890/2836 [09:42<57:38,  3.66s/it]  2024-09-13:13:59:13,040 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:14,547 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1891/2836 [09:45<55:25,  3.52s/it]2024-09-13:13:59:16,119 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:18,306 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1892/2836 [09:49<57:49,  3.68s/it]2024-09-13:13:59:20,259 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:24,346 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1893/2836 [09:56<1:14:29,  4.74s/it]2024-09-13:13:59:26,943 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:28,236 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1894/2836 [09:59<1:03:23,  4.04s/it]2024-09-13:13:59:29,490 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:31,365 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1895/2836 [10:03<1:05:21,  4.17s/it]2024-09-13:13:59:33,586 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:35,033 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1896/2836 [10:05<56:02,  3.58s/it]  2024-09-13:13:59:36,170 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:38,349 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1897/2836 [10:09<57:20,  3.66s/it]2024-09-13:13:59:40,062 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:41,582 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1898/2836 [10:12<53:27,  3.42s/it]2024-09-13:13:59:43,464 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:48,079 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1899/2836 [10:20<1:13:32,  4.71s/it]2024-09-13:13:59:50,559 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:51,964 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1900/2836 [10:22<1:04:12,  4.12s/it]2024-09-13:13:59:53,187 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:55,232 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1901/2836 [10:26<1:01:02,  3.92s/it]2024-09-13:13:59:56,928 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:13:59:59,038 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1902/2836 [10:29<59:44,  3.84s/it]  2024-09-13:14:00:00,499 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:00:02,673 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1903/2836 [10:33<58:22,  3.75s/it]2024-09-13:14:00:04,276 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:00:06,758 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1904/2836 [10:38<1:01:46,  3.98s/it]2024-09-13:14:00:08,553 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:00:10,594 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1905/2836 [10:41<59:36,  3.84s/it]  2024-09-13:14:00:12,611 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:00:23,745 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1906/2836 [10:55<1:47:40,  6.95s/it]2024-09-13:14:00:27,417 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:00:38,415 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:00:41,808 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1907/2836 [11:13<2:37:19, 10.16s/it]2024-09-13:14:00:44,466 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:00:47,593 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1908/2836 [11:18<2:14:23,  8.69s/it]2024-09-13:14:00:49,302 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:00:51,312 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1909/2836 [11:22<1:51:16,  7.20s/it]2024-09-13:14:00:53,127 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:00:55,415 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1910/2836 [11:26<1:37:40,  6.33s/it]2024-09-13:14:00:56,859 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:00,254 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1911/2836 [11:31<1:32:30,  6.00s/it]2024-09-13:14:01:02,406 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:03,928 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1912/2836 [11:35<1:21:58,  5.32s/it]2024-09-13:14:01:06,850 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:09,370 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1913/2836 [11:40<1:20:21,  5.22s/it]2024-09-13:14:01:12,052 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:15,304 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:20,958 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  67%|██████▋   | 1914/2836 [11:52<1:51:22,  7.25s/it]2024-09-13:14:01:23,782 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:25,143 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1915/2836 [11:55<1:33:04,  6.06s/it]2024-09-13:14:01:26,086 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:27,624 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1916/2836 [11:58<1:16:14,  4.97s/it]2024-09-13:14:01:28,384 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:29,434 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1917/2836 [12:00<1:03:11,  4.13s/it]2024-09-13:14:01:31,414 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:35,493 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1918/2836 [12:06<1:11:06,  4.65s/it]2024-09-13:14:01:36,884 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:39,063 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1919/2836 [12:09<1:05:51,  4.31s/it]2024-09-13:14:01:40,315 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:42,064 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1920/2836 [12:12<59:43,  3.91s/it]  2024-09-13:14:01:43,716 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:46,570 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:49,672 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1921/2836 [12:20<1:18:05,  5.12s/it]2024-09-13:14:01:53,318 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:01:57,610 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:01,020 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1922/2836 [12:32<1:47:13,  7.04s/it]2024-09-13:14:02:02,719 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:04,624 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1923/2836 [12:35<1:29:39,  5.89s/it]2024-09-13:14:02:06,409 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:08,847 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1924/2836 [12:39<1:22:46,  5.45s/it]2024-09-13:14:02:10,272 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:15,965 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1925/2836 [12:47<1:31:16,  6.01s/it]2024-09-13:14:02:18,990 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:22,731 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1926/2836 [12:54<1:34:34,  6.24s/it]2024-09-13:14:02:24,770 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:27,628 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1927/2836 [12:58<1:28:04,  5.81s/it]2024-09-13:14:02:29,145 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:30,830 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1928/2836 [13:01<1:15:17,  4.98s/it]2024-09-13:14:02:33,181 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:35,631 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1929/2836 [13:06<1:13:33,  4.87s/it]2024-09-13:14:02:37,617 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:39,777 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1930/2836 [13:11<1:12:05,  4.77s/it]2024-09-13:14:02:42,135 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:44,222 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1931/2836 [13:17<1:19:01,  5.24s/it]2024-09-13:14:02:47,377 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:48,104 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1932/2836 [13:18<1:02:15,  4.13s/it]2024-09-13:14:02:49,339 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:50,861 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1933/2836 [13:21<55:56,  3.72s/it]  2024-09-13:14:02:52,735 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:53,931 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:02:56,298 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1934/2836 [13:27<1:06:49,  4.45s/it]2024-09-13:14:02:58,158 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:03:01,113 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1935/2836 [13:31<1:05:23,  4.36s/it]2024-09-13:14:03:02,735 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:03:06,566 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1936/2836 [13:38<1:14:24,  4.96s/it]2024-09-13:14:03:08,556 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:03:09,986 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1937/2836 [13:40<1:03:33,  4.24s/it]2024-09-13:14:03:12,408 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:03:16,262 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1938/2836 [13:47<1:15:40,  5.06s/it]2024-09-13:14:03:18,194 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:03:22,505 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1939/2836 [13:54<1:21:19,  5.44s/it]2024-09-13:14:03:24,962 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:03:27,584 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1940/2836 [13:58<1:16:10,  5.10s/it]2024-09-13:14:03:29,098 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:03:30,664 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1941/2836 [14:01<1:06:57,  4.49s/it]2024-09-13:14:03:32,312 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:03:34,147 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  68%|██████▊   | 1942/2836 [14:05<1:03:55,  4.29s/it]2024-09-13:14:03:37,773 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:03:41,246 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▊   | 1943/2836 [14:12<1:16:08,  5.12s/it]2024-09-13:14:03:42,913 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:03:44,323 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▊   | 1944/2836 [14:15<1:06:30,  4.47s/it]2024-09-13:14:03:49,758 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:03:53,221 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▊   | 1945/2836 [14:24<1:25:30,  5.76s/it]2024-09-13:14:03:54,487 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:03:56,240 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▊   | 1946/2836 [14:27<1:13:13,  4.94s/it]2024-09-13:14:03:57,643 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:00,351 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▊   | 1947/2836 [14:32<1:12:35,  4.90s/it]2024-09-13:14:04:02,810 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:05,510 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▊   | 1948/2836 [14:36<1:12:01,  4.87s/it]2024-09-13:14:04:06,996 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:08,652 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▊   | 1949/2836 [14:39<1:02:14,  4.21s/it]2024-09-13:14:04:10,242 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:13,271 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1950/2836 [14:44<1:04:55,  4.40s/it]2024-09-13:14:04:15,238 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:17,853 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1951/2836 [14:48<1:05:14,  4.42s/it]2024-09-13:14:04:19,491 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:20,695 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1952/2836 [14:51<58:55,  4.00s/it]  2024-09-13:14:04:22,727 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:24,997 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1953/2836 [14:55<59:05,  4.02s/it]2024-09-13:14:04:26,587 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:29,462 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1954/2836 [15:00<1:03:07,  4.29s/it]2024-09-13:14:04:31,698 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:33,506 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1955/2836 [15:04<59:48,  4.07s/it]  2024-09-13:14:04:35,148 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:38,050 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1956/2836 [15:09<1:04:43,  4.41s/it]2024-09-13:14:04:40,234 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:42,293 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1957/2836 [15:13<1:02:57,  4.30s/it]2024-09-13:14:04:45,294 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:49,581 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1958/2836 [15:20<1:16:02,  5.20s/it]2024-09-13:14:04:51,485 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:53,393 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1959/2836 [15:24<1:08:21,  4.68s/it]2024-09-13:14:04:54,834 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:04:55,984 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1960/2836 [15:27<59:47,  4.10s/it]  2024-09-13:14:04:58,039 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:05:00,266 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1961/2836 [15:31<59:48,  4.10s/it]2024-09-13:14:05:02,388 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:05:05,831 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1962/2836 [15:37<1:07:37,  4.64s/it]2024-09-13:14:05:08,221 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:05:11,117 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1963/2836 [15:42<1:10:30,  4.85s/it]2024-09-13:14:05:12,588 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:05:13,687 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1964/2836 [15:44<58:02,  3.99s/it]  2024-09-13:14:05:15,401 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:05:18,391 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1965/2836 [15:50<1:04:53,  4.47s/it]2024-09-13:14:05:20,760 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:05:22,885 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1966/2836 [15:54<1:02:56,  4.34s/it]2024-09-13:14:05:27,575 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:05:32,373 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1967/2836 [16:04<1:28:47,  6.13s/it]2024-09-13:14:05:35,591 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:05:38,295 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1968/2836 [16:09<1:24:06,  5.81s/it]2024-09-13:14:05:39,816 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-13:14:05:41,963 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Loading splits Secondary:  69%|██████▉   | 1969/2836 [16:13<1:14:40,  5.17s/it]"
     ]
    }
   ],
   "source": [
    "sss._build_vector_stores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Search \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:22:40:13,735 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-09-12:22:40:14,114 INFO     [_client.py:1038] HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "grouped_docs,metainfo=sss.search_internal('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### 1. LongT5: Efficient text-to-text transformer for long sequences (Avg. Score: 0.02)\n",
      "\n",
      "*Mandy Guo, J. Ainslie, David C. Uthus, Santiago Ontañón, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang*\n",
      "\n",
      "**TL;DR:** A new model, called LongT5, is presented, with which the effects of scaling both the input length and model size at the same time are explored, which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs.\n",
      "\n",
      "**Abstract:** Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.\n",
      "\n",
      "**Venue:** NAACL-HLT\n",
      "\n",
      "**Year:** 2021\n",
      "\n",
      "**Citations:** 227  (*Influential: 40*)**Semantic Scholar ID:** 3dfb1f50f2a34a699c339dabaa6f9b3a977973de\n",
      "\n",
      "###### Related Contents\n",
      "**Excerpt 1 (Score: 0.02)**\n",
      "6 6}$ | $\\mathbf{6 6 .\n",
      "\n",
      "\n",
      "##### 2. Erniesparse: Learning hierarchical efficient transformer through regularized self-attention (Avg. Score: 0.02)\n",
      "\n",
      "*Yang Liu, Jiaxiang Liu, L. Chen, Yuxiang Lu, Shi Feng, Zhida Feng, Yu Sun, Hao Tian, Huancheng Wu, Hai-feng Wang*\n",
      "\n",
      "**TL;DR:** Experimental results demonstrate that ERNIE-Sparse significantly outperforms a variety of strong baseline methods including the dense attention and other efficient sparse attention methods and achieves improvements on classification benchmark and on QA downstream tasks.\n",
      "\n",
      "**Abstract:** Sparse Transformer has recently attracted a lot of attention since the ability for reducing the quadratic dependency on the sequence length. We argue that two factors, information bottleneck sensitivity and inconsistency between different attention topologies, could affect the performance of the Sparse Transformer. This paper proposes a well-designed model named ERNIE-Sparse. It consists of two distinctive parts: (i) Hierarchical Sparse Transformer (HST) to sequentially unify local and global information. (ii) Self-Attention Regularization (SAR) method, a novel regularization designed to minimize the distance for transformers with different attention topologies. To evaluate the effectiveness of ERNIE-Sparse, we perform extensive evaluations. Firstly, we perform experiments on a multi-modal long sequence modeling task benchmark, Long Range Arena (LRA). Experimental results demonstrate that ERNIE-Sparse significantly outperforms a variety of strong baseline methods including the dense attention and other efficient sparse attention methods and achieves improvements by 2.77% (57.78% vs. 55.01%). Secondly, to further show the effectiveness of our method, we pretrain ERNIE-Sparse and verified it on 3 text classification and 2 QA downstream tasks, achieve improvements on classification benchmark by 0.83% (92.46% vs. 91.63%), on QA benchmark by 3.24% (74.67% vs. 71.43%). Experimental results continue to demonstrate its superior performance.\n",
      "\n",
      "**Venue:** arXiv.org\n",
      "\n",
      "**Year:** 2022\n",
      "\n",
      "**Citations:** 9  (*Influential: 1*)**Semantic Scholar ID:** 94e46e18d2628343a926acf6c3d0817e11d35d58\n",
      "\n",
      "###### Related Contents\n",
      "**Excerpt 1 (Score: 0.02)**\n",
      "7 6}$ | $\\mathbf{7 6 .\n",
      "\n",
      "\n",
      "##### 3. Structured Denoising Diffusion Models in Discrete State-Spaces  (Avg. Score: 0.00)\n",
      "\n",
      "*Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, Rianne van den Berg*\n",
      "\n",
      "**TL;DR:** D3PMs are diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities and showing that the choice of transition matrix is an important design decision that leads to improved results in image and text domains.\n",
      "\n",
      "**Abstract:** Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.\n",
      "\n",
      "**Venue:** Neural Information Processing Systems\n",
      "\n",
      "**Year:** 2021\n",
      "\n",
      "**Citations:** 530  (*Influential: 83*)**Semantic Scholar ID:** 91b32fc0a23f0af53229fceaae9cce43a0406d2e\n",
      "\n",
      "###### Related Contents\n",
      "**Excerpt 1 (Score: 0.00)**\n",
      "??g? i??t??z??? zero ?wo ha?at??n?ha?ex?en??v?， be?a?e?????n？ｏ？dy ?rch?t?ctu?e ???? f?tur?,v??le rip? ?u ?h?as stevi??pierr?????er?b?t\n",
      "\n",
      "![](https://cdn.mathpix.com/cropped/2024_09_12_d022f2f11ed3ef9cf4ecg-32.jpg?height=34&width=1381&top_left_y=1921&top_left_x=399) g?\n",
      "\n",
      "\n",
      "##### 4. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence (Avg. Score: 0.00)\n",
      "\n",
      "*Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, P. Kazienko, G. Kranthikiran, Jan Koco'n, Bartlomiej Koptyra, Satyapriya Krishna, Ronald McClelland, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanislaw Wo'zniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Ruijie Zhu*\n",
      "\n",
      "**TL;DR:** This work presents Eagle and Finch, sequence models improving upon the RWKV (RWKV-4) architecture, which introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality.\n",
      "\n",
      "**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n",
      "\n",
      "**Venue:** arXiv.org\n",
      "\n",
      "**Year:** 2024\n",
      "\n",
      "**Citations:** 16  (*Influential: 1*)**Semantic Scholar ID:** 157ed5647da39a7f5d33a84a90414b2a9e97e301\n",
      "\n",
      "###### Reference Code\n",
      "<details><summary>Click to expand</summary>\n",
      "\n",
      "```\n",
      "# gab.py\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "from model_discovery.model.utils.modules import GABBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
      "\n",
      "from einops import rearrange\n",
      "\n",
      "from transformers.activations import ACT2FN\n",
      "from typing import TYPE_CHECKING, Optional, Tuple\n",
      "\n",
      "# YOU CAN IMPORT MORE MODULES HERE #\n",
      "\n",
      "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
      "\n",
      "\n",
      "def naive_recurrent_rwkv6(\n",
      "    q: torch.Tensor,\n",
      "    k: torch.Tensor,\n",
      "    v: torch.Tensor,\n",
      "    w: torch.Tensor,\n",
      "    u: torch.Tensor,\n",
      "    scale: Optional[float] = None,\n",
      "):\n",
      "    orig_dtype = q.dtype\n",
      "    B, H, T, K, V = *q.shape, v.shape[-1]\n",
      "    q, k, v, w, u = map(lambda x: x.float(), (q, k, v, w, u))\n",
      "    h = torch.zeros(B, H, K, V, dtype=torch.float32, device=q.device)\n",
      "    o = torch.zeros_like(v)\n",
      "\n",
      "    if scale is None:\n",
      "        scale = K ** -0.5\n",
      "\n",
      "    for i in range(T):\n",
      "        q_i = q[:, :, i, :] * scale\n",
      "        k_i = k[:, :, i]\n",
      "        v_i = v[:, :, i, :]\n",
      "        w_i = w[:, :, i].exp()\n",
      "        kv_i = k_i[..., None] * v_i[..., None, :]\n",
      "        o_i = (h + u[None, ..., None] * kv_i) * q_i[..., None]\n",
      "        o[:, :, i] = o_i.sum(-2)\n",
      "        h = h * w_i[..., None] + kv_i\n",
      "    return o.to(orig_dtype)\n",
      "\n",
      "class RWKV6Attention(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        hidden_size: int = 1024,\n",
      "        num_heads: int = 4,\n",
      "        gate_fn: str = 'swish',\n",
      "        proj_low_rank_dim: int = 32,\n",
      "        gate_low_rank_dim: int = 64,\n",
      "        elementwise_affine: Optional[bool] = True,\n",
      "        norm_eps: float = 1e-5,\n",
      "        device=None,\n",
      "        dtype=None,\n",
      "        **kwargs\n",
      "    ):\n",
      "        super().__init__()\n",
      "\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_heads = num_heads\n",
      "        self.proj_low_rank_dim = proj_low_rank_dim\n",
      "        self.gate_low_rank_dim = gate_low_rank_dim\n",
      "\n",
      "        self.key_dim = hidden_size // 2\n",
      "        self.value_dim = hidden_size\n",
      "\n",
      "        assert self.key_dim % num_heads == 0, f\"key dim must be divisible by num_heads of {num_heads}\"\n",
      "        assert self.value_dim % num_heads == 0, f\"value dim must be divisible by num_heads of {num_heads}\"\n",
      "\n",
      "        self.head_qk_dim = self.key_dim // num_heads\n",
      "        self.head_v_dim = self.value_dim // num_heads\n",
      "\n",
      "        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n",
      "        self.x_proj = nn.Sequential(\n",
      "            LerpLinear(hidden_size, proj_low_rank_dim * 5, device=device, dtype=dtype),\n",
      "            nn.Tanh(),\n",
      "            nn.Linear(proj_low_rank_dim * 5, hidden_size, bias=False, device=device, dtype=dtype)\n",
      "        )\n",
      "        self.x_bias = nn.Parameter(torch.zeros(5, hidden_size, device=device, dtype=dtype))\n",
      "\n",
      "        self.r_proj = DDLerpLinear(hidden_size, self.key_dim, device=device, dtype=dtype)\n",
      "        self.w_proj = DDLerpLinear(hidden_size, self.key_dim, low_rank_dim=gate_low_rank_dim, device=device, dtype=dtype)\n",
      "        self.k_proj = DDLerpLinear(hidden_size, self.key_dim, device=device, dtype=dtype)\n",
      "        self.v_proj = DDLerpLinear(hidden_size, self.value_dim, device=device, dtype=dtype)\n",
      "        self.g_proj = DDLerpLinear(hidden_size, self.value_dim, low_rank_dim=gate_low_rank_dim, device=device, dtype=dtype)\n",
      "        self.bonus = nn.Parameter(torch.zeros(num_heads, self.head_qk_dim, device=device, dtype=dtype))\n",
      "\n",
      "        # self.g_norm = nn.GroupNorm(self.num_heads, self.value_dim, affine=elementwise_affine, eps=norm_eps, device=device, dtype=dtype) # buggy now\n",
      "        self.g_norm = nn.LayerNorm(self.value_dim, elementwise_affine=elementwise_affine, eps=norm_eps, device=device, dtype=dtype)\n",
      "        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False, device=device, dtype=dtype)\n",
      "        self.gate_fn = ACT2FN[gate_fn]\n",
      "\n",
      "        self.apply(self._initialize_weights)\n",
      "\n",
      "    def _initialize_weights(self, module: nn.Module):\n",
      "        if getattr(module, \"_is_hf_initialized\", False):\n",
      "            return\n",
      "        if isinstance(module, nn.Linear):\n",
      "            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n",
      "            if module.bias is not None:\n",
      "                nn.init.zeros_(module.bias)\n",
      "        if isinstance(module, nn.Parameter):\n",
      "            nn.init.xavier_uniform_(module, gain=2 ** -2.5)\n",
      "        module._is_hf_initialized = True\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        **kwargs\n",
      "    ) -> Tuple[torch.Tensor]:\n",
      "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
      "        # launching the triton kernel for just one token will actually be slower\n",
      "        last_state = None\n",
      "\n",
      "        if hidden_states.shape[1] == 1 and last_state is not None:\n",
      "            shifted = last_state[0].unsqueeze(1)\n",
      "        else:\n",
      "            shifted = self.time_shift(hidden_states)\n",
      "            if last_state is not None:\n",
      "                shifted[:, 0] = last_state[0]\n",
      "\n",
      "        delta = shifted - hidden_states\n",
      "        x = self.x_proj[0](hidden_states, delta).view(batch_size, seq_len, -1, self.proj_low_rank_dim)\n",
      "        x = torch.einsum('b l n r, h n r-> b l n h', self.x_proj[1](x), self.x_proj[2].weight.view(hidden_size, 5, -1))\n",
      "\n",
      "        r, w, k, v, g = x.add_(self.x_bias).unbind(-2)\n",
      "        r = self.r_proj(hidden_states, r, delta)\n",
      "        w = self.w_proj(hidden_states, w, delta)\n",
      "        k = self.k_proj(hidden_states, k, delta)\n",
      "        v = self.v_proj(hidden_states, v, delta)\n",
      "        g = self.g_proj(hidden_states, g, delta)\n",
      "\n",
      "        r, w, k, v = map(lambda x: rearrange(x, 'b l (h d) -> b h l d', h=self.num_heads), (r, w, k, v))\n",
      "        w = -torch.exp(w)\n",
      "        u = self.bonus\n",
      "\n",
      "        o = naive_recurrent_rwkv6(r, k, v, w, u, scale=1.0)\n",
      "\n",
      "        o = rearrange(o, 'b h l d -> b l (h d)')\n",
      "        o = self.g_norm(o)\n",
      "        o = o * self.gate_fn(g)\n",
      "        o = self.o_proj(o)\n",
      "\n",
      "        # o = o[:, :_seqlen]\n",
      "        return o\n",
      "\n",
      "    def init_state(self, batch_size: int) -> Tuple[torch.Tensor]:\n",
      "        param = next(self.parameters())\n",
      "        state = [param.new_zeros(batch_size, self.hidden_size),\n",
      "                 param.new_zeros(batch_size, self.num_heads, self.head_qk_dim, self.head_v_dim)]\n",
      "        return state\n",
      "\n",
      "    def state_size(self, **kwargs) -> int:\n",
      "        state_size = self.key_dim * self.head_v_dim\n",
      "        return state_size\n",
      "\n",
      "\n",
      "class LoRA(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        input_dim: int,\n",
      "        output_dim: int,\n",
      "        low_rank_dim: int,\n",
      "        bias: Optional[bool] = True,\n",
      "        device=None,\n",
      "        dtype=None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "\n",
      "        self.input_dim = input_dim\n",
      "        self.output_dim = output_dim\n",
      "        self.low_rank_dim = low_rank_dim\n",
      "        self.bias = bias\n",
      "\n",
      "        self.lora = nn.Sequential(\n",
      "            nn.Linear(input_dim, low_rank_dim, bias=False, device=device, dtype=dtype),\n",
      "            nn.Tanh(),\n",
      "            nn.Linear(low_rank_dim, output_dim, bias=bias, device=device, dtype=dtype)\n",
      "        )\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        s = f\"{self.__class__.__name__}(\"\n",
      "        s += f\"input_dim={self.input_dim}, low_rank_dim={self.low_rank_dim}, output_dim={self.output_dim}\"\n",
      "        if not self.bias:\n",
      "            s += f\", bias={self.bias}\"\n",
      "        s += \")\"\n",
      "        return s\n",
      "\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        return self.lora(x)\n",
      "\n",
      "\n",
      "class LerpLinear(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        input_dim: int,\n",
      "        output_dim: int,\n",
      "        low_rank_dim: Optional[int] = None,\n",
      "        device=None,\n",
      "        dtype=None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "\n",
      "        self.input_dim = input_dim\n",
      "        self.output_dim = output_dim\n",
      "        self.low_rank_dim = low_rank_dim\n",
      "\n",
      "        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n",
      "        if low_rank_dim is None:\n",
      "            self.linear = nn.Linear(input_dim, output_dim, bias=False, device=device, dtype=dtype)\n",
      "        else:\n",
      "            self.linear = LoRA(input_dim, output_dim, low_rank_dim, device=device, dtype=dtype)\n",
      "        self.mu = nn.Parameter(torch.zeros(input_dim, device=device, dtype=dtype))\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        s = f\"{self.__class__.__name__}({self.input_dim}, {self.output_dim}\"\n",
      "        if self.low_rank_dim is not None:\n",
      "            s += f\", low_rank_dim={self.low_rank_dim}\"\n",
      "        s += \")\"\n",
      "        return s\n",
      "\n",
      "    def forward(self, x: torch.Tensor, delta: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
      "        if delta is None:\n",
      "            shifted = self.time_shift(x)\n",
      "            if len(shifted.shape) == 2:\n",
      "                shifted = shifted.unsqueeze(1)\n",
      "            delta = shifted - x\n",
      "        return self.linear(x + delta * self.mu)\n",
      "\n",
      "\n",
      "class DDLerpLinear(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        input_dim: int,\n",
      "        output_dim: int,\n",
      "        low_rank_dim: Optional[int] = None,\n",
      "        device=None,\n",
      "        dtype=None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "\n",
      "        self.input_dim = input_dim\n",
      "        self.output_dim = output_dim\n",
      "        self.low_rank_dim = low_rank_dim\n",
      "\n",
      "        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n",
      "        if low_rank_dim is None:\n",
      "            self.linear = nn.Linear(input_dim, output_dim, bias=False, device=device, dtype=dtype)\n",
      "        else:\n",
      "            self.linear = LoRA(input_dim, output_dim, low_rank_dim, device=device, dtype=dtype)\n",
      "\n",
      "    def __repr__(self) -> str:\n",
      "        s = f\"{self.__class__.__name__}({self.input_dim}, {self.output_dim}\"\n",
      "        if self.low_rank_dim is not None:\n",
      "            s += f\", low_rank_dim={self.low_rank_dim}\"\n",
      "        s += \")\"\n",
      "        return s\n",
      "\n",
      "    def forward(self, x: torch.Tensor, mu: torch.Tensor, delta: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
      "        if delta is None:\n",
      "            shifted = self.time_shift(x)\n",
      "            if len(shifted.shape) == 2:\n",
      "                shifted = shifted.unsqueeze(1)\n",
      "            delta = shifted - x\n",
      "        return self.linear(x + delta * mu)\n",
      "    \n",
      "\n",
      "class RWKV6FeedForward(nn.Module):\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        hidden_size: int,\n",
      "        device=None,\n",
      "        dtype=None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "\n",
      "        self.hidden_size = hidden_size\n",
      "        hidden_ratio = 3.5\n",
      "        intermediate_size = int(hidden_size * hidden_ratio)\n",
      "        intermediate_size = 32 * ((intermediate_size + 32 - 1) // 32)\n",
      "        self.hidden_ratio = hidden_ratio\n",
      "        self.intermediate_size = intermediate_size\n",
      "\n",
      "        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n",
      "\n",
      "        self.key = LerpLinear(hidden_size, intermediate_size, device=device, dtype=dtype)\n",
      "        self.value = nn.Linear(intermediate_size, hidden_size, bias=False, device=device, dtype=dtype)\n",
      "        self.receptance = LerpLinear(hidden_size, hidden_size, device=device, dtype=dtype)\n",
      "        self.relu=nn.ReLU()\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        x: torch.Tensor,\n",
      "    ) -> torch.Tensor:\n",
      "        shifted = self.time_shift(x)\n",
      "        delta = shifted - x\n",
      "        # key = self.act_fn(self.key(x, delta))\n",
      "        _key=self.key(x,delta)\n",
      "        r=self.relu(_key)\n",
      "        key=r*r\n",
      "        value = self.value(key)\n",
      "        receptance = self.receptance(x, delta)\n",
      "\n",
      "        return receptance.sigmoid() * value\n",
      "\n",
      "\n",
      "class GAB(GABBase):\n",
      "    \"\"\"Generalized Autoregressive Block\n",
      "        Input:        X: (batch, seqlen, embed_dim)\n",
      "        Output:       Y: (batch, seqlen, embed_dim)\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "    def __init__(\n",
      "            self,\n",
      "            embed_dim: int, \n",
      "            device=None,\n",
      "            dtype=None,\n",
      "            num_heads: int = 4,\n",
      "            proj_low_rank_dim: int = 32,\n",
      "            gate_low_rank_dim: int = 64,\n",
      "            norm_eps: float = 1e-5,\n",
      "            **kwargs,\n",
      "        ): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
      "        # argv: list of hyperparameters\n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to nn layers\n",
      "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
      "        \n",
      "        # COMPLETING THE CODE HERE #\n",
      "        self.hidden_size = embed_dim\n",
      "\n",
      "        self.attn_norm = nn.LayerNorm(self.hidden_size, bias=True, eps=norm_eps, **factory_kwargs)\n",
      "        self.attn = RWKV6Attention(\n",
      "            hidden_size=self.hidden_size,\n",
      "            num_heads=num_heads,\n",
      "            proj_low_rank_dim=proj_low_rank_dim,\n",
      "            gate_low_rank_dim=gate_low_rank_dim,\n",
      "            norm_eps=norm_eps,\n",
      "            **factory_kwargs\n",
      "        )\n",
      "        self.ffn_norm = nn.LayerNorm(self.hidden_size, bias=True, eps=norm_eps, **factory_kwargs)\n",
      "        self.ffn = RWKV6FeedForward(\n",
      "            hidden_size=self.hidden_size,\n",
      "            **factory_kwargs\n",
      "        )\n",
      "\n",
      "\n",
      "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
      "\n",
      "\n",
      "    def _forward(self,X,**kwargs): # type hints are optional but recommended\n",
      "        # THE CODE HERE MUST BE COMPLETED #\n",
      "        hidden_states = self.attn_norm(X)\n",
      "        X = self.attn(hidden_states) +X\n",
      "        hidden_states = self.ffn_norm(X)\n",
      "        X = self.ffn(hidden_states) + X\n",
      "        return X\n",
      "    \n",
      "    \n",
      "\"\"\" The dictionary of hyperparameters for constructing a GAB layer\n",
      "    embed_dim, device, dtype should NOT be included in gab_config\n",
      "\"\"\"\n",
      "gab_config = {\n",
      "    # THE HYPERPARAMETERS OF ADDITIONAL ARGUMENTS IN GAB CLASS #\n",
      "    'num_heads': 4,\n",
      "    'proj_low_rank_dim': 32,\n",
      "    'gate_low_rank_dim': 64,\n",
      "    'norm_eps': 1e-5,\n",
      "}\n",
      "\n",
      "```\n",
      "\n",
      "</details>\n",
      "\n",
      "###### Related Contents\n",
      "**Excerpt 1 (Score: 0.00)**\n",
      "et al., 2023). As shown in Table 13, the new model consistently outperforms the old one on various tasks. It is to be noted that the new model remains very sensitive to the selected prompt template, just as the old one, as was shown in (Peng et al., 2023). | Dataset | Eagle-7B | Raven-7b |\n",
      "| :--- | :--- | :--- |\n",
      "| Aggression | $\\mathbf{0 . 6 5 8 7}$ | 0.4063 |\n",
      "| MathQA | $\\mathbf{0 . 4 7 6 0}$ | 0.4028 |\n",
      "| Sarcasm | 0.4679 | $\\mathbf{0 . 4 7 8 2}$ |\n",
      "| TweetSent | 0.5355 | $\\mathbf{0 . 5 5 4 1}$ |\n",
      "| Unhealthy | $\\mathbf{0 . 2 9 8 6}$ | 0.2834 |\n",
      "| TweetStance | $\\mathbf{0 . 3 9 3 3}$ | 0.3070 |\n",
      "| Spam | $\\mathbf{0 . 7 2 9 0}$ | 0.4902 |\n",
      "| ColBER | $\\mathbf{0 . 4 0 8 8}$ | 0.2889 |\n",
      "| CoLa | $\\mathbf{0 . 5 2 8 5}$ | 0.4677 |\n",
      "| TextEntail | $\\mathbf{0 . 7 7 6 5}$ | 0.6137 |\n",
      "| GoEmo | $\\mathbf{0 . 0 9 5 6}$ | 0.0814 |\n",
      "| PolEmo | $\\mathbf{0 . 5 0 3 7}$ | 0.2639 |\n",
      "| WNLI | $\\mathbf{0 . 5 2 5 7}$ | 0.4638 |\n",
      "\n",
      "Table 13: Eagle 7B and Raven 7B reasoning performance comparison based on subsets of selected datasets. The used metric is F1-macro (except for MathQA where accuracy is used instead). ## G Hyperparameters\n",
      "\n",
      "All Eagle and Finch models were trained under bfloat 16 format for most parameters, except that float 32 was used to compute $W K V$ for numerical stability. The Adam optimizer was configured with $\\beta_{1}=0.9, \\beta_{2}=0.99$ and 0.001 weight decay applied only to linear layers and\n",
      "embedding weights. The context length for pretraining was 4096 tokens. Learning rate for all models followed a linear 10 step warmup schedule from $20 \\%$ to $100 \\%$ of the maximum learning rate, followed by cosine decay to the minimum learning rate. The time_decay $w$ parameters are placed into a special 2x learning rate multiplier grouping. | Parameters | 0.4 B | $1.5 \\mathrm{~B} / 1.6 \\mathrm{~B}$ | 3 B | 7 B |\n",
      "| :--- | :---: | :---: | :---: | :---: |\n",
      "| Max LR | $4 \\times 10^{-4}$ | $3 \\times 10^{-4}$ | $2 \\times 10^{-4}$ | $1.5 \\times 10^{-4}$ |\n",
      "| Min LR | $2 \\times 10^{-5}$ | $2 \\times 10^{-5}$ | $1.5 \\times 10^{-5}$ | $1 \\times 10^{-5}$ |\n",
      "| Micro Batch Size | 8 | 8 | 4 | 9 |\n",
      "| GPU Count | 24 | 48 | 48 | 64 |\n",
      "| GPU Type | A 100 | A 100 | A100 | H800 |\n",
      "| Batch Size | 786432 | 1572864 | 786432 | 2359296 |\n",
      "\n",
      "Table 14: Learning Rate Hyperparameters for pretrained Eagle and Finch models\n",
      "\n",
      "## H Parameter Initializations\n",
      "\n",
      "Throughout this section, we use $l$ to denote the layer index (layer $l=0$ accepts input embeddings and layer $l=L-1$ produces output), and $i$ the dimension index $(i=0,1, \\cdots, D-1)$. We set $r_{0}=\\frac{l}{L-1}$ and $r_{1}=1-\\frac{l}{L}$ as two parameters for simplicity. The initialization of Eagle is provided as follows:\n",
      "\n",
      "- In the Time Mixing module:\n",
      "- The token-shift coefficients of receptance and gate, $\\mu_{r}$ and $\\mu_{g}$, are initialized to $1-\\left(\\frac{i}{D}\\right)^{r_{1} / 2}$ for i over dimension indices. - The token-shift of key $\\mu_{k}$ is initialized to $1-\\left(\\frac{i}{D}\\right)^{r_{1}}$. - The token-shift of value $\\mu_{\\nu}$ is initialized to $1-\\left(\\frac{i}{D}\\right)^{r_{1}}-0.3 r_{0}$. - The time_decay $w$ is initialized to $-6+5\\left(\\frac{i}{D-1}\\right)^{0.7+1.3 r_{0}}$. - The \"time-first\" $u$ is initialized to $r_{0}\\left(1-\\frac{i}{D-1}\\right)+0.1((i+1) \\bmod 3)$. - The Time Mixing output matrix is initialized to 0. - The WKV GroupNorm weights are initialized with constant value $((1+l) / L)^{0.7}$. - Two-dimensional parameters with the first dimension being larger than the second dimension are initialized with and orthogonal initialization of gain equal to the size of the first dimension divided by the size of the second dimension. - Other parameters are initialized according to PyTorch default. - In the Channel Mixing module:\n",
      "- The token-shift of both key $\\mu_{k}$ and receptance $\\mu_{r}$ are initialized to $1-\\left(\\frac{i}{D}\\right)^{r_{1}}$. - The value and receptance matrices $W_{v}, W_{r}$ are initialized to 0 . - Two-dimensional parameters with the first dimension being larger than the second dimension are initialized with and orthogonal initialization of gain equal to the size of the first dimension divided by the size of the second dimension. - All other parameters are initialized according to PyTorch default. - The input embedding is initialized with a uniform distribution of $\\mathscr{U}(-\\max L R, \\max L R)$, the maximum learning rate. - The output head is initialized with an orthogonal initialization of gain 0.5 . - Bias is set to Fal se for all linear layers. In the Finch architecture, most of the parameters are initialized to the same as Eagle, except for a few changes. In the Time Mixing block, there are several additional parameters initialized as follows:\n",
      "\n",
      "- The token shift of input $\\mu_{x}$ and time decay $\\mu_{w}$ are initialized to $1-\\left(\\frac{i}{D}\\right)^{r_{1}}$. - The lora weights of $\\boldsymbol{A}$ and $\\boldsymbol{B}$ are initialized to uniform distribution of $\\mathscr{U}\\left(-10^{-4}, 10^{-4}\\right)$. ## I Non-English Chat Examples\n",
      "\n",
      "The following are examples of interactions with the base Eagle 7B model in languages other than English. No system prompt was given. Requests are shown in bold. ## RWKV-Eagle\n",
      "\n",
      "User: hi\n",
      "Assistant: Hi.\n",
      "\n",
      "\n",
      "##### 5. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints (Avg. Score: 0.00)\n",
      "\n",
      "*J. Ainslie, J. Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr'on, Sumit K. Sanghai*\n",
      "\n",
      "**TL;DR:** This work proposes a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and introduces grouped-query attention (GQA), a generalization of multi- query attention which uses an intermediate number of query heads.\n",
      "\n",
      "**Abstract:** Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.\n",
      "\n",
      "**Venue:** Conference on Empirical Methods in Natural Language Processing\n",
      "\n",
      "**Year:** 2023\n",
      "\n",
      "**Citations:** 208  (*Influential: 12*)**Semantic Scholar ID:** 5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200\n",
      "\n",
      "###### Reference Code\n",
      "<details><summary>Click to expand</summary>\n",
      "\n",
      "```\n",
      "from typing import Optional, Tuple, Union\n",
      "\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "from einops import einsum, rearrange\n",
      "from torch import Tensor, nn\n",
      "\n",
      "\n",
      "def scaled_dot_product_gqa(\n",
      "    query: Tensor,\n",
      "    key: Tensor,\n",
      "    value: Tensor,\n",
      "    dropout: float = 0.0,\n",
      "    scale: Optional[float] = None,\n",
      "    mask: Optional[Tensor] = None,\n",
      "    is_causal: Optional[bool] = None,\n",
      "    need_weights: bool = False,\n",
      "    average_attn_weights: bool = False,\n",
      "    force_grouped: bool = False,\n",
      "):\n",
      "    \"\"\"Scaled dot product attention with support for grouped queries.\n",
      "\n",
      "    Einstein notation:\n",
      "    - b: batch size\n",
      "    - n / s: sequence length\n",
      "    - h: number of heads\n",
      "    - g: number of groups\n",
      "    - d: dimension of query/key/value\n",
      "\n",
      "    Args:\n",
      "        query: Query tensor of shape (b, n, h, d)\n",
      "        key: Key tensor of shape (b, s, h, d)\n",
      "        value: Value tensor of shape (b, s, h, d)\n",
      "        dropout: Dropout probability (default: 0.0)\n",
      "        scale: Scale factor for query (default: d_query ** 0.5)\n",
      "        mask: Mask tensor of shape (b, n, s) or (b, s). If 'ndim == 2', the mask is\n",
      "            applied to all 'n' rows of the attention matrix. (default: None)\n",
      "        force_grouped: If True, apply grouped-query attention even if the number of\n",
      "            heads is equal for query, key, and value. (default: False)\n",
      "\n",
      "    Returns:\n",
      "        2-tuple of:\n",
      "        - Attention output with shape (b, n, h, d)\n",
      "        - (Optional) Attention weights with shape (b, h, n, s). Only returned if\n",
      "          'need_weights' is True.\n",
      "    \"\"\"\n",
      "    if (mask is not None) and (is_causal is not None):\n",
      "        raise ValueError(\n",
      "            \"Only one of 'mask' and 'is_causal' should be provided, but got both.\"\n",
      "        )\n",
      "    elif not query.ndim == key.ndim == value.ndim == 4:\n",
      "        raise ValueError(\n",
      "            f\"Expected query, key, and value to be 4-dimensional, but got shapes \"\n",
      "            f\"{query.shape}, {key.shape}, and {value.shape}.\"\n",
      "        )\n",
      "\n",
      "    # Move sequence length dimension to axis 2.\n",
      "    # This makes the attention operations below *much* faster.\n",
      "    query = rearrange(query, \"b n h d -> b h n d\")\n",
      "    key = rearrange(key, \"b s h d -> b h s d\")\n",
      "    value = rearrange(value, \"b s h d -> b h s d\")\n",
      "\n",
      "    bq, hq, nq, dq = query.shape\n",
      "    bk, hk, nk, dk = key.shape\n",
      "    bv, hv, nv, dv = value.shape\n",
      "    if not (bq == bk == bv and dq == dk == dv):\n",
      "        raise ValueError(\n",
      "            \"Expected query, key, and value to have the same batch size (dim=0) and \"\n",
      "            f\"embedding dimension (dim=3), but got query: {query.shape}, \"\n",
      "            f\"key: {key.shape}, and value: {value.shape}.\"\n",
      "        )\n",
      "    elif (hk != hv) or (nk != nv):\n",
      "        raise ValueError(\n",
      "            \"Expected key and value to have the same size in dimensions 1 and 2, but \"\n",
      "            f\"got key: {key.shape} and value: {value.shape}.\"\n",
      "        )\n",
      "    elif hq % hk != 0:\n",
      "        raise ValueError(\n",
      "            \"Expected query heads to be a multiple of key/value heads, but got \"\n",
      "            f\"query: {query.shape} and key/value: {key.shape}.\"\n",
      "        )\n",
      "\n",
      "    if scale is None:\n",
      "        scale = query.size(-1) ** 0.5\n",
      "    query = query / scale\n",
      "\n",
      "    num_head_groups = hq // hk\n",
      "    query = rearrange(query, \"b (h g) n d -> b g h n d\", g=num_head_groups)\n",
      "    similarity = einsum(query, key, \"b g h n d, b h s d -> b g h n s\")\n",
      "\n",
      "    if is_causal:\n",
      "        # Mask out the upper triangular portion of the attention matrix. This prevents\n",
      "        # the model from attending to tokens in the future.\n",
      "        mask = torch.ones((bq, nq, nk), device=query.device, dtype=torch.bool).tril_()\n",
      "\n",
      "    if mask is not None:\n",
      "        # Expand mask to match the shape of the attention matrix.\n",
      "        # If mask is 2D, assume that it is applied to the key/value sequence dimension.\n",
      "        # Else if mask is 3D, assume that it is applied to the query/key/value sequence\n",
      "        # dimension for all attention heads.\n",
      "        #\n",
      "        # Users could also provide a 4D mask, which is applied to the query/key/value\n",
      "        # sequence dimension for each attention head (though I don't have a particular\n",
      "        # use case in mind for that).\n",
      "        if mask.ndim == 2:\n",
      "            mask = rearrange(mask, \"b s -> b () () () s\")\n",
      "        elif mask.ndim == 3:\n",
      "            mask = rearrange(mask, \"b n s -> b () () n s\")\n",
      "        # Mask similarity values by setting them to negative infinity.  This guarantees\n",
      "        # that they will not contribute to the softmax computation below.\n",
      "        similarity.masked_fill_(~mask, torch.finfo(similarity.dtype).min)\n",
      "\n",
      "    attention = F.softmax(similarity, dim=-1)\n",
      "    if dropout > 0.0:\n",
      "        attention = F.dropout(attention, p=dropout)\n",
      "\n",
      "    # Apply attention matrix to the value Tensor.\n",
      "    out = einsum(attention, value, \"b g h n s, b h s d -> b g h n d\")\n",
      "    # Move head dimension back to axis 2\n",
      "    out = rearrange(out, \"b g h n d -> b n (h g) d\")\n",
      "\n",
      "    attn_weights: Optional[Tensor] = None\n",
      "    if need_weights:\n",
      "        # Move the sequence dimensions back to positions 1, 2.  Move the head dimension\n",
      "        # to position 3.  This more closely matches the return shape of the attention\n",
      "        # output: (b, n, h, d).\n",
      "        attn_weights = rearrange(attention, \"b g h n s -> b n s (h g)\")\n",
      "        if average_attn_weights:\n",
      "            attn_weights = attn_weights.mean(dim=1)\n",
      "\n",
      "    return out, attn_weights\n",
      "\n",
      "\n",
      "class MultiheadGQA(nn.Module):\n",
      "    \"\"\"Multi-head grouped query attention (GQA) layer.\n",
      "\n",
      "    Reference:\n",
      "        \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\"\n",
      "        https://arxiv.org/pdf/2305.13245v1.pdf\n",
      "\n",
      "    GQA is a variant of multihead attention (MHA) that uses fewer write heads\n",
      "    (key / value) than query heads.  GQA can be viewed as a generalization of\n",
      "    multi-query attention (MQA), which uses a single write head. GQA and MQA give\n",
      "    significant speedups over standard MHA in decoder layers, with minimal loss in\n",
      "    accuracy. In the paper, GQA is shown to be more accurate than MQA, while still\n",
      "    having a significant speedup over MHA.\n",
      "\n",
      "    NOTE: The original authors only benchmark GQA by adapting the T5 (XL or XXL) model\n",
      "    from MHA to GQA.  As a result, they do not mention parameter initialization or\n",
      "    layer normalization strategies.  I follow the best practices laid out in the\n",
      "    MAGNETO paper, which improves Transformer performance through better parameter\n",
      "    initialization and layer norm placement.  See:\n",
      "        https://arxiv.org/pdf/2210.06423.pdf, Fig. 2\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        embed_dim: int,\n",
      "        query_heads: int,\n",
      "        kv_heads: int,\n",
      "        dropout: float = 0.0,\n",
      "        bias: bool = True,\n",
      "        layer_norm: bool = True,\n",
      "        layer_norm_eps: float = 1e-5,\n",
      "        gamma_init: float = 1.0,\n",
      "        device: Optional[Union[torch.device, str]] = None,\n",
      "        dtype: Optional[torch.dtype] = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.query_heads = query_heads\n",
      "        self.kv_heads = kv_heads\n",
      "        self.dropout = dropout\n",
      "        self.layer_norm = layer_norm\n",
      "        self.gamma_init = gamma_init\n",
      "\n",
      "        if self.query_heads % self.kv_heads != 0:\n",
      "            raise ValueError(\n",
      "                f\"query_heads ({query_heads}) must be divisible by \"\n",
      "                f\"kv_heads ({kv_heads})\"\n",
      "            )\n",
      "        elif (embed_dim % self.query_heads != 0) or (embed_dim % self.kv_heads != 0):\n",
      "            raise ValueError(\n",
      "                f\"embed_dim ({embed_dim}) must be divisible by \"\n",
      "                f\"query_heads ({query_heads}) and kv_heads ({kv_heads})\"\n",
      "            )\n",
      "\n",
      "        head_dim = embed_dim // query_heads\n",
      "        if not head_dim % 8 == 0:\n",
      "            raise ValueError(\n",
      "                f\"head_dim (embed_dim / num_heads = {head_dim}) must be divisible by 8\"\n",
      "            )\n",
      "        if not head_dim <= 128:\n",
      "            raise ValueError(\n",
      "                f\"head_dim (embed_dim / num_heads = {head_dim}) must be <= 128\"\n",
      "            )\n",
      "\n",
      "        # Query projection layer is the same as in vanilla MHA.\n",
      "        self.q_proj = nn.Linear(\n",
      "            embed_dim, embed_dim, bias=bias, device=device, dtype=dtype\n",
      "        )\n",
      "        # Key/value projection layers have a smaller output dimension, so that\n",
      "        # the we have fewer key/value attention heads after reshaping.\n",
      "        kv_embed_dim = embed_dim // query_heads * kv_heads\n",
      "        self.k_proj = nn.Linear(\n",
      "            embed_dim, kv_embed_dim, bias=bias, device=device, dtype=dtype\n",
      "        )\n",
      "        self.v_proj = nn.Linear(\n",
      "            embed_dim, kv_embed_dim, bias=bias, device=device, dtype=dtype\n",
      "        )\n",
      "        self.norm: Optional[nn.LayerNorm] = None\n",
      "        if layer_norm:\n",
      "            self.norm = nn.LayerNorm(\n",
      "                embed_dim, eps=layer_norm_eps, device=device, dtype=dtype\n",
      "            )\n",
      "        # Grouped attention output will have the same embedding dimension as the\n",
      "        # key/value Tensors.  So the output projection layer needs to accept the\n",
      "        # same dimension (kv_embed_dim).\n",
      "        self.out_proj = nn.Linear(\n",
      "            embed_dim, embed_dim, bias=bias, device=device, dtype=dtype\n",
      "        )\n",
      "\n",
      "        self._reset_parameters()\n",
      "\n",
      "    def _reset_parameters(self):\n",
      "        nn.init.xavier_normal_(self.q_proj.weight)\n",
      "        if self.q_proj.bias is not None:\n",
      "            nn.init.constant_(self.q_proj.bias, 0)\n",
      "        nn.init.xavier_normal_(self.k_proj.weight)\n",
      "        if self.k_proj.bias is not None:\n",
      "            nn.init.constant_(self.k_proj.bias, 0)\n",
      "\n",
      "        # NOTE: We follow the initialization strategy from MAGNETO.  See:\n",
      "        # https://arxiv.org/pdf/2210.06423.pdf, Fig. 2\n",
      "        # Gain (self.gamma_init) should be provided as a keyword argument when\n",
      "        # initializing the larger Transformer model, since it requires knowledge\n",
      "        # of the number of encoder/decoder layers in the model.\n",
      "\n",
      "        nn.init.xavier_normal_(self.v_proj.weight, gain=self.gamma_init)\n",
      "        if self.v_proj.bias is not None:\n",
      "            nn.init.constant_(self.v_proj.bias, 0)\n",
      "        nn.init.xavier_normal_(self.out_proj.weight, gain=self.gamma_init)\n",
      "        if self.out_proj.bias is not None:\n",
      "            nn.init.constant_(self.out_proj.bias, 0)\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        query: Tensor,\n",
      "        key: Tensor,\n",
      "        value: Tensor,\n",
      "        need_weights: bool = False,\n",
      "        # TODO\n",
      "        # attn_mask: Optional[Tensor] = None,\n",
      "        is_causal: bool = False,\n",
      "        average_attn_weights: bool = False,\n",
      "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
      "        # Notation:\n",
      "        #   b - batch size\n",
      "        #   n - sequence length\n",
      "        #   h - number of heads\n",
      "        #   d - embedding dimension\n",
      "        #\n",
      "        # Input shape: (b, n, d)\n",
      "        q: Tensor = self.q_proj(query)\n",
      "        k: Tensor = self.k_proj(key)\n",
      "        v: Tensor = self.v_proj(value)\n",
      "\n",
      "        # Unfold 'd' dimension into 'h' separate attention heads.\n",
      "        q = rearrange(q, \"b n (h d) -> b n h d\", h=self.query_heads)\n",
      "        k = rearrange(k, \"b n (h d) -> b n h d\", h=self.kv_heads)\n",
      "        v = rearrange(v, \"b n (h d) -> b n h d\", h=self.kv_heads)\n",
      "        # Apply attention, then fold 'h' attention heads back into 'd'.\n",
      "        x, attn = scaled_dot_product_gqa(\n",
      "            query=q,\n",
      "            key=k,\n",
      "            value=v,\n",
      "            # TODO\n",
      "            # mask=attn_mask,\n",
      "            is_causal=is_causal,\n",
      "            need_weights=need_weights,\n",
      "            average_attn_weights=average_attn_weights,\n",
      "            force_grouped=False,\n",
      "        )\n",
      "        x = rearrange(x, \"b n h d -> b n (h d)\")\n",
      "\n",
      "        # NOTE: This is different from 'nn.MultiheadAttention'!  We follow the MAGNETO\n",
      "        # architecture (https://arxiv.org/pdf/2210.06423.pdf), which applies an extra\n",
      "        # layer norm before the linear output projection.  The cross-attention layer in\n",
      "        # the MAGNETO decoder does not include this layer norm, so users have the\n",
      "        # option to disable it (layer_norm=False).\n",
      "        if self.layer_norm:\n",
      "            assert self.norm is not None\n",
      "            x = self.norm(x)\n",
      "        # Linear projection on attention outputs.\n",
      "        x = self.out_proj(x)\n",
      "\n",
      "        return x, attn\n",
      "```\n",
      "\n",
      "</details>\n",
      "\n",
      "###### Related Contents\n",
      "**Excerpt 1 (Score: 0.00)**\n",
      "Int.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ppr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
