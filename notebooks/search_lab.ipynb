{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test search utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13:19:48:41,501 INFO     [model_agent.py:184] Agent name=`designer`, model_details={\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"temperature\": 0.3\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized dataset wikitext-2 from C:\\ChengJunyan1\\Research\\model_discovery\\data\\wikitext-2\\tokenized\\meta-llama/Llama-2-7b-hf\\2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13:19:48:43,824 INFO     [model_agent.py:184] Agent name=`debugger`, model_details={\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-13:19:48:44,142 INFO     [model_agent.py:184] Agent name=`claude`, model_details={\n",
      "    \"model_name\": \"gpt-4o\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evoname': 'test_evo_000', 'scales': '14M,31M,70M', 'selection_ratio': '0.25', 'select_method': 'random', 'design_budget': '0'}\n",
      "Evolution system initialized with scales: ['14M', '31M', '70M']\n",
      "Current scale: 0\n",
      "Budgets remaining: {'70M': 1, '31M': 4, '14M': 16}\n",
      "Checkpoint directory: C:\\ChengJunyan1\\Research\\model_discovery\\ckpt\\test_evo_000\n",
      "gpt2 tree loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13:19:48:45,435 INFO     [model_agent.py:184] Agent name=`designer`, model_details={\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"temperature\": 0.3\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized dataset wikitext-2 from C:\\ChengJunyan1\\Research\\model_discovery\\data\\wikitext-2\\tokenized\\meta-llama/Llama-2-7b-hf\\2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13:19:48:46,205 INFO     [model_agent.py:184] Agent name=`debugger`, model_details={\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-13:19:48:46,532 INFO     [model_agent.py:184] Agent name=`claude`, model_details={\n",
      "    \"model_name\": \"gpt-4o\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-13:19:48:46,821 INFO     [discover_namespace_packages.py:12] Discovering subpackages in _NamespacePath(['c:\\\\Users\\\\ChengJunyan1\\\\anaconda3\\\\envs\\\\modis\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "2024-09-13:19:48:46,824 INFO     [discover_plugins.py:9] Looking for plugins in pinecone_plugins.inference\n",
      "2024-09-13:19:48:46,884 INFO     [installation.py:10] Installing plugin inference into PineconeGRPC\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "import importlib\n",
    "import arxiv\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.utils as U\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "headers={\"X-API-KEY\": os.environ['S2_API_KEY']}\n",
    "paper_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}'\n",
    "\n",
    "\n",
    "strparams=[\n",
    "    f\"evoname=test_evo_000\",\n",
    "    \"scales=14M,31M,70M\",\n",
    "    \"selection_ratio=0.25\",\n",
    "    \"select_method=random\",\n",
    "    \"design_budget=0\",\n",
    "]\n",
    "evo_system = BuildEvolution(\n",
    "    strparams=';'.join(strparams),\n",
    "    do_cache=False,\n",
    "    # cache_type='diskcache',\n",
    ")\n",
    "\n",
    "ptree = evo_system.ptree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get paper files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "refs_s2ids={}\n",
    "REF_TYPES=['Reference','ReferenceCore','ReferenceWithCode','ReferenceCoreWithTree']\n",
    "\n",
    "for i in ptree.G.nodes:\n",
    "    node=ptree.G.nodes[i]['data']\n",
    "    if node.type in REF_TYPES:\n",
    "        s2id=node.s2id\n",
    "        if s2id is not None:\n",
    "            refs_s2ids[i]=s2id\n",
    "        else:\n",
    "            print(i)\n",
    "\n",
    "refs=ptree.filter_by_type(REF_TYPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rec and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_rec_papers(s2id):\n",
    "    url=f'https://api.semanticscholar.org/recommendations/v1/papers/forpaper/{s2id}'\n",
    "    rsp=requests.get(url,headers=headers)\n",
    "    results=rsp.json()\n",
    "    if 'error' in results:\n",
    "        print(s2id)\n",
    "        return None\n",
    "    return results['recommendedPapers']\n",
    "\n",
    "recs={}\n",
    "for i in tqdm(refs_s2ids):\n",
    "    s2id=refs_s2ids[i]\n",
    "    rec=get_rec_papers(s2id)\n",
    "    if rec is not None:\n",
    "        recs[i]=rec\n",
    "    time.sleep(0.1)\n",
    "\n",
    "all_recs={}\n",
    "for i in recs:\n",
    "    for j in recs[i]:\n",
    "        all_recs[j['paperId']]=j['title']\n",
    "\n",
    "# all_recs_filtered={}\n",
    "# for i in all_recs:\n",
    "#     if i in refs_s2ids.values():\n",
    "#         continue\n",
    "#     if i in refs2_detail:\n",
    "#         continue\n",
    "#     all_recs_filtered[i]=all_recs[i]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_paper_detail(s2id):\n",
    "    params={\n",
    "        \"fields\": \"title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,authors,embedding,tldr,openAccessPdf,externalIds,references\",\n",
    "    }\n",
    "    rsp=requests.get(\n",
    "        paper_detail.format(paper_id=s2id),\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    results = rsp.json()\n",
    "    return results\n",
    "\n",
    "save_dir=U.pjoin(ptree.lib_dir,'..','tree_ext','plus')\n",
    "refsp_detail={}\n",
    "U.mkdir(save_dir)\n",
    "for s2id in tqdm(all_recs_filtered):\n",
    "    save_path=U.pjoin(save_dir,f'{s2id}.json')\n",
    "    if U.pexists(save_path):\n",
    "        refsp_detail[s2id]=U.load_json(save_path)\n",
    "        continue\n",
    "    ret=get_paper_detail(s2id)\n",
    "    if 'error' not in ret:\n",
    "        U.save_json(ret,save_path)\n",
    "    refsp_detail[s2id]=ret\n",
    "    time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(refsp_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_recs_filtered)\n",
    "\n",
    "refp_arxiv={}\n",
    "refp_pdfs={}\n",
    "for i in refsp_detail:\n",
    "    if 'openAccessPdf' in refsp_detail[i] and refsp_detail[i]['openAccessPdf'] is not None and 'arxiv' not in refsp_detail[i]['openAccessPdf']['url']:\n",
    "        url=refsp_detail[i]['openAccessPdf']\n",
    "        refp_pdfs[i]=url['url']\n",
    "        continue\n",
    "    if 'externalIds' in refsp_detail[i]:\n",
    "        if 'ArXiv' in refsp_detail[i]['externalIds']:\n",
    "            refp_arxiv[i]=refsp_detail[i]['externalIds']['ArXiv']\n",
    "            continue\n",
    "    # print(i)\n",
    "print(len(refp_arxiv))\n",
    "print(len(refp_pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar5iv_url='https://ar5iv.labs.arxiv.org/html/{arxiv_id}'\n",
    "htmlsp_dir=U.pjoin(ptree.lib_dir,'..','files','htmlsp')\n",
    "U.mkdir(htmlsp_dir)\n",
    "pdfsp_dir=U.pjoin(ptree.lib_dir,'..','files','pdfsp')\n",
    "U.mkdir(pdfsp_dir)\n",
    "\n",
    "    \n",
    "# for i in tqdm(refp_arxiv):\n",
    "#     DIR=U.pjoin(htmlsp_dir,f'{i}.html')\n",
    "#     if os.path.exists(DIR):\n",
    "#         continue\n",
    "#     url=ar5iv_url.format(arxiv_id=refp_arxiv[i])\n",
    "\n",
    "#     # Send a GET request to the URL\n",
    "#     response = requests.get(url)\n",
    "\n",
    "#     # Check if the request was successful (status code 200)\n",
    "#     if response.status_code == 200:\n",
    "#         # Save the HTML content to a file\n",
    "#         with open(DIR, \"w\", encoding=\"utf-8\") as file:\n",
    "#             file.write(response.text)\n",
    "#     else:\n",
    "#         print(f\"Failed to retrieve {i}. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "\n",
    "failed_pdfs={}\n",
    "for i in tqdm(refp_pdfs):\n",
    "    url=refp_pdfs[i]\n",
    "    save_path=U.pjoin(pdfsp_dir,f'{i}.pdf')\n",
    "    if not os.path.exists(save_path):\n",
    "        try:\n",
    "            pdf=urllib.request.urlretrieve(url, save_path)\n",
    "        except:\n",
    "            failed_pdfs[i]=url\n",
    "            print(f'Failed to download {i}, {url}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get references and files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"fields\": \"openAccessPdf,externalIds\",\n",
    "}\n",
    "\n",
    "pdf_urls={}\n",
    "external_ids={}\n",
    "for i in tqdm(refs_s2ids):\n",
    "    s2id=refs_s2ids[i]\n",
    "    rsp=requests.get(\n",
    "        paper_detail.format(paper_id=s2id),\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    results = rsp.json()\n",
    "    if 'openAccessPdf' in results and results['openAccessPdf'] is not None:\n",
    "        pdf_urls[i]=results['openAccessPdf']\n",
    "    if 'externalIds' in results and results['externalIds'] is not None:\n",
    "        external_ids[i]=results['externalIds']\n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "arxiv_ids={}\n",
    "for i,j in external_ids.items():\n",
    "    if 'ArXiv' in j:\n",
    "        arxiv_ids[i]=j['ArXiv']\n",
    "\n",
    "arxiv_ids_add={\n",
    "    'fire':'2310.04418',\n",
    "    'mogrifier':'1909.01792',\n",
    "    'hedgehog':'2402.04347',\n",
    "    'synthesizer':'2005.00743',\n",
    "    'settransformer':'1810.00825',\n",
    "    'infiniteformer':'2109.00301',\n",
    "    'etc':'2004.08483',\n",
    "    'feedbackmem':'2002.09402',\n",
    "    'kangpt':'2408.10205',\n",
    "}\n",
    "\n",
    "arxiv_ids.update(arxiv_ids_add)\n",
    "\n",
    "ar5iv_url='https://ar5iv.labs.arxiv.org/html/{arxiv_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node(id):\n",
    "    return ptree.G.nodes[id]['data']\n",
    "\n",
    "\n",
    "def get_references(id):\n",
    "    params={\n",
    "        \"fields\": \"references\",\n",
    "    }\n",
    "    s2id=refs_s2ids[id]\n",
    "    rsp=requests.get(\n",
    "        paper_detail.format(paper_id=s2id),\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    results = rsp.json()\n",
    "    return results['references']\n",
    "\n",
    "\n",
    "all_refs={}\n",
    "for i in tqdm(refs_s2ids):\n",
    "    all_refs[i]=get_references(i)\n",
    "\n",
    "refs_set={}\n",
    "for i in all_refs:\n",
    "    for ref in all_refs[i]:\n",
    "        refs_set[ref['paperId']]=ref['title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_detail(s2id):\n",
    "    params={\n",
    "        \"fields\": \"title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,authors,tldr,embedding,tldr,openAccessPdf,externalIds,references\",\n",
    "    }\n",
    "    rsp=requests.get(\n",
    "        paper_detail.format(paper_id=s2id),\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    results = rsp.json()\n",
    "    return results\n",
    "\n",
    "refs_detail={}\n",
    "save_dir=U.pjoin(ptree.lib_dir,'..','tree_ext','refs2')\n",
    "for i in tqdm(refs_set):\n",
    "    save_path=U.pjoin(save_dir,f'{i}.json')\n",
    "    if U.pexists(save_path):\n",
    "        refs_detail[i]=U.load_json(save_path)\n",
    "        continue\n",
    "    ret=get_paper_detail(i)\n",
    "    if 'error' not in ret:\n",
    "        U.save_json(ret,save_path)\n",
    "    refs_detail[i]=ret\n",
    "    time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs2_detail={}\n",
    "ref2_dir=U.pjoin(ptree.lib_dir,'..','tree_ext','secondary')\n",
    "for i in os.listdir(ref2_dir):\n",
    "    refs2_detail[i.split('.')[0]]=U.load_json(U.pjoin(ref2_dir,i))\n",
    "\n",
    "ref2_arxiv={}\n",
    "ref2_pdfs={}\n",
    "for i in refs_detail:\n",
    "    if 'externalIds' in refs2_detail[i]:\n",
    "        if 'ArXiv' in refs2_detail[i]['externalIds']:\n",
    "            ref2_arxiv[i]=refs2_detail[i]['externalIds']['ArXiv']\n",
    "            continue\n",
    "    if 'openAccessPdf' in refs2_detail[i] and refs2_detail[i]['openAccessPdf'] is not None and 'arxiv' not in refs2_detail[i]['openAccessPdf']['url']:\n",
    "        url=refs_detail[i]['openAccessPdf']\n",
    "        ref2_pdfs[i]=url['url']\n",
    "        continue\n",
    "    # print(i)\n",
    "\n",
    "print(len(ref2_arxiv))\n",
    "print(len(ref2_pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, urllib.request\n",
    "\n",
    "save_dir=U.pjoin(ptree.lib_dir,'..','pdfs2')\n",
    "arxiv_pdf='https://arxiv.org/pdf/{arxiv_id}'\n",
    "U.mkdir(save_dir)\n",
    "\n",
    "# bar=tqdm(ref2_arxiv)\n",
    "# for i in bar:\n",
    "#     bar.set_description(f'Downloading {i}')\n",
    "#     arxiv_id=ref2_arxiv[i]\n",
    "#     download_arxiv_pdf(arxiv_id,save_dir,f'{i}.pdf')\n",
    "\n",
    "failed_pdfs={}\n",
    "for i in tqdm(ref2_pdfs):\n",
    "    url=ref2_pdfs[i]\n",
    "    save_path=U.pjoin(save_dir,f'{i}.pdf')\n",
    "    if not os.path.exists(save_path):\n",
    "        try:\n",
    "            pdf=urllib.request.urlretrieve(url, save_path)\n",
    "        except:\n",
    "            failed_pdfs[i]=url\n",
    "            print(f'Failed to download {i}, {url}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "save_dir=U.pjoin(ptree.lib_dir,'..','htmls2')\n",
    "U.mkdir(save_dir)\n",
    "ar5iv_url='https://ar5iv.labs.arxiv.org/html/{arxiv_id}'\n",
    "\n",
    "for i in tqdm(ref2_arxiv):\n",
    "    DIR=U.pjoin(save_dir,f'{i}.html')\n",
    "    if os.path.exists(DIR):\n",
    "        continue\n",
    "    url=ar5iv_url.format(arxiv_id=ref2_arxiv[i])\n",
    "        \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Save the HTML content to a file\n",
    "        with open(DIR, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response.text)\n",
    "        # print(f\"HTML content saved to {i}.html\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {i}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmls_dir=U.pjoin(ptree.lib_dir,'..','htmls')\n",
    "pdfs_dir=U.pjoin(ptree.lib_dir,'..','pdfs')\n",
    "\n",
    "htmls=[i.split('.')[0] for i in os.listdir(htmls_dir)]\n",
    "pdfs=[i.split('.')[0] for i in os.listdir(pdfs_dir)]\n",
    "\n",
    "print(len(htmls))\n",
    "print(len(pdfs))\n",
    "\n",
    "arxiv_url='https://arxiv.org/abs/{arxiv_id}'\n",
    "\n",
    "for i in pdfs:\n",
    "    if i not in htmls:\n",
    "        if i in arxiv_ids:\n",
    "            arxiv_id=arxiv_ids[i]\n",
    "            url=ar5iv_url.format(arxiv_id=arxiv_id)\n",
    "            arxiv=arxiv_url.format(arxiv_id=arxiv_id)\n",
    "            print(url,arxiv,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, urllib.request\n",
    "url = 'http://export.arxiv.org/api/query?search_query=\"{query}\"&start=0&max_results=1'\n",
    "title = ptree.G.nodes[refs[0]]['data'].title\n",
    "encoded_title = urllib.parse.quote(title)\n",
    "url=url.format(query=encoded_title)\n",
    "data = urllib.request.urlopen(url)\n",
    "print(data.read().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agents.search_utils \n",
    "importlib.reload(agents.search_utils)\n",
    "\n",
    "from agents.search_utils import SuperScholarSearcher\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dev tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13:19:48:48,850 INFO     [discover_namespace_packages.py:12] Discovering subpackages in _NamespacePath(['c:\\\\Users\\\\ChengJunyan1\\\\anaconda3\\\\envs\\\\modis\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "2024-09-13:19:48:48,852 INFO     [discover_plugins.py:9] Looking for plugins in pinecone_plugins.inference\n",
      "2024-09-13:19:48:48,853 INFO     [installation.py:10] Installing plugin inference into PineconeGRPC\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.search_utils\n",
    "importlib.reload(model_discovery.agents.search_utils)\n",
    "\n",
    "from model_discovery.agents.search_utils import SuperScholarSearcher, pwc_search_patched\n",
    "\n",
    "sss=SuperScholarSearcher(ptree,evo_system.stream)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vector Store for internal Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading splits Secondary:  88%|████████▊ | 2508/2836 [03:06<00:31, 10.36it/s]"
     ]
    }
   ],
   "source": [
    "sss._build_vector_stores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Search \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_docs,metainfo=sss.search_internal('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
