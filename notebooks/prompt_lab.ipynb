{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place for experimenting the progressive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.utils as U\n",
    "from model_discovery.configs.gam_config import GAMConfig, GAMConfig_14M\n",
    "from model_discovery.model.composer import GAUTree,GAUBase,GAUNode\n",
    "# from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "\n",
    "prompts_dir='../model_discovery/agents/prompts/'\n",
    "gab_py = U.read_file(U.pjoin(prompts_dir,'gab_template.py'))\n",
    "gam_py = U.read_file(U.pjoin(prompts_dir,'gam_prompt.py'))\n",
    "GAU_TEMPLATE = U.read_file(U.pjoin(prompts_dir,'gau_template.py'))\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os,sys\n",
    "import importlib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "import model_discovery.agents.prompts.prompts as P\n",
    "import model_discovery.model.composer\n",
    "importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUTree,GAUBase,GAUNode\n",
    "import model_discovery.utils\n",
    "importlib.reload(model_discovery.utils)\n",
    "import model_discovery.utils as U\n",
    "\n",
    "\n",
    "\n",
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "# Placeholder classes for future implementation\n",
    "class MemoryAccessUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, memory_size, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "class DownsamplingUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, downsample_factor, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "\n",
    "class XXX(GAUBase):\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "\n",
    "class GAU(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
    "        # argv: list of hyperparameters\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\n",
    "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        self.a1 = kwarg_all.get('a1', 1)\n",
    "        self.a2: int = kwarg_all.get('a2', 2)\n",
    "\n",
    "        # COMPLETING THE CODE HERE #\n",
    "        self.token_scorer = TokenScoringGAU(embed_dim, **factory_kwargs)\n",
    "        self.dual_path: GAUBase = DualPathGAU(embed_dim, **factory_kwargs)\n",
    "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim, **factory_kwargs)\n",
    "\n",
    "        self.stages: nn.ModuleList = nn.ModuleList([CompressionStageGAU(embed_dim, device=\n",
    "            device, dtype=dtype, **kwargs) for _ in range(2)])\n",
    "\n",
    "        self.modelx=nn.ModuleList([\n",
    "            nn.Conv2d(1,20,5),\n",
    "            nn.ReLU(),\n",
    "            LatentAttentionGAU(),\n",
    "            nn.Conv2d(20,64,5),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "            \n",
    "\n",
    "        self.model1 = (\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU(),\n",
    "            'gau3': CompressionStageGAU()\n",
    "        })\n",
    "\n",
    "        self.model2 = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('gau2', DualPathGAU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "    def init_something(self):\n",
    "        self.modelx[3]=LatentAttentionGAU()\n",
    "        self.modelx.append(LatentAttentionGAU())\n",
    "        self.modelx.insert(0,LatentAttentionGAU())\n",
    "        self.activations['gau1']=LatentAttentionGAU()\n",
    "        self.model2.add_module('gau4',LatentAttentionGAU())\n",
    "\n",
    "        X=self.dual_path(X)\n",
    "    \n",
    "    def _forward(self, X, **Z): \n",
    "\n",
    "        # THE CODE HERE MUST BE COMPLETED #\n",
    "        # Step 1: Score tokens\n",
    "        X, Z = self.token_scorer(X, **Z)\n",
    "        # Step 2: Route through dual paths\n",
    "        # Step 3: Apply latent attention\n",
    "        Y, Z = self.latent_attention(X, A=A, B=B, **Z)\n",
    "        for stage in self.stages:\n",
    "            X = stage(X, **Z)\n",
    "        \n",
    "        X, Z = self.dual_path(X, **Z)\n",
    "        X=self.activations['gau3'](X)\n",
    "\n",
    "        return Y, Z\n",
    "\n",
    "@gau_test\n",
    "def test1(a=1,device=None):\n",
    "    print('test1')\n",
    "\n",
    "    \n",
    "@gau_test\n",
    "def test2(device=None):\n",
    "    print('test2')\n",
    "\n",
    "'''\n",
    "children = ['TokenScoringGAU', 'DualPathGAU', 'LatentAttentionGAU','CompressionStageGAU'] # solve redundancy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "class RelevanceScoringGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT KEEP THE ORIGINAL ONES #\n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        # Initialize a linear projection layer\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim // 2, **self.factory_kwargs)\n",
    "        self.projection.weight.requires_grad = True\n",
    "        self.projection.bias.requires_grad = True\n",
    "\n",
    "        # Initialize a learnable query vector\n",
    "        self.query_vector = nn.Parameter(torch.randn(embed_dim // 2, requires_grad=True, **self.factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        # Project the input embeddings\n",
    "        projected_X = self.projection(X)\n",
    "\n",
    "        # Compute similarity scores\n",
    "        similarity_scores = torch.matmul(projected_X, self.query_vector)\n",
    "\n",
    "        # Normalize the similarity scores\n",
    "        relevance_scores = F.softmax(similarity_scores, dim=-1)\n",
    "\n",
    "        # Weight the input embeddings with relevance scores\n",
    "        Y = X * relevance_scores.unsqueeze(-1)\n",
    "\n",
    "        # Return weighted embeddings and updated Z\n",
    "        return Y, {}\n",
    "\n",
    "\n",
    "# WRITE YOUR UNIT TEST FUNCTIONS HERE #\n",
    "\n",
    "@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON'T BE RECOGNIZED AS A UNIT TEST #\n",
    "def test_relevance_scoring_gau(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\n",
    "    # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\n",
    "\n",
    "    # Initialize the GAU\n",
    "    embed_dim = 512\n",
    "    block_loc = (0, 6)\n",
    "    relevance_scoring_gau = RelevanceScoringGAU(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={}, device=device, dtype=dtype)\n",
    "\n",
    "    # Mock input\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "\n",
    "    # Forward pass\n",
    "    Y, Z_ = relevance_scoring_gau(X, **Z)\n",
    "\n",
    "    # Assertions\n",
    "    assert Y.shape == (batch_size, seq_len, embed_dim), \"Output shape mismatch\"\n",
    "    assert isinstance(Z_, dict), \"Z_ should be a dictionary\"\n",
    "\n",
    "    # Print for debugging\n",
    "    print(\"Output embeddings:\", Y)\n",
    "    print(\"Updated Z:\", Z_)\n",
    "\n",
    "    # Additional checks\n",
    "    assert torch.allclose(Y.sum(dim=-1), X.sum(dim=-1), atol=1e-6), \"Weighted embeddings should preserve the sum along the embedding dimension\"\n",
    "\n",
    "    print(\"RelevanceScoringGAU test passed!\")\n",
    "'''\n",
    "children=['ProjectionGAU','SimilarityComputationGAU','NormalizationGAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_scorer': {'class': 'TokenScoringGAU', 'node': <ast.Assign object at 0x000001E8EC7C3F90>}, 'dual_path': {'class': 'DualPathGAU', 'node': <ast.AnnAssign object at 0x000001E8EDB7A590>}, 'latent_attention': {'class': 'LatentAttentionGAU', 'node': <ast.AnnAssign object at 0x000001E8EC780490>}, 'stages.Comprehension': {'type': 'Comprehension', 'node': <ast.Call object at 0x000001E8EAA14E90>}, 'modelx.ModuleList[2]': {'type': 'ModuleList', 'node': <ast.Call object at 0x000001E8EB4B1D10>}, 'model1.NestedCollection[2]': {'type': 'NestedCollection', 'node': <ast.Call object at 0x000001E8EBD8FF90>}, 'activations.ModuleDict[gau3]': {'type': 'ModuleDict', 'node': <ast.Call object at 0x000001E8ED9E1050>}, 'modelx[3]': {'class': 'LatentAttentionGAU', 'node': <ast.Assign object at 0x000001E8EDB9F050>}, 'activations[gau1]': {'class': 'LatentAttentionGAU', 'node': <ast.Assign object at 0x000001E8EDB9CCD0>}, 'model2.gau4': {'class': 'LatentAttentionGAU', 'node': <ast.Call object at 0x000001E8EDB9FC50>}}\n",
      "70\n",
      "Call(func=Attribute(value=Name(id='self', ctx=Load()), attr='dual_path', ctx=Load()), args=[Name(id='X', ctx=Load())], keywords=[])\n",
      "Name(id='X', ctx=Store())\n",
      "73\n",
      "Call(func=Attribute(value=Name(id='self', ctx=Load()), attr='token_scorer', ctx=Load()), args=[Name(id='X', ctx=Load())], keywords=[keyword(value=Name(id='Z', ctx=Load()))])\n",
      "Tuple(elts=[Name(id='X', ctx=Store()), Name(id='Z', ctx=Store())], ctx=Store())\n",
      "74\n",
      "Call(func=Attribute(value=Name(id='self', ctx=Load()), attr='latent_attention', ctx=Load()), args=[Name(id='X', ctx=Load())], keywords=[keyword(arg='A', value=Name(id='A', ctx=Load())), keyword(arg='B', value=Name(id='B', ctx=Load())), keyword(value=Name(id='Z', ctx=Load()))])\n",
      "Tuple(elts=[Name(id='Y', ctx=Store()), Name(id='Z', ctx=Store())], ctx=Store())\n",
      "77\n",
      "Call(func=Attribute(value=Name(id='self', ctx=Load()), attr='dual_path', ctx=Load()), args=[Name(id='X', ctx=Load())], keywords=[keyword(value=Name(id='Z', ctx=Load()))])\n",
      "Tuple(elts=[Name(id='X', ctx=Store()), Name(id='Z', ctx=Store())], ctx=Store())\n",
      "Reformatted Code:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "class MemoryAccessUnit(nn.Module):\n",
      "\n",
      "    def __init__(self, embed_dim, memory_size, device=None, dtype=None):\n",
      "        super().__init__(embed_dim)\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        return X, {}\n",
      "\n",
      "\n",
      "class DownsamplingUnit(nn.Module):\n",
      "\n",
      "    def __init__(self, embed_dim, downsample_factor, device=None, dtype=None):\n",
      "        super().__init__(embed_dim)\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        return X, {}\n",
      "\n",
      "\n",
      "class XAU(GAUBase):\n",
      "    \"\"\"Generalized Autoregressive Block Unit\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, embed_dim: int, block_loc, kwarg_all, device=None,\n",
      "        dtype=None, a1=1, a2=2, **kwargs):\n",
      "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "        self.token_scorer = TokenScoringGAU(embed_dim=self.embed_dim,\n",
      "            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n",
      "            self.factory_kwargs, **self.kwarg_all)\n",
      "        self.dual_path: GAUBase = DualPathGAU(embed_dim=self.embed_dim,\n",
      "            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n",
      "            self.factory_kwargs, **self.kwarg_all)\n",
      "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim=\n",
      "            self.embed_dim, block_loc=self.block_loc, kwarg_all=\n",
      "            self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n",
      "        self.stages: nn.ModuleList = nn.ModuleList([CompressionStageGAU(\n",
      "            embed_dim=self.embed_dim, block_loc=self.block_loc, kwarg_all=\n",
      "            self.kwarg_all, **self.factory_kwargs, **self.kwarg_all) for _ in\n",
      "            range(2)])\n",
      "        self.modelx = nn.ModuleList([nn.Conv2d(1, 20, 5), nn.ReLU(),\n",
      "            LatentAttentionGAU(embed_dim=self.embed_dim, block_loc=\n",
      "            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n",
      "            **self.kwarg_all), nn.Conv2d(20, 64, 5), nn.ReLU()])\n",
      "        self.model1 = nn.Conv2d(1, 20, 5), nn.ReLU(), LatentAttentionGAU(\n",
      "            embed_dim=self.embed_dim, block_loc=self.block_loc, kwarg_all=\n",
      "            self.kwarg_all, **self.factory_kwargs, **self.kwarg_all\n",
      "            ), nn.Conv2d(20, 64, 5), nn.ReLU(), nn.Sequential(OrderedDict([\n",
      "            ('conv1', nn.Conv2d(1, 20, 5)), ('relu1', nn.ReLU()), ('gau2',\n",
      "            DualPathGAU()), ('conv2', nn.Conv2d(20, 64, 5)), ('relu2', nn.\n",
      "            ReLU())]))\n",
      "        self.model = nn.Sequential(nn.Conv2d(1, 20, 5), nn.ReLU(),\n",
      "            LatentAttentionGAU(), nn.Conv2d(20, 64, 5), nn.ReLU(), nn.\n",
      "            Sequential(OrderedDict([('conv1', nn.Conv2d(1, 20, 5)), (\n",
      "            'relu1', nn.ReLU()), ('gau2', DualPathGAU()), ('conv2', nn.\n",
      "            Conv2d(20, 64, 5)), ('relu2', nn.ReLU())])))\n",
      "        self.activations = nn.ModuleDict({'relu': nn.ReLU(), 'lrelu': nn.\n",
      "            LeakyReLU(), 'gau3': CompressionStageGAU(embed_dim=\n",
      "            self.embed_dim, block_loc=self.block_loc, kwarg_all=\n",
      "            self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)})\n",
      "        self.model2 = nn.Sequential(OrderedDict([('conv1', nn.Conv2d(1, 20,\n",
      "            5)), ('relu1', nn.ReLU()), ('gau2', DualPathGAU()), ('conv2',\n",
      "            nn.Conv2d(20, 64, 5)), ('relu2', nn.ReLU())]))\n",
      "        self.a1 = a1\n",
      "        self.a2: int = a2\n",
      "\n",
      "    def init_something(self):\n",
      "        self.modelx[3] = LatentAttentionGAU(embed_dim=self.embed_dim,\n",
      "            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n",
      "            self.factory_kwargs, **self.kwarg_all)\n",
      "        self.modelx.append(LatentAttentionGAU(embed_dim=self.embed_dim,\n",
      "            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n",
      "            self.factory_kwargs, **self.kwarg_all))\n",
      "        self.modelx.insert(0, LatentAttentionGAU(embed_dim=self.embed_dim,\n",
      "            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n",
      "            self.factory_kwargs, **self.kwarg_all))\n",
      "        self.activations['gau1'] = LatentAttentionGAU(embed_dim=\n",
      "            self.embed_dim, block_loc=self.block_loc, kwarg_all=\n",
      "            self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n",
      "        self.model2.add_module('gau4', LatentAttentionGAU(embed_dim=\n",
      "            self.embed_dim, block_loc=self.block_loc, kwarg_all=\n",
      "            self.kwarg_all, **self.factory_kwargs, **self.kwarg_all))\n",
      "        X = self.dual_path(X)\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        X, Z = self.token_scorer(X, **Z)\n",
      "        Y, Z = self.latent_attention(X, A=A, B=B, **Z)\n",
      "        for stage in self.stages:\n",
      "            X = stage(X, **Z)\n",
      "        X, Z = self.dual_path(X, **Z)\n",
      "        X = self.activations['gau3'](X)\n",
      "        return Y, Z\n",
      "\n",
      "Errors:\n",
      " [\"Error: GAUBase class 'XXX' you defined is not provided in children list, if you need to define a children in your GAU, you need to declare it and provide in your children list.\", 'line 70:         X = self.dual_path(X): Error: GAU call must have the sequence as the first argument and the **Z. If you need to pass in other arguments, you can do so in the **Z.', 'line 70:         X = self.dual_path(X): Error: GAU call always returns a tuple of two variables, the first is a sequence and the second must be the updated **Z. If you need to return other variables, you can do so in the **Z.', 'line 74:         Y, Z = self.latent_attention(X, A=A, B=B, **Z): Error: GAU call must have the sequence as the first argument and the **Z. If you need to pass in other arguments, you can do so in the **Z.', 'line 52:             LatentAttentionGAU(), nn.Conv2d(20, 64, 5), nn.ReLU(), nn.: Error: nn.Sequential is not supported in GAU. You may use ModuleList, ModuleDict.', \"line 53:             Sequential(OrderedDict([('conv1', nn.Conv2d(1, 20, 5)), (: Error: nn.Sequential is not supported in GAU. You may use ModuleList, ModuleDict.\", \"line 58:         self.model2 = nn.Sequential(OrderedDict([('conv1', nn.Conv2d(1, 20,: Error: nn.Sequential is not supported in GAU. You may use ModuleList, ModuleDict.\"]\n",
      "Warnings:\n",
      " ['line 65:         self.modelx[3] = LatentAttentionGAU(): Warning: In-place modification of collection is dynamic and cannot be accurately tracked. It is unsafe as the system cannot track whether it is called properly.', 'line 66:         self.modelx.append(LatentAttentionGAU()): Warning: The index of the appended element is dynamic and cannot be accurately tracked. It is unsafe as the system cannot track whether it is called properly.', 'line 67:         self.modelx.insert(0, LatentAttentionGAU()): Warning: The index of the inserted element is dynamic and cannot be accurately tracked. It is unsafe as the system cannot track whether it is called properly.', \"line 68:         self.activations['gau1'] = LatentAttentionGAU(): Warning: In-place modification of collection is dynamic and cannot be accurately tracked. It is unsafe as the system cannot track whether it is called properly.\"]\n",
      "New Arguments:\n",
      " {'a1': 1, 'a2': 2}\n"
     ]
    }
   ],
   "source": [
    "unit_name = \"XAU\"  # Provide the unit_name to rename GAU class\n",
    "reformatted_code, new_args, gau_tests, errors, warnings = check_and_reformat_gau_code(code, unit_name,children)\n",
    "print(\"Reformatted Code:\\n\" + reformatted_code)\n",
    "print(\"Errors:\\n\", errors)\n",
    "print(\"Warnings:\\n\", warnings)\n",
    "print(\"New Arguments:\\n\", new_args)\n",
    "\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "spec=P.UnitSpec(\n",
    "   unitname = unit_name,\n",
    "   document = 'Test',\n",
    "   inputs = [],\n",
    "   outputs = [],\n",
    ")\n",
    "test_tree.add_unit(\n",
    "    spec, reformatted_code, new_args, None, None, None, children, gau_tests, None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 1: import torch\n",
      "line 2: import torch.nn as nn\n",
      "line 3: from model_discovery.model.utils.modules import GAUBase, gau_test\n",
      "line 4: import torch.nn.functional as F\n",
      "line 5: \n",
      "line 6: \n",
      "line 7: class MemoryAccessUnit(nn.Module):\n",
      "line 8: \n",
      "line 9:     def __init__(self, embed_dim, memory_size, device=None, dtype=None):\n",
      "line 10:         super().__init__(embed_dim)\n",
      "line 11: \n",
      "line 12:     def _forward(self, X, **Z):\n",
      "line 13:         return X, {}\n",
      "line 14: \n",
      "line 15: \n",
      "line 16: class DownsamplingUnit(nn.Module):\n",
      "line 17: \n",
      "line 18:     def __init__(self, embed_dim, downsample_factor, device=None, dtype=None):\n",
      "line 19:         super().__init__(embed_dim)\n",
      "line 20: \n",
      "line 21:     def _forward(self, X, **Z):\n",
      "line 22:         return X, {}\n",
      "line 23: \n",
      "line 24: \n",
      "line 25: class XAU(GAUBase):\n",
      "line 26:     \"\"\"Generalized Autoregressive Block Unit\n",
      "line 27:         Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "line 28:         Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "line 29:         Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "line 30:     \"\"\"\n",
      "line 31: \n",
      "line 32:     def __init__(self, embed_dim: int, block_loc, kwarg_all, device=None,\n",
      "line 33:         dtype=None, a1=1, a2=2, **kwargs):\n",
      "line 34:         self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "line 35:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 36:         factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "line 37:         self.token_scorer = TokenScoringGAU(embed_dim=embed_dim, block_loc=\n",
      "line 38:             block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all)\n",
      "line 39:         self.dual_path: GAUBase = DualPathGAU(embed_dim=embed_dim,\n",
      "line 40:             block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
      "line 41:             **kwarg_all)\n",
      "line 42:         self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim=\n",
      "line 43:             embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **\n",
      "line 44:             self.factory_kwargs, **kwarg_all)\n",
      "line 45:         self.stages: nn.ModuleList = nn.ModuleList([CompressionStageGAU(\n",
      "line 46:             embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all,\n",
      "line 47:             **self.factory_kwargs, **kwarg_all) for _ in range(2)])\n",
      "line 48:         self.model = nn.Sequential(nn.Conv2d(1, 20, 5), nn.ReLU(),\n",
      "line 49:             LatentAttentionGAU(), nn.Conv2d(20, 64, 5), nn.ReLU(), nn.\n",
      "line 50:             Sequential(OrderedDict([('conv1', nn.Conv2d(1, 20, 5)), (\n",
      "line 51:             'relu1', nn.ReLU()), ('gau2', DualPathGAU()), ('conv2', nn.\n",
      "line 52:             Conv2d(20, 64, 5)), ('relu2', nn.ReLU())])))\n",
      "line 53:         self.activations = nn.ModuleDict({'relu': nn.ReLU(), 'lrelu': nn.\n",
      "line 54:             LeakyReLU(), 'gau3': CompressionStageGAU(embed_dim=embed_dim,\n",
      "line 55:             block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
      "line 56:             **kwarg_all)})\n",
      "line 57:         self.model2 = nn.Sequential(OrderedDict([('conv1', nn.Conv2d(1, 20,\n",
      "line 58:             5)), ('relu1', nn.ReLU()), ('gau2', DualPathGAU()), ('conv2',\n",
      "line 59:             nn.Conv2d(20, 64, 5)), ('relu2', nn.ReLU())]))\n",
      "line 60:         self.a1 = a1\n",
      "line 61:         self.a2: int = a2\n",
      "line 62: \n",
      "line 63:     def _forward(self, X, **Z):\n",
      "line 64:         X, Z = self.token_scorer(X, **Z)\n",
      "line 65:         Y, Z = self.latent_attention(X, **Z)\n",
      "line 66:         X, Z = self.dual_path(X, **Z)\n",
      "line 67:         return Y, Z\n",
      "line 68: \n",
      "line 69: \n",
      "line 70: class CompressionStageGAU(GAUBase): \n",
      "line 71:     def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
      "line 72:         self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "line 73:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 74:         \n",
      "line 75:     def _forward(self, X, **Z): \n",
      "line 76:         return X\n",
      "line 77: \n",
      "line 78: \n",
      "line 79: class DualPathGAU(GAUBase): \n",
      "line 80:     def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
      "line 81:         self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "line 82:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 83:         \n",
      "line 84:     def _forward(self, X, **Z): \n",
      "line 85:         return X\n",
      "line 86: \n",
      "line 87: \n",
      "line 88: class TokenScoringGAU(GAUBase): \n",
      "line 89:     def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
      "line 90:         self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "line 91:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 92:         \n",
      "line 93:     def _forward(self, X, **Z): \n",
      "line 94:         return X\n",
      "line 95: \n",
      "line 96: \n",
      "line 97: class LatentAttentionGAU(GAUBase): \n",
      "line 98:     def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
      "line 99:         self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "line 100:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 101:         \n",
      "line 102:     def _forward(self, X, **Z): \n",
      "line 103:         return X\n",
      "line 104: \n",
      "line 105: \n",
      "line 106: @gau_test\n",
      "line 107: def test_XAU_test1(a=1, device=None, dtype=None):\n",
      "line 108:     print('test1')\n",
      "line 109: \n",
      "line 110: \n",
      "line 111: @gau_test\n",
      "line 112: def test_XAU_test2(device=None, dtype=None):\n",
      "line 113:     print('test2')\n",
      "line 114: \n",
      "line 115: \n",
      "line 116: def run_XAU_tests():\n",
      "line 117: \ttry:\n",
      "line 118: \t\ttest_XAU_test1()\n",
      "line 119: \texcept Exception as e:\n",
      "line 120: \t\tprint(\"Error in running test1:\", e)\n",
      "line 121: \ttry:\n",
      "line 122: \t\ttest_XAU_test2()\n",
      "line 123: \texcept Exception as e:\n",
      "line 124: \t\tprint(\"Error in running test2:\", e)\n",
      "line 125: \n",
      "line 126: \n",
      "line 127: if __name__ == \"__main__\":\n",
      "line 128: \trun_XAU_tests()\n"
     ]
    }
   ],
   "source": [
    "code=test_tree.compose_unit('XAU')\n",
    "code=U.add_line_num(code)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class DilatedReformerRootGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.dilated_attention = DilatedAttentionGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.reversible_residual = ReversibleResidualGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **\n",
    "            self.factory_kwargs, **kwarg_all)\n",
    "        self.lsh_attention = LSHAttentionGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.dilated_attention(X, **Z)\n",
    "        X, Z = self.reversible_residual(X, **Z)\n",
    "        X, Z = self.lsh_attention(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class LSHAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class DilatedAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class ReversibleResidualGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "@gau_test\n",
    "def test_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau(device=\n",
    "    None, dtype=None) ->None:\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    embed_dim = 16\n",
    "    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "    gau = DilatedReformerRootGAU(embed_dim=embed_dim, block_loc=(0, 6), kwarg_all={}, device=\n",
    "        device, dtype=dtype)\n",
    "    Y, Z_ = gau(X, **Z)\n",
    "    assert Y.shape == X.shape, 'Output shape should match input shape'\n",
    "    assert isinstance(Z_, dict), 'Z_ should be a dictionary'\n",
    "    print('Input X:', X)\n",
    "    print('Output Y:', Y)\n",
    "    print('Intermediate variables Z_:', Z_)\n",
    "\n",
    "\n",
    "def run_DilatedReformerRootGAU_tests():\n",
    "\ttry:\n",
    "\t\ttest_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"Error in running unit_test_dilated_reformer_root_gau:\", e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\trun_DilatedReformerRootGAU_tests()\n",
    "'''\n",
    "\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "output=io.StringIO()\n",
    "with redirect_stdout(output):\n",
    "    exec(code)\n",
    "captured = str(output.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ResonanceLOCOSTGAMRoot(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class ResonanceLOCOSTGAMRoot(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.resonance_rope = ResonanceRoPEUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.state_space = StateSpaceUnit(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all)\n",
    "        self.inter_block_comm = InterBlockCommUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y, Z_ = self.resonance_rope(X, **Z)\n",
    "        Y, Z_ = self.state_space(Y, **Z_)\n",
    "        Y, Z_ = self.inter_block_comm(Y, **Z_)\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class ResonanceRoPEUnit(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.freqs = self._init_frequencies(embed_dim)\n",
    "\n",
    "    def _init_frequencies(self, embed_dim):\n",
    "        half_dim = embed_dim // 2\n",
    "        freqs = torch.exp(-torch.arange(half_dim, dtype=torch.float32,\n",
    "            device=self.factory_kwargs['device']) * (math.log(10000.0) / (\n",
    "            half_dim - 1)))\n",
    "        return freqs\n",
    "\n",
    "    def _apply_rope(self, X):\n",
    "        batch_size, seqlen, _ = X.size()\n",
    "        position_ids = torch.arange(seqlen, dtype=X.dtype, device=X.device)\n",
    "        sinusoid_inp = torch.einsum('i,j->ij', position_ids, self.freqs)\n",
    "        sin, cos = sinusoid_inp.sin(), sinusoid_inp.cos()\n",
    "        X1, X2 = X[..., ::2], X[..., 1::2]\n",
    "        Y1 = X1 * cos - X2 * sin\n",
    "        Y2 = X1 * sin + X2 * cos\n",
    "        return torch.stack([Y1, Y2], dim=-1).flatten(-2)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = self._apply_rope(X)\n",
    "        Z_ = Z\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "class StateSpaceUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class InterBlockCommUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "    \n",
    "\n",
    "\n",
    "gab_config = {}\n",
    "\n",
    "\n",
    "# Check if to device still work!\n",
    "\n",
    "gab = GAB(128, (0, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResonanceLOCOSTGAMRoot(\n",
       "  (resonance_rope): ResonanceRoPEUnit()\n",
       "  (state_space): StateSpaceUnit()\n",
       "  (inter_block_comm): InterBlockCommUnit()\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Returns the device of the first parameter found in the given nn.Module.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    If the module has no parameters, returns None.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m----> 8\u001b[0m \u001b[43mget_module_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[88], line 6\u001b[0m, in \u001b[0;36mget_module_device\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_module_device\u001b[39m(model):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Returns the device of the first parameter found in the given nn.Module.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    If the module has no parameters, returns None.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_module_device(model):\n",
    "    \"\"\"\n",
    "    Returns the device of the first parameter found in the given nn.Module.\n",
    "    If the module has no parameters, returns None.\n",
    "    \"\"\"\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "get_module_device(gab.root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(gab.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "children={\n",
    "    'unit1': 'MockUnit1',\n",
    "    'unit2': 'MockUnit2',\n",
    "}\n",
    "called=['unit1', 'unit2', 'unit1']\n",
    "test_tree.units['DualPathGAU'] = GAUNode('DualPathGAU', None, None, None, called, None, None, children, None)\n",
    "children={\n",
    "    'unit1': 'MockUnit3',\n",
    "    'unit2': 'MockUnit4',\n",
    "}\n",
    "called=['unit2', 'unit2']\n",
    "test_tree.units['MockUnit1'] = GAUNode('MockUnit1', None, None, None, called, None, None, children, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DualPathGAU': GAUNode(name='DualPathGAU', code=None, args=None, desc=None, path=['unit1', 'unit2', 'unit1'], review=None, rating=None, children={'unit1': 'MockUnit1', 'unit2': 'MockUnit2'}, suggestions=None),\n",
       " 'MockUnit1': GAUNode(name='MockUnit1', code=None, args=None, desc=None, path=['unit2', 'unit2'], review=None, rating=None, children={'unit1': 'MockUnit3', 'unit2': 'MockUnit4'}, suggestions=None)}"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tree.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in A: 2 3\n",
      "Kwargs in A: {'a': 1, 'b': 2, 'c': 3}\n",
      "Kwargs in B: {'a': 1, 'b': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.B at 0x2dfbf677b30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class A:\n",
    "    def __init__(self,r,d,e,**Z):\n",
    "        print('Args in A:',d,e)\n",
    "        print('Kwargs in A:',Z)\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self,_Z,c=1,d=2,e=3,**Z):\n",
    "        args2={'d':d,'e':e}\n",
    "        super().__init__(c,**_Z,**args2)\n",
    "        print('Kwargs in B:',Z)\n",
    "\n",
    "cfg={\n",
    "    'a':1,\n",
    "    'b':2,\n",
    "    'c':3,\n",
    "}\n",
    "\n",
    "B(cfg,**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'a': 3}\n",
      "{'a': 3}\n"
     ]
    }
   ],
   "source": [
    "def A(X,Z,**kwargs):\n",
    "    print(X)\n",
    "    print(Z)\n",
    "    print(kwargs)\n",
    "\n",
    "def B(X,Z,a,**kwargs):\n",
    "    A(X,Z,**Z)\n",
    "\n",
    "Z={'a':3}\n",
    "B(1,Z,**Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "    '''\n",
    "    GAU Tree Map of Butterfly-AFT Generalized Autoregressive Block (BAGAB):\n",
    "    ButterflyAFTGAU (Rating: 3.5/5)\n",
    "        |- AFTMechanismGAU (Rating: 3.5/5)\n",
    "            |- PositionBiasGAU (Rating: 3.5/5)\n",
    "            |- ElementWiseOperationGAU (Rating: 3.5/5)\n",
    "        |- ButterflyTransformGAU (Rating: 3.5/5)\n",
    "            |- ButterflyLayerGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "            |- ButterflyInitializationGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "                |- ButterflyParameterGAU (Unimplemented)\n",
    "\n",
    "    Implemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ElementWiseOperationGAU, ButterflyAFTGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    Unimplemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ButterflyParameterGAU, ElementWiseOperationGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    '''\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ButterflyAFTGAU(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase\n",
    "\n",
    "\n",
    "class ButterflyAFTGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.aft_mechanism = AFTMechanismGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.butterfly_transform = ButterflyTransformGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        aft_output, Z = self.aft_mechanism(X, **Z)\n",
    "        butterfly_output, Z = self.butterfly_transform(aft_output, **Z)\n",
    "        return butterfly_output, Z\n",
    "\n",
    "\n",
    "class ButterflyTransformGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_layer = ButterflyLayerGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_init = ButterflyInitializationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.butterfly_init(X, **Z)\n",
    "        Y, Z = self.butterfly_layer(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ButterflyLayerGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.stages = nn.ModuleList([ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all) for _ in range(kwarg_all.get('num_stages', 4))])\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for stage in self.stages:\n",
    "            X, Z = stage(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "class ButterflyMatrixGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ButterflyInitializationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_size = kwarg_all.get('butterfly_size', embed_dim)\n",
    "        self.butterfly_matrices = nn.ParameterList([nn.Parameter(torch.\n",
    "            randn(self.butterfly_size, self.butterfly_size, **self.\n",
    "            factory_kwargs)) for _ in range(int(torch.log2(torch.tensor(\n",
    "            self.butterfly_size))))])\n",
    "        self.butterfly_stage = ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_parameter = ButterflyParameterGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for matrix in self.butterfly_matrices:\n",
    "            X = F.linear(X, matrix)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class ButterflyParameterGAU(GAUBase):\n",
    "    \"\"\"\n",
    "    Generalized Autoregressive Block Unit for learning parameters of Butterfly Factorization.\n",
    "    Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "    Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "    Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "    embed_dim:    The dimension of the input embeddings\n",
    "    block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "    kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.param_size = kwarg_all.get('param_size', 128)\n",
    "        self.butterfly_weights = nn.Parameter(torch.randn(self.param_size,\n",
    "            embed_dim, **self.factory_kwargs))\n",
    "        nn.init.xavier_uniform_(self.butterfly_weights)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X = torch.matmul(X, self.butterfly_weights)\n",
    "        Z_ = {'butterfly_weights': self.butterfly_weights}\n",
    "        return transformed_X, Z_\n",
    "        # transformed_X = F.linear(X, self.butterfly_weights.T)\n",
    "        # return transformed_X, {}\n",
    "\n",
    "\n",
    "class ButterflyComputationGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class ButterflyStageGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_params = ButterflyParameterGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X, Z = self.butterfly_params(X, **Z)\n",
    "        assert transformed_X.shape == X.shape, f'Output shape {transformed_X.shape} does not match input shape {X.shape}'\n",
    "        return transformed_X, Z\n",
    "\n",
    "class AFTMechanismGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = PositionBiasGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.element_wise_operation = ElementWiseOperationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.position_bias(X, **Z)\n",
    "        Y, Z = self.element_wise_operation(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ElementWiseOperationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, embed_dim, **self.\n",
    "            factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        queries = Z.get('queries', X)\n",
    "        keys = Z.get('keys', X)\n",
    "        values = Z.get('values', X)\n",
    "        elementwise_product = queries * keys\n",
    "        elementwise_sum = elementwise_product + values + self.position_bias\n",
    "        return elementwise_sum, {}\n",
    "\n",
    "\n",
    "class PositionBiasGAU(GAUBase):\n",
    "    \"\"\"Position Bias Generalized Autoregressive Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, 1, embed_dim, **\n",
    "            self.factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = X + self.position_bias\n",
    "        return Y, {}\n",
    "    \n",
    "\n",
    "class GABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100, **kwargs):\n",
    "        super().__init__()\n",
    "        self.gab = GAB(embed_dim, (0,1), **kwargs)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, Z = self.gab(X, **Z)\n",
    "        return self.lm_head(X),X\n",
    "\n",
    "\n",
    "\n",
    "gab_config = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 54495796.0\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 899675.4375\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 475023.78125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 810216.875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 777344.75\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 783032.0625\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 771978.3125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 62701.796875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 66148.203125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 67731.2734375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 69984.171875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 74028.8046875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 70948.921875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 78776.75\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 608838.3125\n",
      "Loss: 18159550.0\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 680764.9375\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 377442.28125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 609263.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 591532.1875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 592608.125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 558789.9375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 48893.484375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 53887.46875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 54524.15625\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 57871.140625\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 63347.60546875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 56995.46875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 57605.421875\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 362053.15625\n",
      "Loss: 2057852.75\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 380637.28125\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 183177.609375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 376077.53125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 359930.09375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 367729.90625\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 371797.71875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 24987.49609375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 30496.25\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 29164.060546875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 32006.0234375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 32426.490234375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 33316.421875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 33375.78125\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 196014.078125\n",
      "Loss: 0.0\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 0.0\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 0.0\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 0.0\n",
      "Loss: 0.0\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 0.0\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 0.0\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 0.0\n"
     ]
    }
   ],
   "source": [
    "class SimpleGABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100):\n",
    "        super().__init__()\n",
    "        self.butterfly_param = ButterflyParameterGAU(embed_dim, (0,1), {})\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, _ = self.butterfly_param(X, **Z)\n",
    "        return self.lm_head(X), X\n",
    "\n",
    "# Test the simplified model\n",
    "simple_model = GABLM(128)\n",
    "mock_input = torch.randn(1, 10, 128)\n",
    "mock_y = torch.randint(0, 100, (1, 10))\n",
    "\n",
    "optimizer = optim.Adam(simple_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for _ in range(5):  # Run for a few iterations\n",
    "    optimizer.zero_grad()\n",
    "    y, X = simple_model(mock_input)\n",
    "    loss = criterion(y.view(-1, 100), mock_y.view(-1))\n",
    "    loss.backward()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    for name, param in simple_model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"Gradient for {name}: {param.grad.abs().mean()}\")\n",
    "        else:\n",
    "            print(f\"No gradient for {name}\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Used parameter gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights requires gradients but has none.\n",
      "Error: Used parameter gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights requires gradients but has none.\n",
      "Differentiability test failed due to missing gradients.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim \n",
    "\n",
    "model=GABLM(128, **gab_config)\n",
    "vocab_size=100\n",
    "DEFAULT_CONTEXT_LENGTH=10\n",
    "\n",
    "mock_input=torch.randn(1, DEFAULT_CONTEXT_LENGTH, 128)\n",
    "mock_y=torch.randint(0, 100, (1, 10))\n",
    "mock_Z={}\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "y,X=model(mock_input, **mock_Z)\n",
    "loss=nn.CrossEntropyLoss()(y.view(-1, 100), mock_y.view(-1))\n",
    "loss.backward()\n",
    "\n",
    "used_params = set()\n",
    "\n",
    "# Hook to register which parameters are used during the forward pass\n",
    "def hook_fn(module, input, output):\n",
    "    for param in module.parameters():\n",
    "        used_params.add(param)\n",
    "\n",
    "# Register hooks for all submodules\n",
    "hooks = []\n",
    "for submodule in model.modules():\n",
    "    hook = submodule.register_forward_hook(hook_fn)\n",
    "    hooks.append(hook)\n",
    "\n",
    "# Forward pass again to collect used parameters\n",
    "model(mock_input)\n",
    "\n",
    "# Remove all hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "all_gradients_present = True\n",
    "\n",
    "# Check gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if param in used_params and param.grad is None:\n",
    "            # Used parameter without gradient -> Error\n",
    "            print(f\"Error: Used parameter {name} requires gradients but has none.\")\n",
    "            all_gradients_present = False\n",
    "        elif param not in used_params:\n",
    "            # Unused parameter -> Warning\n",
    "            print(f\"Warning: Parameter {name} was not used in the forward pass.\")\n",
    "\n",
    "if all_gradients_present:\n",
    "    print('Differentiability test passed')\n",
    "else:\n",
    "    print('Differentiability test failed due to missing gradients.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from core library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "GAU Tree Map of GPT2:\n",
      "  GPT2\n",
      "       |- MHA\n",
      "           |- RotaryPositionalEmbeddings\n",
      "       |- GatedMLP\n",
      "       |- RMSNorm\n",
      "\n",
      "Implemented Units: MHA, GatedMLP, GPT2, RotaryPositionalEmbeddings, RMSNorm\n",
      "All units are implemented.\n",
      "\n",
      "Specifications for Implemented Units:\n",
      "\n",
      "Unit Name: GatedMLP\n",
      "'''\n",
      "\n",
      "Gated MLP\n",
      "  \n",
      "'''\n",
      "- Inputs: X\n",
      "- Outputs: Y\n",
      "\n",
      "\n",
      "Unit Name: GPT2\n",
      "'''\n",
      "\n",
      "GPT2\n",
      "  \n",
      "'''\n",
      "- Inputs: X\n",
      "- Outputs: Y\n",
      "\n",
      "\n",
      "Unit Name: MHA\n",
      "'''\n",
      "\n",
      "MHA\n",
      "  \n",
      "'''\n",
      "- Inputs: X\n",
      "- Outputs: Y\n",
      "\n",
      "\n",
      "Unit Name: RMSNorm\n",
      "'''\n",
      "\n",
      "RMSNorm\n",
      "  \n",
      "'''\n",
      "- Inputs: X\n",
      "- Outputs: Y\n",
      "\n",
      "\n",
      "Unit Name: RotaryPositionalEmbeddings\n",
      "'''\n",
      "\n",
      "This class implements Rotary Positional Embeddings (RoPE)\n",
      "proposed in https://arxiv.org/abs/2104.09864.\n",
      "\n",
      "Reference implementation (used for correctness verfication)\n",
      "can be found here:\n",
      "https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n",
      "\n",
      "In this implementation we cache the embeddings for each position upto\n",
      "``max_seq_len`` by computing this during init.\n",
      "\n",
      "Args:\n",
      "    dim (int): Embedding dimension. This is usually set to the dim of each\n",
      "        head in the attention module computed as ````embed_dim`` // ``num_heads````\n",
      "    max_seq_len (int): Maximum expected sequence length for the\n",
      "        model, if exceeded the cached freqs will be recomputed\n",
      "    base (int): The base for the geometric progression used to compute\n",
      "        the rotation angles\n",
      "  \n",
      "'''\n",
      "- Inputs: input_emb, *input_pos\n",
      "- Outputs: output_emb\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import importlib\n",
    "import model_discovery.model.composer \n",
    "# importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUTree\n",
    "\n",
    "import model_discovery.utils as U\n",
    "\n",
    "# dir='/home/junyanc/model_discovery/model_discovery/model/library/core/gpt2/units'\n",
    "dir='C://ChengJunyan1//Research//model_discovery//model_discovery//model//library//core//gpt2//units'\n",
    "tree=GAUTree.load_from_base(dir)\n",
    "\n",
    "print(tree.view()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<model_discovery.model.composer.GAUTree at 0x2efaab17aa0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "copy.deepcopy(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
