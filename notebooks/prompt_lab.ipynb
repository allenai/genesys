{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place for experimenting the progressive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.utils as U\n",
    "from model_discovery.configs.gam_config import GAMConfig, GAMConfig_14M\n",
    "from model_discovery.model.composer import GABTree,ROOT_UNIT_TEMPLATE,GAUBase\n",
    "# from model_discovery.evolution import  BuildEvolution\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "db_dir = U.pjoin(ckpt_dir, 'test_composer', 'db')\n",
    "test_tree = GABTree('TestTree', db_dir)\n",
    "\n",
    "prompts_dir='../model_discovery/agents/prompts/'\n",
    "gab_py = U.read_file(U.pjoin(prompts_dir,'gab_template.py'))\n",
    "gam_py = U.read_file(U.pjoin(prompts_dir,'gam_prompt.py'))\n",
    "GAU_TEMPLATE = U.read_file(U.pjoin(prompts_dir,'gau_template.py'))\n",
    "GAU_BASE=inspect.getsource(GAUBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# TestTree.py\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom model_discovery.model.utils.modules import GABUnit # DO NOT CHANGE THIS IMPORT STATEMENT #\\n\\n\\n# YOU CAN IMPORT MORE MODULES HERE #\\n\\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\\n\\n\\nclass TestTree(GABUnit): \\n    \"\"\"Generalized Autoregressive Block\\n        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\\n        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\\n        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\\n    \"\"\"\\n    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\\n        # argv: list of hyperparameters\\n        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\\n        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\\n        \\n        # COMPLETING THE CODE HERE #\\n\\n\\n    # YOU CAN ADD MORE FUNCTIONS HERE #\\n\\n\\n    def _forward(self, X, **Z): \\n\\n        # COMPLETING THE CODE HERE #\\n        \\n        return X\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tree.path\n",
    "test_tree.get_source('TestTree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a professional AI researcher focusing on discovering the best\n",
      "autoregressive language model block. You goal is to design a novel block\n",
      "following the Generalized Autoregressive Block (GAB) structure defined in the\n",
      "following base class:\n",
      "\n",
      "```python \n",
      "class GABBase(nn.Module):\n",
      "    \"\"\" Base class for Generalized Autoregressive Block \"\"\"\n",
      "    def __init__(self,embed_dim: int, block_loc: tuple): \n",
      "        super().__init__()\n",
      "        self.embed_dim = embed_dim\n",
      "        self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\n",
      "\n",
      "    def _forward(self,X,**kwargs): \n",
      "        raise NotImplementedError\n",
      "     \n",
      "    # YOU ARE NOT ALLOW TO OVERRIDE THIS METHOD #\n",
      "    def forward(self,X,**Z):\n",
      "        \"\"\"Forward pass of the model\"\"\"\n",
      "        assert X.shape[-1] == self.embed_dim\n",
      "        Y_=self._forward(X,**Z)\n",
      "        if isinstance(Y_,tuple):\n",
      "            Y, Z = Y_\n",
      "        else:\n",
      "            Z = {}\n",
      "        assert Y.shape == X.shape\n",
      "        return Y, Z\n",
      " ```\n",
      "\n",
      "\n",
      "The GAB will be used to construct a Generalized Autoregressive Model (GAM)\n",
      "defined as follows:\n",
      "\n",
      "```python from transformers.modeling_outputs import CausalLMOutput\n",
      "from transformers import PreTrainedModel, PretrainedConfig\n",
      "from dataclasses import dataclass\n",
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "\n",
      "from gab import GAB, gab_config\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class GAMConfig(PretrainedConfig):\n",
      "    '''Configurations for Generalized Autoregressive Models.'''\n",
      "\n",
      "    d_model: int\n",
      "    n_block: int\n",
      "    batch_tokens: int \n",
      "    vocab_size: int = None\n",
      "\n",
      "\n",
      "class GAM(nn.Module):\n",
      "    ''' Generalized Autoregressive Models\n",
      "        Input:        X: (batch, seqlen, embed_dim)\n",
      "        Output:       Y: (batch, seqlen, embed_dim)\n",
      "    '''\n",
      "    def __init__(\n",
      "        self,\n",
      "        d_model: int,\n",
      "        n_block: int,\n",
      "        vocab_size: int = 50277,\n",
      "        norm_epsilon: float = 1e-5,\n",
      "        device = None,\n",
      "        dtype = None,\n",
      "    ) -> None:\n",
      "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      "        super().__init__()\n",
      "        self.d_model = d_model\n",
      "        self.embedding = nn.Embedding(vocab_size, d_model, **self.factory_kwargs)\n",
      "\n",
      "        block_config = gab_config()\n",
      "        self.blocks = nn.ModuleList(\n",
      "            [\n",
      "                GAB(\n",
      "                    embed_dim=d_model, \n",
      "                    block_loc=(layer_idx,n_block),\n",
      "                    device=device, \n",
      "                    dtype=dtype, \n",
      "                    **block_config\n",
      "                )\n",
      "                for layer_idx in range(n_block)\n",
      "            ]\n",
      "        )\n",
      "        self.norm_out = nn.LayerNorm(\n",
      "            d_model, eps=norm_epsilon, **self.factory_kwargs\n",
      "        )\n",
      "\n",
      "    def forward(self, input_ids):\n",
      "        hidden_states = self.embedding(input_ids)\n",
      "        intermediate_vars = {}\n",
      "        for block in self.blocks:\n",
      "            hidden_states, intermediate_vars = block(\n",
      "                hidden_states,\n",
      "                **intermediate_vars\n",
      "            )\n",
      "        hidden_states = self.norm_out(hidden_states)\n",
      "        return hidden_states\n",
      "\n",
      "\n",
      "class ModisLMHeadModel(PreTrainedModel):\n",
      "    ''' Generalized Autoregressive Models with LM Head '''\n",
      "    config_class = GAMConfig\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: GAMConfig,\n",
      "        device=None,\n",
      "        dtype=None,\n",
      "    ) -> None:\n",
      "        super().__init__(config)\n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      "        self.backbone = GAM(\n",
      "            d_model=config.d_model,\n",
      "            n_block=config.n_block,\n",
      "            vocab_size=config.vocab_size,\n",
      "            **factory_kwargs,\n",
      "        )\n",
      "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False, **factory_kwargs)\n",
      "\n",
      "    def forward(self, input_ids, **gab_kwargs):\n",
      "        hidden_states = self.backbone(input_ids, **gab_kwargs)\n",
      "        lm_logits = self.lm_head(hidden_states)\n",
      "        return CausalLMOutput(logits=lm_logits)\n",
      " ```\n",
      "\n",
      "The produced language model will be pretrained with the corpus and then be\n",
      "applied for downstream tasks. The new model is expected to have a low\n",
      "perplexity, high accuracy, robustness, efficiency, and most importantly, good\n",
      "scalability. You have two roles 1) to propose ideas, analyze the problems,\n",
      "design the model and implement it and; 2) to write the reports that justify your\n",
      "ideas. You do not need to immediately do everything at one response, following\n",
      "the provided instructions, and finish those tasks step by step in the coming\n",
      "multi-round dialog. \n",
      "\n",
      "Since the autoregressive model design is complicated, so we will break it down\n",
      "into smaller parts. We represent a block as multiple nested units, the\n",
      "Generalized Autoregressive Unit (GAU). Each GAU accepts a sequence of embeddings\n",
      "X and a dictionary of intermediate variables Z as input, and outputs a sequence\n",
      "of embeddings Y and a dictionary of new or updated intermediate variables Z_. Z_\n",
      "is optional, when it is provided, it will be used to update Z for the next unit\n",
      "by Z.update(Z_). A GAU is defined in the following base class:\n",
      "\n",
      "```python class GAUBase(nn.Module): \n",
      "    \"\"\" \n",
      "    Instead of directly giving the full implementation of a GAB block, the agent need to \n",
      "    design a series of nested GAB units and construct the full GAB block as a pipeline of these units.\n",
      "\n",
      "    GAB is fractal, like GAB itself, each GAB unit accepts X and Z as input and returns Y and Z as output.\n",
      "    \"\"\" \n",
      "    def __init__(self, embed_dim: int):\n",
      "        super().__init__()\n",
      "        self.embed_dim = embed_dim\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        raise NotImplementedError\n",
      "    \n",
      "    def forward(self, X, Z):\n",
      "        assert len(X.shape) == 3, \"Input shape must be (batch, seqlen, embed_dim)\"\n",
      "        assert X.shape[-1] == self.embed_dim\n",
      "        _params = inspect.signature(self._forward).parameters\n",
      "        _Z = {k: v for k, v in Z.items() if k in _params}\n",
      "        Y = self._forward(X, **_Z)\n",
      "        if isinstance(Y, tuple):\n",
      "            Y, Z_ = Y\n",
      "        else:\n",
      "            Z_ = {}\n",
      "        assert Y.shape == X.shape, f\"GAB Unit must has a sequence with the same shape as input in output, got {Y.shape} instead\"\n",
      "        assert isinstance(Z_, dict), \"Intermediate variables must be stored in a tuple\"\n",
      "        Z.update(Z_) # the new intermediate variables are updated to the current Z\n",
      "        return Y, Z\n",
      " ```\n",
      "\n",
      "You will design a GAU by completing the blanks marked in this template, which\n",
      "includes the initialization where you can define your custom arguments with\n",
      "optional default values, the forward function where you can define convenient\n",
      "functions or classes in the GAB class such as caches, notice that you are only\n",
      "allowed to have only one GAU which inherited from the GAUBase class in the file:\n",
      " \n",
      "```python # gab.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
      "\n",
      "\n",
      "# YOU CAN IMPORT MORE MODULES HERE #\n",
      "\n",
      "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
      "\n",
      "\n",
      "class GAU(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\n",
      "    \"\"\"Generalized Autoregressive Block Unit\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
      "        # argv: list of hyperparameters\n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\n",
      "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
      "        \n",
      "        # COMPLETING THE CODE HERE #\n",
      "\n",
      "        raise NotImplementedError\n",
      "\n",
      "\n",
      "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
      "\n",
      "\n",
      "    def _forward(self, X, **Z): \n",
      "        \n",
      "        # THE CODE HERE MUST BE COMPLETED #\n",
      "\n",
      "        raise NotImplementedError\n",
      "     ```\n",
      "\n",
      "In a GAU, you can call other GAUs, as such, you can create a complicated GAB\n",
      "block by nesting multiple GAUs. However, each GAU should be not too complex, if\n",
      "you want to create complex block, you should break it down into smaller GAUs and\n",
      "nest them. As such, you should design a GAB block in a top-down manner. \n",
      "\n",
      "Notice that, everytime you are only allowed to edit within one GAU. You can\n",
      "leave placeholder definition and calls of the GAUs that you wish to implement\n",
      "later in your GAU. The system will automatically create an initial GAU code for\n",
      "the placeholders. Once a GAU is provided, it will be inserted into the entire\n",
      "GAB composed based on the tree of GAUs under your design and tested for\n",
      "correctness then reviewed for novelty and quality. You will need to ensure the\n",
      "correctness of all the GAUs in the final GAB at the end.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import model_discovery.agents.prompts.prompts as P\n",
    "importlib.reload(P)\n",
    "\n",
    "gu_system_prompt=P.GU_DESIGNER_SYSTEM_prompt.format(GAB_BASE=P.GAB_BASE,GAM_PY=gam_py,GAU_BASE=GAU_BASE,GAU_TEMPLATE=GAU_TEMPLATE)\n",
    "\n",
    "print(gu_system_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Intuitions and Analysis\n",
      "\n",
      "In designing a novel autoregressive block, we aim to create a structure that is both innovative and powerful, capable of outperforming existing state-of-the-art models. The core idea is to leverage a combination of attention mechanisms, feedforward networks, and gating mechanisms to enhance the model's ability to capture complex dependencies in the data. \n",
      "\n",
      "1. **Attention Mechanism**: While attention mechanisms are not new, we can innovate by introducing a dynamic attention mechanism that adapts based on the input sequence characteristics. This can help the model focus on the most relevant parts of the sequence, improving efficiency and accuracy.\n",
      "\n",
      "2. **Feedforward Networks**: We can enhance the traditional feedforward networks by incorporating non-linear transformations and residual connections, allowing the model to learn more complex patterns.\n",
      "\n",
      "3. **Gating Mechanisms**: Introducing gating mechanisms can help control the flow of information through the network, allowing the model to dynamically adjust its behavior based on the input. This can improve robustness and scalability.\n",
      "\n",
      "4. **Scalability and Efficiency**: The design should ensure that the model scales well with increasing data sizes and remains efficient in terms of computational resources.\n",
      "\n",
      "### Rough Plan for Children GABUnits\n",
      "\n",
      "1. **DynamicAttentionUnit**: A unit that implements a dynamic attention mechanism, adapting based on input characteristics.\n",
      "2. **EnhancedFeedforwardUnit**: A unit that incorporates non-linear transformations and residual connections in the feedforward network.\n",
      "3. **GatingMechanismUnit**: A unit that introduces gating mechanisms to control information flow.\n",
      "\n",
      "### Pseudo Code\n",
      "\n",
      "```python\n",
      "class DynamicAttentionUnit(GABUnit):\n",
      "    def __init__(self, embed_dim, **kwargs):\n",
      "        # Initialize attention mechanism\n",
      "        pass\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        # Compute dynamic attention\n",
      "        return Y, Z_\n",
      "\n",
      "class EnhancedFeedforwardUnit(GABUnit):\n",
      "    def __init__(self, embed_dim, **kwargs):\n",
      "        # Initialize feedforward network with non-linear transformations\n",
      "        pass\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        # Apply feedforward transformations\n",
      "        return Y, Z_\n",
      "\n",
      "class GatingMechanismUnit(GABUnit):\n",
      "    def __init__(self, embed_dim, **kwargs):\n",
      "        # Initialize gating mechanisms\n",
      "        pass\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        # Apply gating mechanisms\n",
      "        return Y, Z_\n",
      "```\n",
      "\n",
      "### Name of the GABUnit\n",
      "\n",
      "```unit_name {AdaptiveGAB}```\n",
      "\n",
      "### Full Implementation\n",
      "\n",
      "```python\n",
      "# GAB_UNIT_IMPLEMENTATION\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "from model_discovery.model.utils.modules import GABUnit\n",
      "\n",
      "# YOU CAN IMPORT MORE MODULES HERE #\n",
      "\n",
      "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
      "\n",
      "class AdaptiveGAB(GABUnit): \n",
      "    \"\"\"Generalized Autoregressive Block\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      "        super().__init__(embed_dim)\n",
      "        \n",
      "        # Define the sub-units\n",
      "        self.dynamic_attention: GABUnit = DynamicAttentionUnit(embed_dim, **factory_kwargs)\n",
      "        self.enhanced_feedforward: GABUnit = EnhancedFeedforwardUnit(embed_dim, **factory_kwargs)\n",
      "        self.gating_mechanism: GABUnit = GatingMechanismUnit(embed_dim, **factory_kwargs)\n",
      "\n",
      "    def _forward(self, X, **Z): \n",
      "        # Apply dynamic attention\n",
      "        X, Z = self.dynamic_attention(X, **Z)\n",
      "        \n",
      "        # Apply enhanced feedforward network\n",
      "        X, Z = self.enhanced_feedforward(X, **Z)\n",
      "        \n",
      "        # Apply gating mechanisms\n",
      "        X, Z = self.gating_mechanism(X, **Z)\n",
      "        \n",
      "        return X, Z\n",
      "```\n",
      "\n",
      "### Config\n",
      "\n",
      "```config {\n",
      "    # ADD HYPERPARAMETERS HERE #\n",
      "    \"attention_heads\": 8,\n",
      "    \"feedforward_dim\": 2048,\n",
      "    \"gating_type\": \"sigmoid\"\n",
      "} ``` \n",
      "\n",
      "This design introduces a novel combination of dynamic attention, enhanced feedforward networks, and gating mechanisms, aiming to improve the model's ability to capture complex dependencies while maintaining efficiency and scalability. The next steps will involve implementing the placeholder units and refining the design based on experimental results.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "test_output={'text': '### Intuitions and Analysis\\n\\nIn designing a novel autoregressive block, we aim to create a structure that is both innovative and powerful, capable of outperforming existing state-of-the-art models. The core idea is to leverage a combination of attention mechanisms, feedforward networks, and gating mechanisms to enhance the model\\'s ability to capture complex dependencies in the data. \\n\\n1. **Attention Mechanism**: While attention mechanisms are not new, we can innovate by introducing a dynamic attention mechanism that adapts based on the input sequence characteristics. This can help the model focus on the most relevant parts of the sequence, improving efficiency and accuracy.\\n\\n2. **Feedforward Networks**: We can enhance the traditional feedforward networks by incorporating non-linear transformations and residual connections, allowing the model to learn more complex patterns.\\n\\n3. **Gating Mechanisms**: Introducing gating mechanisms can help control the flow of information through the network, allowing the model to dynamically adjust its behavior based on the input. This can improve robustness and scalability.\\n\\n4. **Scalability and Efficiency**: The design should ensure that the model scales well with increasing data sizes and remains efficient in terms of computational resources.\\n\\n### Rough Plan for Children GABUnits\\n\\n1. **DynamicAttentionUnit**: A unit that implements a dynamic attention mechanism, adapting based on input characteristics.\\n2. **EnhancedFeedforwardUnit**: A unit that incorporates non-linear transformations and residual connections in the feedforward network.\\n3. **GatingMechanismUnit**: A unit that introduces gating mechanisms to control information flow.\\n\\n### Pseudo Code\\n\\n```python\\nclass DynamicAttentionUnit(GABUnit):\\n    def __init__(self, embed_dim, **kwargs):\\n        # Initialize attention mechanism\\n        pass\\n\\n    def _forward(self, X, **Z):\\n        # Compute dynamic attention\\n        return Y, Z_\\n\\nclass EnhancedFeedforwardUnit(GABUnit):\\n    def __init__(self, embed_dim, **kwargs):\\n        # Initialize feedforward network with non-linear transformations\\n        pass\\n\\n    def _forward(self, X, **Z):\\n        # Apply feedforward transformations\\n        return Y, Z_\\n\\nclass GatingMechanismUnit(GABUnit):\\n    def __init__(self, embed_dim, **kwargs):\\n        # Initialize gating mechanisms\\n        pass\\n\\n    def _forward(self, X, **Z):\\n        # Apply gating mechanisms\\n        return Y, Z_\\n```\\n\\n### Name of the GABUnit\\n\\n```unit_name {AdaptiveGAB}```\\n\\n### Full Implementation\\n\\n```python\\n# GAB_UNIT_IMPLEMENTATION\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom model_discovery.model.utils.modules import GABUnit\\n\\n# YOU CAN IMPORT MORE MODULES HERE #\\n\\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\\n\\nclass AdaptiveGAB(GABUnit): \\n    \"\"\"Generalized Autoregressive Block\\n        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\\n        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\\n        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\\n    \"\"\"\\n    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n        super().__init__(embed_dim)\\n        \\n        # Define the sub-units\\n        self.dynamic_attention: GABUnit = DynamicAttentionUnit(embed_dim, **factory_kwargs)\\n        self.enhanced_feedforward: GABUnit = EnhancedFeedforwardUnit(embed_dim, **factory_kwargs)\\n        self.gating_mechanism: GABUnit = GatingMechanismUnit(embed_dim, **factory_kwargs)\\n\\n    def _forward(self, X, **Z): \\n        # Apply dynamic attention\\n        X, Z = self.dynamic_attention(X, **Z)\\n        \\n        # Apply enhanced feedforward network\\n        X, Z = self.enhanced_feedforward(X, **Z)\\n        \\n        # Apply gating mechanisms\\n        X, Z = self.gating_mechanism(X, **Z)\\n        \\n        return X, Z\\n```\\n\\n### Config\\n\\n```config {\\n    # ADD HYPERPARAMETERS HERE #\\n    \"attention_heads\": 8,\\n    \"feedforward_dim\": 2048,\\n    \"gating_type\": \"sigmoid\"\\n} ``` \\n\\nThis design introduces a novel combination of dynamic attention, enhanced feedforward networks, and gating mechanisms, aiming to improve the model\\'s ability to capture complex dependencies while maintaining efficiency and scalability. The next steps will involve implementing the placeholder units and refining the design based on experimental results.', '_details': {'cost': 0.0, 'running_cost': 0}}\n",
    "raw_text=test_output['text']\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Intuitions and Analysis\n",
      "\n",
      "In designing a novel autoregressive block, we aim to create a structure that is both innovative and powerful, capable of outperforming existing state-of-the-art models. The core idea is to leverage a combination of attention mechanisms, feedforward networks, and gating mechanisms to enhance the model's ability to capture complex dependencies in the data. \n",
      "\n",
      "1. **Attention Mechanism**: While attention mechanisms are not new, we can innovate by introducing a dynamic attention mechanism that adapts based on the input sequence characteristics. This can help the model focus on the most relevant parts of the sequence, improving efficiency and accuracy.\n",
      "\n",
      "2. **Feedforward Networks**: We can enhance the traditional feedforward networks by incorporating non-linear transformations and residual connections, allowing the model to learn more complex patterns.\n",
      "\n",
      "3. **Gating Mechanisms**: Introducing gating mechanisms can help control the flow of information through the network, allowing the model to dynamically adjust its behavior based on the input. This can improve robustness and scalability.\n",
      "\n",
      "4. **Scalability and Efficiency**: The design should ensure that the model scales well with increasing data sizes and remains efficient in terms of computational resources.\n",
      "\n",
      "### Rough Plan for Children GABUnits\n",
      "\n",
      "1. **DynamicAttentionUnit**: A unit that implements a dynamic attention mechanism, adapting based on input characteristics.\n",
      "2. **EnhancedFeedforwardUnit**: A unit that incorporates non-linear transformations and residual connections in the feedforward network.\n",
      "3. **GatingMechanismUnit**: A unit that introduces gating mechanisms to control information flow.\n",
      "\n",
      "### Pseudo Code\n",
      "\n",
      "```python\n",
      "class DynamicAttentionUnit(GABUnit):\n",
      "    def __init__(self, embed_dim, **kwargs):\n",
      "        # Initialize attention mechanism\n",
      "        pass\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        # Compute dynamic attention\n",
      "        return Y, Z_\n",
      "\n",
      "class EnhancedFeedforwardUnit(GABUnit):\n",
      "    def __init__(self, embed_dim, **kwargs):\n",
      "        # Initialize feedforward network with non-linear transformations\n",
      "        pass\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        # Apply feedforward transformations\n",
      "        return Y, Z_\n",
      "\n",
      "class GatingMechanismUnit(GABUnit):\n",
      "    def __init__(self, embed_dim, **kwargs):\n",
      "        # Initialize gating mechanisms\n",
      "        pass\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        # Apply gating mechanisms\n",
      "        return Y, Z_\n",
      "```\n",
      "\n",
      "### Name of the GABUnit\n",
      "\n",
      "```unit_name {AdaptiveGAB}```\n",
      "\n",
      "### Full Implementation\n",
      "\n",
      "```python\n",
      "# GAB_UNIT_IMPLEMENTATION\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "from model_discovery.model.utils.modules import GABUnit\n",
      "\n",
      "# YOU CAN IMPORT MORE MODULES HERE #\n",
      "\n",
      "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
      "\n",
      "class AdaptiveGAB(GABUnit): \n",
      "    \"\"\"Generalized Autoregressive Block\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      "        super().__init__(embed_dim)\n",
      "        \n",
      "        # Define the sub-units\n",
      "        self.dynamic_attention: GABUnit = DynamicAttentionUnit(embed_dim, **factory_kwargs)\n",
      "        self.enhanced_feedforward: GABUnit = EnhancedFeedforwardUnit(embed_dim, **factory_kwargs)\n",
      "        self.gating_mechanism: GABUnit = GatingMechanismUnit(embed_dim, **factory_kwargs)\n",
      "\n",
      "    def _forward(self, X, **Z): \n",
      "        # Apply dynamic attention\n",
      "        X, Z = self.dynamic_attention(X, **Z)\n",
      "        \n",
      "        # Apply enhanced feedforward network\n",
      "        X, Z = self.enhanced_feedforward(X, **Z)\n",
      "        \n",
      "        # Apply gating mechanisms\n",
      "        X, Z = self.gating_mechanism(X, **Z)\n",
      "        \n",
      "        return X, Z\n",
      "```\n",
      "\n",
      "### Config\n",
      "\n",
      "```config {\n",
      "    # ADD HYPERPARAMETERS HERE #\n",
      "    \"attention_heads\": 8,\n",
      "    \"feedforward_dim\": 2048,\n",
      "    \"gating_type\": \"sigmoid\"\n",
      "} ``` \n",
      "\n",
      "This design introduces a novel combination of dynamic attention, enhanced feedforward networks, and gating mechanisms, aiming to improve the model's ability to capture complex dependencies while maintaining efficiency and scalability. The next steps will involve implementing the placeholder units and refining the design based on experimental results.\n"
     ]
    }
   ],
   "source": [
    "codes = re.findall(r\"```python(.*?)```\", raw_text, re.DOTALL)\n",
    "unit_name = re.findall(r\"```unit_name(.*?)```\", raw_text, re.DOTALL)\n",
    "config = re.findall(r\"```config(.*?)```\", raw_text, re.DOTALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "testtext={'text': '### Step 1: Intuitions and Analysis\\n\\nThe Dynamic Normalized Attention Block (DNAB) aims to address the limitations of traditional softmax-based attention mechanisms by introducing a dynamic normalization approach. The key insights are:\\n\\n1. **Normalization over Softmax**: By replacing softmax with a normalization function, we can reduce sensitivity to hyperparameters and data biases. This approach is expected to provide a more stable and robust attention mechanism, especially for long sequences.\\n\\n2. **Dynamic Recurrence**: Incorporating dynamic recurrence with matrix-valued states enhances the model\\'s expressivity, allowing it to capture complex dependencies within the input sequence. This is crucial for handling varying sequence lengths and complexities efficiently.\\n\\n3. **Efficiency and Scalability**: Utilizing low-rank projections and layer normalization will help balance expressivity and efficiency, making the model scalable for larger datasets and longer sequences.\\n\\n### Step 2: Rough Plan for Children GAUs\\n\\n1. **DynamicNormalizationUnit**: This unit will implement the dynamic normalization function, combining layer normalization and causal normalization.\\n\\n2. **DynamicRecurrenceUnit**: This unit will handle the dynamic recurrence mechanism, incorporating matrix-valued states to enhance expressivity.\\n\\n3. **LowRankProjectionUnit**: This unit will focus on reducing computational complexity through low-rank projections.\\n\\n### Step 3: Pseudo Code\\n\\n```python\\nclass DynamicNormalizedAttentionBlock(GAUBase):\\n def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\\n super().__init__(embed_dim)\\n self.dynamic_norm: GAUBase = DynamicNormalizationUnit(embed_dim, **kwargs)\\n self.dynamic_recurrence: GAUBase = DynamicRecurrenceUnit(embed_dim, **kwargs)\\n self.low_rank_projection: GAUBase = LowRankProjectionUnit(embed_dim, **kwargs)\\n \\n def _forward(self, X, **Z):\\n X, Z = self.dynamic_norm(X, **Z)\\n X, Z = self.dynamic_recurrence(X, **Z)\\n X, Z = self.low_rank_projection(X, **Z)\\n return X, Z\\n```\\n\\n### Step 4: Name of the GAU\\n\\nThe name of the GAU is ```DynamicNormalizedAttentionBlock```.\\n\\n### Step 5: Full Implementation\\n\\n```python\\n# gau.py # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\\n\\n# YOU CAN IMPORT MORE MODULES HERE #\\n\\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\\n\\nclass DynamicNormalizationUnit(GAUBase):\\n def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\\n super().__init__(embed_dim)\\n # Placeholder for dynamic normalization logic\\n\\n def _forward(self, X, **Z):\\n # Placeholder for dynamic normalization logic\\n return X, Z\\n\\nclass DynamicRecurrenceUnit(GAUBase):\\n def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\\n super().__init__(embed_dim)\\n # Placeholder for dynamic recurrence logic\\n\\n def _forward(self, X, **Z):\\n # Placeholder for dynamic recurrence logic\\n return X, Z\\n\\nclass LowRankProjectionUnit(GAUBase):\\n def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\\n super().__init__(embed_dim)\\n # Placeholder for low-rank projection logic\\n\\n def _forward(self, X, **Z):\\n # Placeholder for low-rank projection logic\\n return X, Z\\n\\nclass DynamicNormalizedAttentionBlock(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\\n \"\"\"Generalized Autoregressive Block Unit\\n Input: X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\\n Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\\n Constraints: Causal, differentiable, parameter number, complexity, parallelizable\\n \"\"\"\\n def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\\n # argv: list of hyperparameters\\n factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\\n super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\\n \\n # COMPLETING THE CODE HERE #\\n self.dynamic_norm: GAUBase = DynamicNormalizationUnit(embed_dim, **kwargs)\\n self.dynamic_recurrence: GAUBase = DynamicRecurrenceUnit(embed_dim, **kwargs)\\n self.low_rank_projection: GAUBase = LowRankProjectionUnit(embed_dim, **kwargs)\\n\\n def _forward(self, X, **Z): \\n X, Z = self.dynamic_norm(X, **Z)\\n X, Z = self.dynamic_recurrence(X, **Z)\\n X, Z = self.low_rank_projection(X, **Z)\\n return X, Z\\n```\\n\\nThis implementation sets up the structure for the Dynamic Normalized Attention Block (DNAB) with placeholders for the specific units that will be implemented in future steps.', '_details': {'cost': 0.0, 'running_cost': 0}}\n",
    "\n",
    "raw_text=testtext['text']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
