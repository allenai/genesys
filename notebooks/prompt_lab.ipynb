{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place for experimenting the progressive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.utils as U\n",
    "from model_discovery.configs.gam_config import GAMConfig, GAMConfig_14M\n",
    "from model_discovery.model.composer import GABTree,ROOT_UNIT_TEMPLATE,GAUBase\n",
    "# from model_discovery.evolution import  BuildEvolution\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "db_dir = U.pjoin(ckpt_dir, 'test_composer', 'db')\n",
    "test_tree = GABTree('TestTree', db_dir)\n",
    "\n",
    "prompts_dir='../model_discovery/agents/prompts/'\n",
    "gab_py = U.read_file(U.pjoin(prompts_dir,'gab_template.py'))\n",
    "gam_py = U.read_file(U.pjoin(prompts_dir,'gam_prompt.py'))\n",
    "GAU_TEMPLATE = U.read_file(U.pjoin(prompts_dir,'gau_template.py'))\n",
    "GAU_BASE=inspect.getsource(GAUBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# TestTree.py\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom model_discovery.model.utils.modules import GABUnit # DO NOT CHANGE THIS IMPORT STATEMENT #\\n\\n\\n# YOU CAN IMPORT MORE MODULES HERE #\\n\\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\\n\\n\\nclass TestTree(GABUnit): \\n    \"\"\"Generalized Autoregressive Block\\n        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\\n        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\\n        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\\n    \"\"\"\\n    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\\n        # argv: list of hyperparameters\\n        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\\n        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\\n        \\n        # COMPLETING THE CODE HERE #\\n\\n\\n    # YOU CAN ADD MORE FUNCTIONS HERE #\\n\\n\\n    def _forward(self, X, **Z): \\n\\n        # COMPLETING THE CODE HERE #\\n        \\n        return X\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tree.path\n",
    "test_tree.get_source('TestTree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a professional AI researcher focusing on discovering the best\n",
      "autoregressive language model block. You goal is to design a novel block\n",
      "following the Generalized Autoregressive Block (GAB) structure defined in the\n",
      "following base class:\n",
      "\n",
      "```python \n",
      "class GABBase(nn.Module):\n",
      "    \"\"\" Base class for Generalized Autoregressive Block \"\"\"\n",
      "    def __init__(self,embed_dim: int, block_loc: tuple): \n",
      "        super().__init__()\n",
      "        self.embed_dim = embed_dim\n",
      "        self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\n",
      "\n",
      "    def _forward(self,X,**kwargs): \n",
      "        raise NotImplementedError\n",
      "     \n",
      "    # YOU ARE NOT ALLOW TO OVERRIDE THIS METHOD #\n",
      "    def forward(self,X,**Z):\n",
      "        \"\"\"Forward pass of the model\"\"\"\n",
      "        assert X.shape[-1] == self.embed_dim\n",
      "        Y_=self._forward(X,**Z)\n",
      "        if isinstance(Y_,tuple):\n",
      "            Y, Z = Y_\n",
      "        else:\n",
      "            Z = {}\n",
      "        assert Y.shape == X.shape\n",
      "        return Y, Z\n",
      " ```\n",
      "\n",
      "\n",
      "The GAB will be used to construct a Generalized Autoregressive Model (GAM)\n",
      "defined as follows:\n",
      "\n",
      "```python from transformers.modeling_outputs import CausalLMOutput\n",
      "from transformers import PreTrainedModel, PretrainedConfig\n",
      "from dataclasses import dataclass\n",
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "\n",
      "from gab import GAB, gab_config\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class GAMConfig(PretrainedConfig):\n",
      "    '''Configurations for Generalized Autoregressive Models.'''\n",
      "\n",
      "    d_model: int\n",
      "    n_block: int\n",
      "    batch_tokens: int \n",
      "    vocab_size: int = None\n",
      "\n",
      "\n",
      "class GAM(nn.Module):\n",
      "    ''' Generalized Autoregressive Models\n",
      "        Input:        X: (batch, seqlen, embed_dim)\n",
      "        Output:       Y: (batch, seqlen, embed_dim)\n",
      "    '''\n",
      "    def __init__(\n",
      "        self,\n",
      "        d_model: int,\n",
      "        n_block: int,\n",
      "        vocab_size: int = 50277,\n",
      "        norm_epsilon: float = 1e-5,\n",
      "        device = None,\n",
      "        dtype = None,\n",
      "    ) -> None:\n",
      "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      "        super().__init__()\n",
      "        self.d_model = d_model\n",
      "        self.embedding = nn.Embedding(vocab_size, d_model, **self.factory_kwargs)\n",
      "\n",
      "        block_config = gab_config()\n",
      "        self.blocks = nn.ModuleList(\n",
      "            [\n",
      "                GAB(\n",
      "                    embed_dim=d_model, \n",
      "                    block_loc=(layer_idx,n_block),\n",
      "                    device=device, \n",
      "                    dtype=dtype, \n",
      "                    **block_config\n",
      "                )\n",
      "                for layer_idx in range(n_block)\n",
      "            ]\n",
      "        )\n",
      "        self.norm_out = nn.LayerNorm(\n",
      "            d_model, eps=norm_epsilon, **self.factory_kwargs\n",
      "        )\n",
      "\n",
      "    def forward(self, input_ids):\n",
      "        hidden_states = self.embedding(input_ids)\n",
      "        intermediate_vars = {}\n",
      "        for block in self.blocks:\n",
      "            hidden_states, intermediate_vars = block(\n",
      "                hidden_states,\n",
      "                **intermediate_vars\n",
      "            )\n",
      "        hidden_states = self.norm_out(hidden_states)\n",
      "        return hidden_states\n",
      "\n",
      "\n",
      "class ModisLMHeadModel(PreTrainedModel):\n",
      "    ''' Generalized Autoregressive Models with LM Head '''\n",
      "    config_class = GAMConfig\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: GAMConfig,\n",
      "        device=None,\n",
      "        dtype=None,\n",
      "    ) -> None:\n",
      "        super().__init__(config)\n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      "        self.backbone = GAM(\n",
      "            d_model=config.d_model,\n",
      "            n_block=config.n_block,\n",
      "            vocab_size=config.vocab_size,\n",
      "            **factory_kwargs,\n",
      "        )\n",
      "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False, **factory_kwargs)\n",
      "\n",
      "    def forward(self, input_ids, **gab_kwargs):\n",
      "        hidden_states = self.backbone(input_ids, **gab_kwargs)\n",
      "        lm_logits = self.lm_head(hidden_states)\n",
      "        return CausalLMOutput(logits=lm_logits)\n",
      " ```\n",
      "\n",
      "The produced language model will be pretrained with the corpus and then be\n",
      "applied for downstream tasks. The new model is expected to have a low\n",
      "perplexity, high accuracy, robustness, efficiency, and most importantly, good\n",
      "scalability. You have two roles 1) to propose ideas, analyze the problems,\n",
      "design the model and implement it and; 2) to write the reports that justify your\n",
      "ideas. You do not need to immediately do everything at one response, following\n",
      "the provided instructions, and finish those tasks step by step in the coming\n",
      "multi-round dialog. \n",
      "\n",
      "Since the autoregressive model design is complicated, so we will break it down\n",
      "into smaller parts. We represent a block as multiple nested units, the\n",
      "Generalized Autoregressive Unit (GAU). Each GAU accepts a sequence of embeddings\n",
      "X and a dictionary of intermediate variables Z as input, and outputs a sequence\n",
      "of embeddings Y and a dictionary of new or updated intermediate variables Z_. Z_\n",
      "is optional, when it is provided, it will be used to update Z for the next unit\n",
      "by Z.update(Z_). A GAU is defined in the following base class:\n",
      "\n",
      "```python class GAUBase(nn.Module): \n",
      "    \"\"\" \n",
      "    Instead of directly giving the full implementation of a GAB block, the agent need to \n",
      "    design a series of nested GAB units and construct the full GAB block as a pipeline of these units.\n",
      "\n",
      "    GAB is fractal, like GAB itself, each GAB unit accepts X and Z as input and returns Y and Z as output.\n",
      "    \"\"\" \n",
      "    def __init__(self, embed_dim: int):\n",
      "        super().__init__()\n",
      "        self.embed_dim = embed_dim\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        raise NotImplementedError\n",
      "    \n",
      "    def forward(self, X, Z):\n",
      "        assert len(X.shape) == 3, \"Input shape must be (batch, seqlen, embed_dim)\"\n",
      "        assert X.shape[-1] == self.embed_dim\n",
      "        _params = inspect.signature(self._forward).parameters\n",
      "        _Z = {k: v for k, v in Z.items() if k in _params}\n",
      "        Y = self._forward(X, **_Z)\n",
      "        if isinstance(Y, tuple):\n",
      "            Y, Z_ = Y\n",
      "        else:\n",
      "            Z_ = {}\n",
      "        assert Y.shape == X.shape, f\"GAB Unit must has a sequence with the same shape as input in output, got {Y.shape} instead\"\n",
      "        assert isinstance(Z_, dict), \"Intermediate variables must be stored in a tuple\"\n",
      "        Z.update(Z_) # the new intermediate variables are updated to the current Z\n",
      "        return Y, Z\n",
      " ```\n",
      "\n",
      "You will design a GAU by completing the blanks marked in this template, which\n",
      "includes the initialization where you can define your custom arguments with\n",
      "optional default values, the forward function where you can define convenient\n",
      "functions or classes in the GAB class such as caches, notice that you are only\n",
      "allowed to have only one GAU which inherited from the GAUBase class in the file:\n",
      " \n",
      "```python # gab.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
      "\n",
      "\n",
      "# YOU CAN IMPORT MORE MODULES HERE #\n",
      "\n",
      "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
      "\n",
      "\n",
      "class GAU(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\n",
      "    \"\"\"Generalized Autoregressive Block Unit\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
      "        # argv: list of hyperparameters\n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\n",
      "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
      "        \n",
      "        # COMPLETING THE CODE HERE #\n",
      "\n",
      "        raise NotImplementedError\n",
      "\n",
      "\n",
      "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
      "\n",
      "\n",
      "    def _forward(self, X, **Z): \n",
      "        \n",
      "        # THE CODE HERE MUST BE COMPLETED #\n",
      "\n",
      "        raise NotImplementedError\n",
      "     ```\n",
      "\n",
      "In a GAU, you can call other GAUs, as such, you can create a complicated GAB\n",
      "block by nesting multiple GAUs. However, each GAU should be not too complex, if\n",
      "you want to create complex block, you should break it down into smaller GAUs and\n",
      "nest them. As such, you should design a GAB block in a top-down manner. \n",
      "\n",
      "Notice that, everytime you are only allowed to edit within one GAU. You can\n",
      "leave placeholder definition and calls of the GAUs that you wish to implement\n",
      "later in your GAU. The system will automatically create an initial GAU code for\n",
      "the placeholders. Once a GAU is provided, it will be inserted into the entire\n",
      "GAB composed based on the tree of GAUs under your design and tested for\n",
      "correctness then reviewed for novelty and quality. You will need to ensure the\n",
      "correctness of all the GAUs in the final GAB at the end.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import model_discovery.agents.prompts.prompts as P\n",
    "importlib.reload(P)\n",
    "\n",
    "gu_system_prompt=P.GU_DESIGNER_SYSTEM_prompt.format(GAB_BASE=P.GAB_BASE,GAM_PY=gam_py,GAU_BASE=GAU_BASE,GAU_TEMPLATE=GAU_TEMPLATE)\n",
    "\n",
    "print(gu_system_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Intuitions and Analysis\n",
      "\n",
      "In designing a novel autoregressive block, we aim to create a structure that is both innovative and powerful, capable of outperforming existing state-of-the-art models. The core idea is to leverage a combination of attention mechanisms, feedforward networks, and gating mechanisms to enhance the model's ability to capture complex dependencies in the data. \n",
      "\n",
      "1. **Attention Mechanism**: While attention mechanisms are not new, we can innovate by introducing a dynamic attention mechanism that adapts based on the input sequence characteristics. This can help the model focus on the most relevant parts of the sequence, improving efficiency and accuracy.\n",
      "\n",
      "2. **Feedforward Networks**: We can enhance the traditional feedforward networks by incorporating non-linear transformations and residual connections, allowing the model to learn more complex patterns.\n",
      "\n",
      "3. **Gating Mechanisms**: Introducing gating mechanisms can help control the flow of information through the network, allowing the model to dynamically adjust its behavior based on the input. This can improve robustness and scalability.\n",
      "\n",
      "4. **Scalability and Efficiency**: The design should ensure that the model scales well with increasing data sizes and remains efficient in terms of computational resources.\n",
      "\n",
      "### Rough Plan for Children GABUnits\n",
      "\n",
      "1. **DynamicAttentionUnit**: A unit that implements a dynamic attention mechanism, adapting based on input characteristics.\n",
      "2. **EnhancedFeedforwardUnit**: A unit that incorporates non-linear transformations and residual connections in the feedforward network.\n",
      "3. **GatingMechanismUnit**: A unit that introduces gating mechanisms to control information flow.\n",
      "\n",
      "### Pseudo Code\n",
      "\n",
      "```python\n",
      "class DynamicAttentionUnit(GABUnit):\n",
      "    def __init__(self, embed_dim, **kwargs):\n",
      "        # Initialize attention mechanism\n",
      "        pass\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        # Compute dynamic attention\n",
      "        return Y, Z_\n",
      "\n",
      "class EnhancedFeedforwardUnit(GABUnit):\n",
      "    def __init__(self, embed_dim, **kwargs):\n",
      "        # Initialize feedforward network with non-linear transformations\n",
      "        pass\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        # Apply feedforward transformations\n",
      "        return Y, Z_\n",
      "\n",
      "class GatingMechanismUnit(GABUnit):\n",
      "    def __init__(self, embed_dim, **kwargs):\n",
      "        # Initialize gating mechanisms\n",
      "        pass\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        # Apply gating mechanisms\n",
      "        return Y, Z_\n",
      "```\n",
      "\n",
      "### Name of the GABUnit\n",
      "\n",
      "```unit_name {AdaptiveGAB}```\n",
      "\n",
      "### Full Implementation\n",
      "\n",
      "```python\n",
      "# GAB_UNIT_IMPLEMENTATION\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "from model_discovery.model.utils.modules import GABUnit\n",
      "\n",
      "# YOU CAN IMPORT MORE MODULES HERE #\n",
      "\n",
      "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
      "\n",
      "class AdaptiveGAB(GABUnit): \n",
      "    \"\"\"Generalized Autoregressive Block\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      "        super().__init__(embed_dim)\n",
      "        \n",
      "        # Define the sub-units\n",
      "        self.dynamic_attention: GABUnit = DynamicAttentionUnit(embed_dim, **factory_kwargs)\n",
      "        self.enhanced_feedforward: GABUnit = EnhancedFeedforwardUnit(embed_dim, **factory_kwargs)\n",
      "        self.gating_mechanism: GABUnit = GatingMechanismUnit(embed_dim, **factory_kwargs)\n",
      "\n",
      "    def _forward(self, X, **Z): \n",
      "        # Apply dynamic attention\n",
      "        X, Z = self.dynamic_attention(X, **Z)\n",
      "        \n",
      "        # Apply enhanced feedforward network\n",
      "        X, Z = self.enhanced_feedforward(X, **Z)\n",
      "        \n",
      "        # Apply gating mechanisms\n",
      "        X, Z = self.gating_mechanism(X, **Z)\n",
      "        \n",
      "        return X, Z\n",
      "```\n",
      "\n",
      "### Config\n",
      "\n",
      "```config {\n",
      "    # ADD HYPERPARAMETERS HERE #\n",
      "    \"attention_heads\": 8,\n",
      "    \"feedforward_dim\": 2048,\n",
      "    \"gating_type\": \"sigmoid\"\n",
      "} ``` \n",
      "\n",
      "This design introduces a novel combination of dynamic attention, enhanced feedforward networks, and gating mechanisms, aiming to improve the model's ability to capture complex dependencies while maintaining efficiency and scalability. The next steps will involve implementing the placeholder units and refining the design based on experimental results.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "test_output={'text': '### Intuitions and Analysis\\n\\nIn designing a novel autoregressive block, we aim to create a structure that is both innovative and powerful, capable of outperforming existing state-of-the-art models. The core idea is to leverage a combination of attention mechanisms, feedforward networks, and gating mechanisms to enhance the model\\'s ability to capture complex dependencies in the data. \\n\\n1. **Attention Mechanism**: While attention mechanisms are not new, we can innovate by introducing a dynamic attention mechanism that adapts based on the input sequence characteristics. This can help the model focus on the most relevant parts of the sequence, improving efficiency and accuracy.\\n\\n2. **Feedforward Networks**: We can enhance the traditional feedforward networks by incorporating non-linear transformations and residual connections, allowing the model to learn more complex patterns.\\n\\n3. **Gating Mechanisms**: Introducing gating mechanisms can help control the flow of information through the network, allowing the model to dynamically adjust its behavior based on the input. This can improve robustness and scalability.\\n\\n4. **Scalability and Efficiency**: The design should ensure that the model scales well with increasing data sizes and remains efficient in terms of computational resources.\\n\\n### Rough Plan for Children GABUnits\\n\\n1. **DynamicAttentionUnit**: A unit that implements a dynamic attention mechanism, adapting based on input characteristics.\\n2. **EnhancedFeedforwardUnit**: A unit that incorporates non-linear transformations and residual connections in the feedforward network.\\n3. **GatingMechanismUnit**: A unit that introduces gating mechanisms to control information flow.\\n\\n### Pseudo Code\\n\\n```python\\nclass DynamicAttentionUnit(GABUnit):\\n    def __init__(self, embed_dim, **kwargs):\\n        # Initialize attention mechanism\\n        pass\\n\\n    def _forward(self, X, **Z):\\n        # Compute dynamic attention\\n        return Y, Z_\\n\\nclass EnhancedFeedforwardUnit(GABUnit):\\n    def __init__(self, embed_dim, **kwargs):\\n        # Initialize feedforward network with non-linear transformations\\n        pass\\n\\n    def _forward(self, X, **Z):\\n        # Apply feedforward transformations\\n        return Y, Z_\\n\\nclass GatingMechanismUnit(GABUnit):\\n    def __init__(self, embed_dim, **kwargs):\\n        # Initialize gating mechanisms\\n        pass\\n\\n    def _forward(self, X, **Z):\\n        # Apply gating mechanisms\\n        return Y, Z_\\n```\\n\\n### Name of the GABUnit\\n\\n```unit_name {AdaptiveGAB}```\\n\\n### Full Implementation\\n\\n```python\\n# GAB_UNIT_IMPLEMENTATION\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom model_discovery.model.utils.modules import GABUnit\\n\\n# YOU CAN IMPORT MORE MODULES HERE #\\n\\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\\n\\nclass AdaptiveGAB(GABUnit): \\n    \"\"\"Generalized Autoregressive Block\\n        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\\n        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\\n        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\\n    \"\"\"\\n    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n        super().__init__(embed_dim)\\n        \\n        # Define the sub-units\\n        self.dynamic_attention: GABUnit = DynamicAttentionUnit(embed_dim, **factory_kwargs)\\n        self.enhanced_feedforward: GABUnit = EnhancedFeedforwardUnit(embed_dim, **factory_kwargs)\\n        self.gating_mechanism: GABUnit = GatingMechanismUnit(embed_dim, **factory_kwargs)\\n\\n    def _forward(self, X, **Z): \\n        # Apply dynamic attention\\n        X, Z = self.dynamic_attention(X, **Z)\\n        \\n        # Apply enhanced feedforward network\\n        X, Z = self.enhanced_feedforward(X, **Z)\\n        \\n        # Apply gating mechanisms\\n        X, Z = self.gating_mechanism(X, **Z)\\n        \\n        return X, Z\\n```\\n\\n### Config\\n\\n```config {\\n    # ADD HYPERPARAMETERS HERE #\\n    \"attention_heads\": 8,\\n    \"feedforward_dim\": 2048,\\n    \"gating_type\": \"sigmoid\"\\n} ``` \\n\\nThis design introduces a novel combination of dynamic attention, enhanced feedforward networks, and gating mechanisms, aiming to improve the model\\'s ability to capture complex dependencies while maintaining efficiency and scalability. The next steps will involve implementing the placeholder units and refining the design based on experimental results.', '_details': {'cost': 0.0, 'running_cost': 0}}\n",
    "raw_text=test_output['text']\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Intuitions and Analysis\n",
      "\n",
      "In designing a novel autoregressive block, we aim to create a structure that is both innovative and powerful, capable of outperforming existing state-of-the-art models. The core idea is to leverage a combination of attention mechanisms, feedforward networks, and gating mechanisms to enhance the model's ability to capture complex dependencies in the data. \n",
      "\n",
      "1. **Attention Mechanism**: While attention mechanisms are not new, we can innovate by introducing a dynamic attention mechanism that adapts based on the input sequence characteristics. This can help the model focus on the most relevant parts of the sequence, improving efficiency and accuracy.\n",
      "\n",
      "2. **Feedforward Networks**: We can enhance the traditional feedforward networks by incorporating non-linear transformations and residual connections, allowing the model to learn more complex patterns.\n",
      "\n",
      "3. **Gating Mechanisms**: Introducing gating mechanisms can help control the flow of information through the network, allowing the model to dynamically adjust its behavior based on the input. This can improve robustness and scalability.\n",
      "\n",
      "4. **Scalability and Efficiency**: The design should ensure that the model scales well with increasing data sizes and remains efficient in terms of computational resources.\n",
      "\n",
      "### Rough Plan for Children GABUnits\n",
      "\n",
      "1. **DynamicAttentionUnit**: A unit that implements a dynamic attention mechanism, adapting based on input characteristics.\n",
      "2. **EnhancedFeedforwardUnit**: A unit that incorporates non-linear transformations and residual connections in the feedforward network.\n",
      "3. **GatingMechanismUnit**: A unit that introduces gating mechanisms to control information flow.\n",
      "\n",
      "### Pseudo Code\n",
      "\n",
      "```python\n",
      "class DynamicAttentionUnit(GABUnit):\n",
      "    def __init__(self, embed_dim, **kwargs):\n",
      "        # Initialize attention mechanism\n",
      "        pass\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        # Compute dynamic attention\n",
      "        return Y, Z_\n",
      "\n",
      "class EnhancedFeedforwardUnit(GABUnit):\n",
      "    def __init__(self, embed_dim, **kwargs):\n",
      "        # Initialize feedforward network with non-linear transformations\n",
      "        pass\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        # Apply feedforward transformations\n",
      "        return Y, Z_\n",
      "\n",
      "class GatingMechanismUnit(GABUnit):\n",
      "    def __init__(self, embed_dim, **kwargs):\n",
      "        # Initialize gating mechanisms\n",
      "        pass\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        # Apply gating mechanisms\n",
      "        return Y, Z_\n",
      "```\n",
      "\n",
      "### Name of the GABUnit\n",
      "\n",
      "```unit_name {AdaptiveGAB}```\n",
      "\n",
      "### Full Implementation\n",
      "\n",
      "```python\n",
      "# GAB_UNIT_IMPLEMENTATION\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "from model_discovery.model.utils.modules import GABUnit\n",
      "\n",
      "# YOU CAN IMPORT MORE MODULES HERE #\n",
      "\n",
      "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
      "\n",
      "class AdaptiveGAB(GABUnit): \n",
      "    \"\"\"Generalized Autoregressive Block\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      "        super().__init__(embed_dim)\n",
      "        \n",
      "        # Define the sub-units\n",
      "        self.dynamic_attention: GABUnit = DynamicAttentionUnit(embed_dim, **factory_kwargs)\n",
      "        self.enhanced_feedforward: GABUnit = EnhancedFeedforwardUnit(embed_dim, **factory_kwargs)\n",
      "        self.gating_mechanism: GABUnit = GatingMechanismUnit(embed_dim, **factory_kwargs)\n",
      "\n",
      "    def _forward(self, X, **Z): \n",
      "        # Apply dynamic attention\n",
      "        X, Z = self.dynamic_attention(X, **Z)\n",
      "        \n",
      "        # Apply enhanced feedforward network\n",
      "        X, Z = self.enhanced_feedforward(X, **Z)\n",
      "        \n",
      "        # Apply gating mechanisms\n",
      "        X, Z = self.gating_mechanism(X, **Z)\n",
      "        \n",
      "        return X, Z\n",
      "```\n",
      "\n",
      "### Config\n",
      "\n",
      "```config {\n",
      "    # ADD HYPERPARAMETERS HERE #\n",
      "    \"attention_heads\": 8,\n",
      "    \"feedforward_dim\": 2048,\n",
      "    \"gating_type\": \"sigmoid\"\n",
      "} ``` \n",
      "\n",
      "This design introduces a novel combination of dynamic attention, enhanced feedforward networks, and gating mechanisms, aiming to improve the model's ability to capture complex dependencies while maintaining efficiency and scalability. The next steps will involve implementing the placeholder units and refining the design based on experimental results.\n"
     ]
    }
   ],
   "source": [
    "codes = re.findall(r\"```python(.*?)```\", raw_text, re.DOTALL)\n",
    "unit_name = re.findall(r\"```unit_name(.*?)```\", raw_text, re.DOTALL)\n",
    "config = re.findall(r\"```config(.*?)```\", raw_text, re.DOTALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Proposal: Scalable and Efficient Generalized Autoregressive Block (SE-GAB)\n",
      "\n",
      "## 1. Title: Scalable and Efficient Generalized Autoregressive Block (SE-GAB)\n",
      "\n",
      "## 2. Motivation\n",
      "\n",
      "The current landscape of autoregressive models, particularly those based on the Transformer architecture, faces challenges in terms of scalability, efficiency, and handling long sequences. The quadratic complexity of self-attention mechanisms limits their applicability to long sequences, and while alternative architectures like linear attention and state space models exist, they often underperform in terms of pretraining efficiency and downstream task accuracy. Inspired by recent advancements such as the Megalodon architecture, which introduces efficient sequence modeling with unlimited context length, this proposal aims to design a novel Generalized Autoregressive Block (GAB) that addresses these challenges.\n",
      "\n",
      "## 3. Problem Analysis\n",
      "\n",
      "The primary issues with existing autoregressive models include:\n",
      "\n",
      "- **Quadratic Complexity**: The self-attention mechanism in Transformers has a quadratic complexity with respect to the sequence length, making it computationally expensive for long sequences.\n",
      "- **Weak Length Extrapolation**: Transformers struggle with extrapolating to sequences longer than those seen during training.\n",
      "- **Inefficiency in Pretraining**: Sub-quadratic solutions often underperform in terms of pretraining efficiency and accuracy.\n",
      "- **Scalability**: As models grow in size, maintaining efficiency and performance becomes increasingly challenging.\n",
      "\n",
      "## 4. Core Idea and Philosophy\n",
      "\n",
      "The core idea behind SE-GAB is to create a scalable and efficient autoregressive block that can handle long sequences without sacrificing performance. The design philosophy is centered around:\n",
      "\n",
      "- **Efficiency**: Incorporating mechanisms that reduce computational complexity while maintaining or improving performance.\n",
      "- **Scalability**: Designing the architecture to scale effectively with model size and sequence length.\n",
      "- **Robustness**: Ensuring the model is robust to variations in input sequence length and can generalize well to unseen data.\n",
      "- **Modularity**: Building the GAB as a composition of smaller, efficient units (GAUs) that can be easily modified or extended.\n",
      "\n",
      "## 5. Plan of the Design\n",
      "\n",
      "### 5.1. Efficient Attention Mechanism\n",
      "\n",
      "- **Chunked Attention**: Implement a chunked attention mechanism to reduce the computational complexity of self-attention. This involves dividing the sequence into smaller chunks and applying attention within each chunk.\n",
      "- **Rotary Positional Embeddings**: Use rotary positional embeddings to enhance the model's ability to handle long sequences and improve extrapolation capabilities.\n",
      "\n",
      "### 5.2. Moving Average Gated Attention\n",
      "\n",
      "- **Exponential Moving Average (EMA)**: Integrate an EMA mechanism to capture long-term dependencies efficiently.\n",
      "- **Gated Attention**: Use gated attention to control the flow of information and improve model stability.\n",
      "\n",
      "### 5.3. Normalized Feedforward Network\n",
      "\n",
      "- **Layer Normalization**: Apply layer normalization to stabilize training and improve convergence.\n",
      "- **Feedforward Network with SWIGLU**: Implement a feedforward network with the SWIGLU activation function to enhance non-linear transformations.\n",
      "\n",
      "### 5.4. Modular Design with GAUs\n",
      "\n",
      "- **Nested GAUs**: Design the GAB as a composition of nested GAUs, each responsible for a specific function (e.g., attention, normalization, feedforward).\n",
      "- **Intermediate Variable Management**: Efficiently manage intermediate variables to facilitate information flow between GAUs.\n",
      "\n",
      "## 6. Conclusion\n",
      "\n",
      "The proposed SE-GAB aims to address the limitations of current autoregressive models by introducing a scalable and efficient architecture. By leveraging techniques such as chunked attention, EMA, and modular design, SE-GAB is expected to achieve lower perplexity, higher accuracy, and better scalability. This design will serve as a foundation for future advancements in autoregressive models, enabling them to handle increasingly complex tasks and longer sequences.\n",
      "\n",
      "## 7. References\n",
      "\n",
      "- Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou. \"MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length.\" arXiv.org, 2024.\n"
     ]
    }
   ],
   "source": [
    "testtext={'text': '# Proposal: Scalable and Efficient Generalized Autoregressive Block (SE-GAB)\\n\\n## 1. Title: Scalable and Efficient Generalized Autoregressive Block (SE-GAB)\\n\\n## 2. Motivation\\n\\nThe current landscape of autoregressive models, particularly those based on the Transformer architecture, faces challenges in terms of scalability, efficiency, and handling long sequences. The quadratic complexity of self-attention mechanisms limits their applicability to long sequences, and while alternative architectures like linear attention and state space models exist, they often underperform in terms of pretraining efficiency and downstream task accuracy. Inspired by recent advancements such as the Megalodon architecture, which introduces efficient sequence modeling with unlimited context length, this proposal aims to design a novel Generalized Autoregressive Block (GAB) that addresses these challenges.\\n\\n## 3. Problem Analysis\\n\\nThe primary issues with existing autoregressive models include:\\n\\n- **Quadratic Complexity**: The self-attention mechanism in Transformers has a quadratic complexity with respect to the sequence length, making it computationally expensive for long sequences.\\n- **Weak Length Extrapolation**: Transformers struggle with extrapolating to sequences longer than those seen during training.\\n- **Inefficiency in Pretraining**: Sub-quadratic solutions often underperform in terms of pretraining efficiency and accuracy.\\n- **Scalability**: As models grow in size, maintaining efficiency and performance becomes increasingly challenging.\\n\\n## 4. Core Idea and Philosophy\\n\\nThe core idea behind SE-GAB is to create a scalable and efficient autoregressive block that can handle long sequences without sacrificing performance. The design philosophy is centered around:\\n\\n- **Efficiency**: Incorporating mechanisms that reduce computational complexity while maintaining or improving performance.\\n- **Scalability**: Designing the architecture to scale effectively with model size and sequence length.\\n- **Robustness**: Ensuring the model is robust to variations in input sequence length and can generalize well to unseen data.\\n- **Modularity**: Building the GAB as a composition of smaller, efficient units (GAUs) that can be easily modified or extended.\\n\\n## 5. Plan of the Design\\n\\n### 5.1. Efficient Attention Mechanism\\n\\n- **Chunked Attention**: Implement a chunked attention mechanism to reduce the computational complexity of self-attention. This involves dividing the sequence into smaller chunks and applying attention within each chunk.\\n- **Rotary Positional Embeddings**: Use rotary positional embeddings to enhance the model\\'s ability to handle long sequences and improve extrapolation capabilities.\\n\\n### 5.2. Moving Average Gated Attention\\n\\n- **Exponential Moving Average (EMA)**: Integrate an EMA mechanism to capture long-term dependencies efficiently.\\n- **Gated Attention**: Use gated attention to control the flow of information and improve model stability.\\n\\n### 5.3. Normalized Feedforward Network\\n\\n- **Layer Normalization**: Apply layer normalization to stabilize training and improve convergence.\\n- **Feedforward Network with SWIGLU**: Implement a feedforward network with the SWIGLU activation function to enhance non-linear transformations.\\n\\n### 5.4. Modular Design with GAUs\\n\\n- **Nested GAUs**: Design the GAB as a composition of nested GAUs, each responsible for a specific function (e.g., attention, normalization, feedforward).\\n- **Intermediate Variable Management**: Efficiently manage intermediate variables to facilitate information flow between GAUs.\\n\\n## 6. Conclusion\\n\\nThe proposed SE-GAB aims to address the limitations of current autoregressive models by introducing a scalable and efficient architecture. By leveraging techniques such as chunked attention, EMA, and modular design, SE-GAB is expected to achieve lower perplexity, higher accuracy, and better scalability. This design will serve as a foundation for future advancements in autoregressive models, enabling them to handle increasingly complex tasks and longer sequences.\\n\\n## 7. References\\n\\n- Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou. \"MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length.\" arXiv.org, 2024.', '_details': {'cost': 0.0, 'running_cost': 0}}\n",
    "\n",
    "\n",
    "print(testtext['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
