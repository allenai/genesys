{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place for experimenting the progressive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\prompts\\prompts.py:1471: SyntaxWarning: invalid escape sequence '\\('\n",
      "  GUE_DESIGN_PROPOSER_SYSTEM_prompt = \"\"\"\n",
      "c:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\prompts\\prompts.py:1696: SyntaxWarning: invalid escape sequence '\\('\n",
      "  GUE_PROPOSAL_REVIEWER_SYSTEM_prompt = \"\"\"\n",
      "c:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\prompts\\prompts.py:2325: SyntaxWarning: invalid escape sequence '\\('\n",
      "  GUE_DESIGNER_SYSTEM_prompt_part4 = \"\"\"\n",
      "c:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\prompts\\prompts.py:2631: SyntaxWarning: invalid escape sequence '\\('\n",
      "  GUE_IMPLEMENTATION_REVIEWER_SYSTEM_prompt = \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.utils as U\n",
    "from model_discovery.configs.gam_config import GAMConfig, GAMConfig_14M\n",
    "# from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "from model_discovery.model.composer import GAUBase, GAUTree, check_tree_name, GABComposer\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "from model_discovery.evolution import BuildEvolution\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "\n",
    "prompts_dir='../model_discovery/agents/prompts/'\n",
    "gab_py = U.read_file(U.pjoin(prompts_dir,'gab_template.py'))\n",
    "gam_prompt_path = os.path.join(prompts_dir,'gam_prompt.py')\n",
    "gau_template_path = os.path.join(prompts_dir,'gau_template.py')\n",
    "GAM_TEMPLATE=open(gam_prompt_path).read()\n",
    "GAU_TEMPLATE=open(gau_template_path).read()\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n",
    "\n",
    "GAB_BASE=inspect.getsource(GABBase)\n",
    "GAB_COMPOSER=inspect.getsource(GABComposer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os,sys\n",
    "import importlib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "import model_discovery.agents.prompts.prompts as P\n",
    "import model_discovery.model.composer\n",
    "importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUTree,GAUBase,GAUNode\n",
    "import model_discovery.utils\n",
    "importlib.reload(model_discovery.utils)\n",
    "import model_discovery.utils as U\n",
    "\n",
    "\n",
    "\n",
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "# Placeholder classes for future implementation\n",
    "class MemoryAccessUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, memory_size, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "class DownsamplingUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, downsample_factor, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "\n",
    "class XXX(GAUBase,object):\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "\n",
    "class GAU(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
    "        # argv: list of hyperparameters\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\n",
    "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        self.a1 = kwarg_all.get('a1', 1)\n",
    "        self.a2: int = kwarg_all.get('a2', 2)\n",
    "\n",
    "        # COMPLETING THE CODE HERE #\n",
    "        self.token_scorer = TokenScoringGAU(embed_dim, **factory_kwargs)\n",
    "        self.dual_path: GAUBase = DualPathGAU(embed_dim, **factory_kwargs)\n",
    "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim, **factory_kwargs)\n",
    "\n",
    "        self.stages: nn.ModuleList = nn.ModuleList([CompressionStageGAU(embed_dim, device=\n",
    "            device, dtype=dtype, **kwargs) for _ in range(2)])\n",
    "\n",
    "        self.modelx=nn.ModuleList([\n",
    "            nn.Conv2d(1,20,5),\n",
    "            nn.ReLU(),\n",
    "            LatentAttentionGAU(),\n",
    "            nn.Conv2d(20,64,5),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "            \n",
    "\n",
    "        self.model1 = (\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU(),\n",
    "            'gau3': CompressionStageGAU()\n",
    "        })\n",
    "\n",
    "        self.model2 = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('gau2', DualPathGAU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "    def init_something(self):\n",
    "        self.modelx[3]=LatentAttentionGAU()\n",
    "        self.modelx.append(LatentAttentionGAU())\n",
    "        self.modelx.insert(0,LatentAttentionGAU())\n",
    "        self.activations['gau1']=LatentAttentionGAU()\n",
    "        self.model2.add_module('gau4',LatentAttentionGAU())\n",
    "\n",
    "        X=self.dual_path(X)\n",
    "    \n",
    "    def _forward(self, X, **Z): \n",
    "\n",
    "        # THE CODE HERE MUST BE COMPLETED #\n",
    "        # Step 1: Score tokens\n",
    "        X, Z = self.token_scorer(X, **Z)\n",
    "        # Step 2: Route through dual paths\n",
    "        # Step 3: Apply latent attention\n",
    "        Y, Z = self.latent_attention(X, A=A, B=B, **Z)\n",
    "        for stage in self.stages:\n",
    "            X = stage(X, **Z)\n",
    "        \n",
    "        X, Z = self.dual_path(X, **Z)\n",
    "        X=self.activations['gau3'](X)\n",
    "\n",
    "        return Y, Z\n",
    "\n",
    "@gau_test\n",
    "def test1(a=1,device=None):\n",
    "    print('test1')\n",
    "\n",
    "    \n",
    "@gau_test\n",
    "def test2(device=None):\n",
    "    print('test2')\n",
    "\n",
    "\n",
    "CHILDREN_DECLARATIONS = [\n",
    "    UnitDecl(\n",
    "        unitname='TokenScoringGAU', \n",
    "        requirements='Token scoring GAU',\n",
    "        inputs=[],\n",
    "        outputs=[],\n",
    "    ),\n",
    "    UnitDecl(\n",
    "        unitname='DualPathGAU',\n",
    "        requirements='Dual path GAU',\n",
    "        inputs=[],\n",
    "        outputs=[],\n",
    "    ),\n",
    "    UnitDecl(\n",
    "        unitname='LatentAttentionGAU',\n",
    "        requirements='Latent attention GAU',\n",
    "        inputs=[],\n",
    "        outputs=[],\n",
    "    ),\n",
    "    UnitDecl(\n",
    "        unitname='CompressionStageGAU',\n",
    "        requirements='Compression stage GAU',\n",
    "        inputs=[],\n",
    "        outputs=[],\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "class MHA(GAUBase):\n",
    "    \"\"\"\n",
    "    MHA combines hierarchical gating mechanisms from HGRN2 with state space modeling (SSM) to improve long-range dependency capture and computational efficiency.\n",
    "\n",
    "    This class replaces the traditional multi-head attention mechanism with a more efficient approach that maintains linear complexity with respect to sequence length.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): The embedding dimension of the input sequence.\n",
    "        block_loc (tuple): The location of the block within the network.\n",
    "        kwarg_all (dict): Dictionary of additional keyword arguments for initialization.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        expand_ratio (int): The ratio for expansion layers.\n",
    "        d_state (int): The dimension of the state in the state space model.\n",
    "\n",
    "    Returns:\n",
    "        Y (Tensor): The output sequence with the same shape as the input.\n",
    "\n",
    "    Example:\n",
    "        >>> mha = MHA(embed_dim=512, block_loc=(0, 0), kwarg_all={}, num_heads=8, expand_ratio=4, d_state=64)\n",
    "        >>> X = torch.randn(2, 10, 512)\n",
    "        >>> Y, Z = mha(X)\n",
    "\n",
    "    Note:\n",
    "        This implementation maintains interpretability through the `get_attention_map` method, which provides a way to visualize effective attention patterns.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, num_heads: int, expand_ratio: int, d_state: int, device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.d_state = d_state\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # HGRN2-inspired projections\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "        # SSM components\n",
    "        self.ssm_A = nn.Parameter(torch.randn(num_heads, d_state, d_state))\n",
    "        self.ssm_B = nn.Parameter(torch.randn(num_heads, d_state, 1))\n",
    "        self.ssm_C = nn.Parameter(torch.randn(num_heads, 1, d_state))\n",
    "\n",
    "        # Expansion layers\n",
    "        self.expand = nn.Linear(embed_dim, embed_dim * expand_ratio, bias=True)\n",
    "        self.contract = nn.Linear(embed_dim * expand_ratio, embed_dim, bias=True)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "        # Layernorm for stability\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize SSM components\n",
    "        nn.init.orthogonal_(self.ssm_A)\n",
    "        nn.init.normal_(self.ssm_B, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.ssm_C, mean=0.0, std=0.02)\n",
    "\n",
    "        # Initialize other weights\n",
    "        nn.init.xavier_uniform_(self.q_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.k_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.v_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.expand.weight)\n",
    "        nn.init.xavier_uniform_(self.contract.weight)\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        B, L, D = X.shape\n",
    "        H = self.num_heads\n",
    "\n",
    "        # HGRN2-inspired projections\n",
    "        Q = self.q_proj(X).view(B, L, H, -1)\n",
    "        K = self.k_proj(X).view(B, L, H, -1)\n",
    "        V = self.v_proj(X).view(B, L, H, -1)\n",
    "\n",
    "        # Expansion\n",
    "        X_exp = self.expand(X).view(B, L, H, -1)\n",
    "\n",
    "        # SSM state update\n",
    "        state = torch.zeros(B, H, self.d_state, 1, device=X.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(L):\n",
    "            # Update state\n",
    "            state = torch.matmul(self.ssm_A, state) + self.ssm_B * K[:, t, :, :, None]\n",
    "\n",
    "            # Compute output\n",
    "            output = torch.matmul(self.ssm_C, state).squeeze(-1)\n",
    "            output = output * Q[:, t, :, :]\n",
    "            output = torch.sum(output * V[:, t, :, :], dim=-1)\n",
    "\n",
    "            # Apply expansion\n",
    "            output = output * X_exp[:, t, :, :]\n",
    "\n",
    "            outputs.append(output)\n",
    "\n",
    "        output = torch.stack(outputs, dim=1)\n",
    "        output = output.view(B, L, -1)\n",
    "\n",
    "        # Contraction\n",
    "        output = self.contract(output)\n",
    "\n",
    "        # Output projection and residual connection\n",
    "        output = self.out_proj(output)\n",
    "        output = self.norm(X + output)\n",
    "\n",
    "        return output, Z\n",
    "\n",
    "    def get_attention_map(self, X):\n",
    "        B, L, D = X.shape\n",
    "        H = self.num_heads\n",
    "\n",
    "        Q = self.q_proj(X).view(B, L, H, -1)\n",
    "        K = self.k_proj(X).view(B, L, H, -1)\n",
    "\n",
    "        attention_map = torch.zeros(B, H, L, L, device=X.device)\n",
    "        state = torch.zeros(B, H, self.d_state, 1, device=X.device)\n",
    "\n",
    "        for t in range(L):\n",
    "            state = torch.matmul(self.ssm_A, state) + self.ssm_B * K[:, t, :, :, None]\n",
    "            output = torch.matmul(self.ssm_C, state).squeeze(-1)\n",
    "            attention_map[:, :, t, :t+1] = torch.sum(output * Q[:, :t+1, :, :], dim=-1).transpose(1, 2)\n",
    "\n",
    "        return attention_map\n",
    "\n",
    "\n",
    "# WRITE YOUR UNIT TEST FUNCTIONS HERE #\n",
    "\n",
    "@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON'T BE RECOGNIZED AS A UNIT TEST #\n",
    "def test_mha(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\n",
    "    # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\n",
    "    embed_dim = 128\n",
    "    block_loc = (0, 0)\n",
    "    kwarg_all = {}\n",
    "    num_heads = 8\n",
    "    expand_ratio = 4\n",
    "    d_state = 64\n",
    "    mha = MHA(embed_dim, block_loc, kwarg_all, num_heads, expand_ratio, d_state, device=device, dtype=dtype)\n",
    "    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "    y, Z_ = mha(x, **Z)\n",
    "    assert y.shape == (1, 100, 128)\n",
    "\n",
    "    # WRITE ASSERTIONS TO PERFORM THE TEST, USE PRINT TO DEBUG #\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "    # raise NotImplementedError # YOU MUST IMPLEMENT THIS FUNCTION #\n",
    "'''\n",
    "children=['ProjectionGAU','SimilarityComputationGAU','NormalizationGAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_name = \"MHA\"  # Provide the unit_name to rename GAU class\n",
    "reformatted_code, new_args, gau_tests, errors, warnings, fetal_errors,docstring,children_decl = check_and_reformat_gau_code(code, unit_name)\n",
    "print(\"Reformatted Code:\\n\" + reformatted_code)\n",
    "print(\"Errors:\\n\", '\\n\\t'.join(errors+fetal_errors))\n",
    "print(\"Warnings:\\n\", '\\n\\t'.join(warnings))\n",
    "print(\"New Arguments:\\n\", new_args)\n",
    "print(\"Docstring:\\n\", docstring)\n",
    "print(\"Children Declarations:\\n\", children_decl)\n",
    "\n",
    "\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "spec=P.UnitSpec(\n",
    "   unitname = unit_name,\n",
    "   document = 'Test',\n",
    "   inputs = [],\n",
    "   outputs = [],\n",
    ")\n",
    "# test_tree.add_unit(\n",
    "#     spec, reformatted_code, new_args, None, None, None, children, gau_tests, None\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[{'role': 'system', 'content': '\\nYou are a professional AI researcher focusing on discovering the best\\nautoregressive language model block. Your goal is to design a novel block\\nfollowing the Generalized Autoregressive Block (GAB) structure defined in the\\nfollowing base class:\\n\\n```python class GABBase(nn.Module):\\n \"\"\" Base class for Generalized Autoregressive Block \"\"\"\\n def __init__(self,embed_dim: int, block_loc: tuple):\\n super().__init__()\\n self.embed_dim = embed_dim\\n self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\\n\\n def _forward(self, X, **Z): \\n raise NotImplementedError\\n \\n # YOU ARE NOT ALLOW TO OVERRIDE THIS METHOD #\\n def forward(self, X, **Z): # kwargs not parsable by torchscript but more flexible\\n \"\"\"Forward pass of the model\"\"\"\\n assert len(X.shape) == 3, f\"Input shape must be (batch, seqlen, embed_dim), got {len(X.shape)}D instead\"\\n assert X.shape[-1] == self.embed_dim, f\"Input shape must be (batch, seqlen, embed_dim), got {X.shape} instead\"\\n Y = self._forward(X, **Z)\\n if isinstance(Y, tuple):\\n Y, Z_ = Y\\n else:\\n Z_ = {}\\n assert Y.shape == X.shape, f\"GAB Output shape must be the same as input shape of {X.shape}, got {Y.shape} instead\"\\n assert isinstance(Z, dict), \"Intermediate variables must be stored in a dict\"\\n Z.update(Z_) # the new intermediate variables are updated to the current Z\\n return Y, Z\\n ```\\n\\nThe GAB will be used to construct a Generalized Autoregressive Model (GAM)\\ndefined as follows:\\n\\n```python from transformers.modeling_outputs import CausalLMOutput\\nfrom transformers import PreTrainedModel, PretrainedConfig\\nfrom dataclasses import dataclass\\n\\nimport torch\\nfrom torch import nn\\n\\nfrom gab import GAB, gab_config\\n\\n\\n@dataclass\\nclass GAMConfig(PretrainedConfig):\\n \\'\\'\\'Configurations for Generalized Autoregressive Models.\\'\\'\\'\\n\\n d_model: int\\n n_block: int\\n batch_tokens: int \\n vocab_size: int = None\\n\\n\\nclass GAM(nn.Module):\\n \\'\\'\\' Generalized Autoregressive Models\\n Input: X: (batch, seqlen, embed_dim)\\n Output: Y: (batch, seqlen, embed_dim)\\n \\'\\'\\'\\n def __init__(\\n self,\\n d_model: int,\\n n_block: int,\\n vocab_size: int = 50277,\\n norm_epsilon: float = 1e-5,\\n device = None,\\n dtype = None,\\n ) -> None:\\n self.factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n super().__init__()\\n self.d_model = d_model\\n self.embedding = nn.Embedding(vocab_size, d_model, **self.factory_kwargs)\\n\\n block_config = gab_config()\\n self.blocks = nn.ModuleList(\\n [\\n GAB(\\n embed_dim=d_model, \\n block_loc=(layer_idx,n_block),\\n device=device, \\n dtype=dtype, \\n **block_config\\n )\\n for layer_idx in range(n_block)\\n ]\\n )\\n self.norm_out = nn.LayerNorm(\\n d_model, eps=norm_epsilon, **self.factory_kwargs\\n )\\n\\n def forward(self, input_ids):\\n hidden_states = self.embedding(input_ids)\\n intermediate_vars = {}\\n for block in self.blocks:\\n hidden_states, intermediate_vars = block(\\n hidden_states,\\n **intermediate_vars\\n )\\n hidden_states = self.norm_out(hidden_states)\\n return hidden_states\\n\\n\\nclass GLMHeadModel(PreTrainedModel):\\n \\'\\'\\' Generalized Autoregressive Models with LM Head \\'\\'\\'\\n config_class = GAMConfig\\n\\n def __init__(\\n self,\\n config: GAMConfig,\\n device=None,\\n dtype=None,\\n ) -> None:\\n super().__init__(config)\\n factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n self.backbone = GAM(\\n d_model=config.d_model,\\n n_block=config.n_block,\\n vocab_size=config.vocab_size,\\n **factory_kwargs,\\n )\\n self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False, **factory_kwargs)\\n\\n def forward(self, input_ids, **gab_kwargs):\\n hidden_states = self.backbone(input_ids, **gab_kwargs)\\n lm_logits = self.lm_head(hidden_states)\\n return CausalLMOutput(logits=lm_logits)\\n ```\\n\\nThe produced language model will be pretrained with the corpus and then be\\napplied for downstream tasks. The new model is expected to have a low\\nperplexity, high accuracy, robustness, efficiency, and most importantly, good\\nscalability. \\n\\nSince the autoregressive model design is complicated, so we will break it down\\ninto smaller parts. We represent a block as multiple nested units, the\\nGeneralized Autoregressive Unit (GAU). Each GAU accepts a sequence of embeddings\\nX and a dictionary of intermediate variables Z as input, and outputs a sequence\\nof embeddings Y and a dictionary of new or updated intermediate variables Z_. Z_\\nis optional, when it is provided, it will be used to update Z for the next unit\\nby Z.update(Z_). A GAU is defined in the following base class:\\n\\n```python class GAUBase(nn.Module): \\n \"\"\" \\n Instead of directly giving the full implementation of a GAB block, the agent need to \\n design a series of nested GAB units and construct the full GAB block as a pipeline of these units.\\n\\n GAB is fractal, like GAB itself, each GAB unit accepts X and Z as input and returns Y and Z as output.\\n \"\"\" \\n def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict):\\n super().__init__()\\n self.embed_dim = embed_dim\\n self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\\n self.kwarg_all = kwarg_all # dict of all kwargs, use it to initialize the children units\\n\\n def _forward(self, X, **Z): # you can define the arguments instead of using **Z\\n raise NotImplementedError\\n \\n def forward(self, X, **Z):\\n assert len(X.shape) == 3 and X.shape[-1] == self.embed_dim, f\"Input X must be a sequence of shape (batch, seqlen, embed_dim), all other kinds of variables should be passed by Z. Got {X.shape} instead.\"\\n _params = inspect.signature(self._forward).parameters\\n X=X.to(**self.factory_kwargs)\\n _Z = {k: v for k, v in Z.items() if k in _params}\\n Y = self._forward(X, **_Z)\\n if isinstance(Y, tuple):\\n Y, Z_ = Y\\n else:\\n Z_ = {}\\n assert Y.shape == X.shape, f\"GAU output Y must be a sequence with the same shape as input of {X.shape}, got {Y.shape} instead, all other kinds of variables should be passed by Z.\"\\n assert isinstance(Z_, dict), \"Intermediate variables must be stored in a dict\"\\n Z.update(Z_) # the new intermediate variables are updated to the current Z\\n return Y, Z\\n ```\\n\\nYou will design a GAU by completing the blanks marked in this template, which\\nincludes the initialization where you can define your custom arguments with\\noptional default values, the forward function where you can define convenient\\nfunctions or classes in the GAB class such as caches, notice that you are only\\nallowed to have only one GAU which inherited from the GAUBase class in the file:\\n \\n```python # gau.py # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\\n\\n\\n# YOU CAN IMPORT MORE MODULES HERE #\\n\\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\\n\\n\\nclass UnitName(GAUBase):\\n \"\"\"Generalized Autoregressive Block Unit\\n Input: X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \\n Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\\n Constraints: Causal, differentiable, parameter number, complexity, parallelizable...\\n\\n embed_dim: The dimension of the input embeddings\\n block_loc: The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\\n kwarg_all: A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\\n \"\"\"\\n def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT KEEP THE ORIGINAL ONES #\\n self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\\n super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\\n \\n # COMPLETING THE CODE HERE #\\n\\n raise NotImplementedError\\n\\n\\n # YOU CAN ADD MORE FUNCTIONS HERE #\\n\\n\\n def _forward(self, X, **Z): \\n \\n # THE CODE HERE MUST BE COMPLETED #\\n\\n raise NotImplementedError\\n\\n\\n# WRITE YOUR UNIT TEST FUNCTIONS HERE #\\n\\n@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON\\'T BE RECOGNIZED AS A UNIT TEST #\\ndef unit_test_name(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\\n # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\\n\\n # WRITE ASSERTIONS TO PERFORM THE TEST, USE PRINT TO DEBUG #\\n \\n raise NotImplementedError # YOU MUST IMPLEMENT THIS FUNCTION #\\n ```\\n\\nIn a GAU, you can call other GAUs, as such, you can create a complicated GAB\\nblock by nesting multiple GAUs. However, each GAU should be not too complex, if\\nyou want to create complex block, you should break it down into smaller GAUs and\\nnest them. As such, you should design a GAB block in a top-down manner. \\n\\nInstead of starting from scratch, you will start from an existing design and\\nimprove it. You will be provided with the full information of the design,\\nincluding the proposal, the tree structure, the implementations of the GAUs. You\\nare only allowed to modify *one GAU* from the existing design. \\n\\nYou need to select one GAU to modify, you can define new children GAUs, however\\nyou need to guarantee that your modification wont affect the correctness of the\\noverall design. \\n\\nYou will start by writing down an overal proposal for the design you want to\\nhave, the proposal decides a direction, phylosophy and the plan of the design,\\nand the analysis of the problem and how you gonna solve it by modifying one GAU\\nfrom the existing design. You will be provided with one or multiple references\\nto consider that may inspire you if there are references provided.\\n\\nYour response should include: \\n\\n1. The proposal, it should include but not restrict to the following parts: a. A\\n title with the name of the design in the level 1 header format. You shuld\\n have only one level 1 header in your response which is the name of the\\n design.\\n\\n b. Your motivation of the design. What problem you want to solve based on the\\n insights or observations you have about the autoregressive models today,\\n and any inspirations you may have from the references. \\n\\n c. The analysis of the problem.\\n\\n d. The core idea and phylosophy behind of your design that may solve the\\n problem you proposed. \\n\\n e. The plan of the design. You should include subsections of that describe\\n the details of each part of the design with the justifications. The\\n selection of the GAU to modify and the reasoning of the selection. \\n\\n f. A conclution of the proposal. \\n\\n g. Optional, the references you used in your proposal, should be in the right\\n format.\\n2. The name of the variant of the model you are going to design.\\n3. The selection of the GAU to modify.\\n\\nThe proposal will be reviewed and you will be asked to modify it if it is not\\npassed. You can start to implement the design after the proposal is passed. \\n\\nThe proposal should be as detailed as possible, DO NOT WORRY IF THE PROPOSAL IS\\nTOO LONG, BUT ALSO DO NOT FILL IN BY REDUNDANT WORDS, USE PRECISE AND CONCRETE\\nLANGUAGE, the proposal will be the guideline for the entire design process so it\\nshould be clear and detailed. \\n'}]\n",
    "print(x[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code=test_tree.compose_unit('XAU')\n",
    "code=U.add_line_num(code)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class DilatedReformerRootGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.dilated_attention = DilatedAttentionGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.reversible_residual = ReversibleResidualGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **\n",
    "            self.factory_kwargs, **kwarg_all)\n",
    "        self.lsh_attention = LSHAttentionGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.dilated_attention(X, **Z)\n",
    "        X, Z = self.reversible_residual(X, **Z)\n",
    "        X, Z = self.lsh_attention(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class LSHAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class DilatedAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class ReversibleResidualGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "@gau_test\n",
    "def test_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau(device=\n",
    "    None, dtype=None) ->None:\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    embed_dim = 16\n",
    "    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "    gau = DilatedReformerRootGAU(embed_dim=embed_dim, block_loc=(0, 6), kwarg_all={}, device=\n",
    "        device, dtype=dtype)\n",
    "    Y, Z_ = gau(X, **Z)\n",
    "    assert Y.shape == X.shape, 'Output shape should match input shape'\n",
    "    assert isinstance(Z_, dict), 'Z_ should be a dictionary'\n",
    "    print('Input X:', X)\n",
    "    print('Output Y:', Y)\n",
    "    print('Intermediate variables Z_:', Z_)\n",
    "\n",
    "\n",
    "def run_DilatedReformerRootGAU_tests():\n",
    "\ttry:\n",
    "\t\ttest_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"Error in running unit_test_dilated_reformer_root_gau:\", e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\trun_DilatedReformerRootGAU_tests()\n",
    "'''\n",
    "\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "output=io.StringIO()\n",
    "with redirect_stdout(output):\n",
    "    exec(code)\n",
    "captured = str(output.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ResonanceLOCOSTGAMRoot(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class ResonanceLOCOSTGAMRoot(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.resonance_rope = ResonanceRoPEUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.state_space = StateSpaceUnit(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all)\n",
    "        self.inter_block_comm = InterBlockCommUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y, Z_ = self.resonance_rope(X, **Z)\n",
    "        Y, Z_ = self.state_space(Y, **Z_)\n",
    "        Y, Z_ = self.inter_block_comm(Y, **Z_)\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class ResonanceRoPEUnit(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.freqs = self._init_frequencies(embed_dim)\n",
    "\n",
    "    def _init_frequencies(self, embed_dim):\n",
    "        half_dim = embed_dim // 2\n",
    "        freqs = torch.exp(-torch.arange(half_dim, dtype=torch.float32,\n",
    "            device=self.factory_kwargs['device']) * (math.log(10000.0) / (\n",
    "            half_dim - 1)))\n",
    "        return freqs\n",
    "\n",
    "    def _apply_rope(self, X):\n",
    "        batch_size, seqlen, _ = X.size()\n",
    "        position_ids = torch.arange(seqlen, dtype=X.dtype, device=X.device)\n",
    "        sinusoid_inp = torch.einsum('i,j->ij', position_ids, self.freqs)\n",
    "        sin, cos = sinusoid_inp.sin(), sinusoid_inp.cos()\n",
    "        X1, X2 = X[..., ::2], X[..., 1::2]\n",
    "        Y1 = X1 * cos - X2 * sin\n",
    "        Y2 = X1 * sin + X2 * cos\n",
    "        return torch.stack([Y1, Y2], dim=-1).flatten(-2)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = self._apply_rope(X)\n",
    "        Z_ = Z\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "class StateSpaceUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class InterBlockCommUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "    \n",
    "\n",
    "\n",
    "gab_config = {}\n",
    "\n",
    "\n",
    "# Check if to device still work!\n",
    "\n",
    "gab = GAB(128, (0, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_device(model):\n",
    "    \"\"\"\n",
    "    Returns the device of the first parameter found in the given nn.Module.\n",
    "    If the module has no parameters, returns None.\n",
    "    \"\"\"\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "get_module_device(gab.root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gab.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children={\n",
    "    'unit1': 'MockUnit1',\n",
    "    'unit2': 'MockUnit2',\n",
    "}\n",
    "called=['unit1', 'unit2', 'unit1']\n",
    "test_tree.units['DualPathGAU'] = GAUNode('DualPathGAU', None, None, None, called, None, None, children, None)\n",
    "children={\n",
    "    'unit1': 'MockUnit3',\n",
    "    'unit2': 'MockUnit4',\n",
    "}\n",
    "called=['unit2', 'unit2']\n",
    "test_tree.units['MockUnit1'] = GAUNode('MockUnit1', None, None, None, called, None, None, children, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self,r,d,e,**Z):\n",
    "        print('Args in A:',d,e)\n",
    "        print('Kwargs in A:',Z)\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self,_Z,c=1,d=2,e=3,**Z):\n",
    "        args2={'d':d,'e':e}\n",
    "        super().__init__(c,**_Z,**args2)\n",
    "        print('Kwargs in B:',Z)\n",
    "\n",
    "cfg={\n",
    "    'a':1,\n",
    "    'b':2,\n",
    "    'c':3,\n",
    "}\n",
    "\n",
    "B(cfg,**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(X,Z,**kwargs):\n",
    "    print(X)\n",
    "    print(Z)\n",
    "    print(kwargs)\n",
    "\n",
    "def B(X,Z,a,**kwargs):\n",
    "    A(X,Z,**Z)\n",
    "\n",
    "Z={'a':3}\n",
    "B(1,Z,**Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "    '''\n",
    "    GAU Tree Map of Butterfly-AFT Generalized Autoregressive Block (BAGAB):\n",
    "    ButterflyAFTGAU (Rating: 3.5/5)\n",
    "        |- AFTMechanismGAU (Rating: 3.5/5)\n",
    "            |- PositionBiasGAU (Rating: 3.5/5)\n",
    "            |- ElementWiseOperationGAU (Rating: 3.5/5)\n",
    "        |- ButterflyTransformGAU (Rating: 3.5/5)\n",
    "            |- ButterflyLayerGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "            |- ButterflyInitializationGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "                |- ButterflyParameterGAU (Unimplemented)\n",
    "\n",
    "    Implemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ElementWiseOperationGAU, ButterflyAFTGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    Unimplemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ButterflyParameterGAU, ElementWiseOperationGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    '''\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ButterflyAFTGAU(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase\n",
    "\n",
    "\n",
    "class ButterflyAFTGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.aft_mechanism = AFTMechanismGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.butterfly_transform = ButterflyTransformGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        aft_output, Z = self.aft_mechanism(X, **Z)\n",
    "        butterfly_output, Z = self.butterfly_transform(aft_output, **Z)\n",
    "        return butterfly_output, Z\n",
    "\n",
    "\n",
    "class ButterflyTransformGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_layer = ButterflyLayerGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_init = ButterflyInitializationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.butterfly_init(X, **Z)\n",
    "        Y, Z = self.butterfly_layer(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ButterflyLayerGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.stages = nn.ModuleList([ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all) for _ in range(kwarg_all.get('num_stages', 4))])\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for stage in self.stages:\n",
    "            X, Z = stage(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "class ButterflyMatrixGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ButterflyInitializationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_size = kwarg_all.get('butterfly_size', embed_dim)\n",
    "        self.butterfly_matrices = nn.ParameterList([nn.Parameter(torch.\n",
    "            randn(self.butterfly_size, self.butterfly_size, **self.\n",
    "            factory_kwargs)) for _ in range(int(torch.log2(torch.tensor(\n",
    "            self.butterfly_size))))])\n",
    "        self.butterfly_stage = ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_parameter = ButterflyParameterGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for matrix in self.butterfly_matrices:\n",
    "            X = F.linear(X, matrix)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class ButterflyParameterGAU(GAUBase):\n",
    "    \"\"\"\n",
    "    Generalized Autoregressive Block Unit for learning parameters of Butterfly Factorization.\n",
    "    Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "    Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "    Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "    embed_dim:    The dimension of the input embeddings\n",
    "    block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "    kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.param_size = kwarg_all.get('param_size', 128)\n",
    "        self.butterfly_weights = nn.Parameter(torch.randn(self.param_size,\n",
    "            embed_dim, **self.factory_kwargs))\n",
    "        nn.init.xavier_uniform_(self.butterfly_weights)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X = torch.matmul(X, self.butterfly_weights)\n",
    "        Z_ = {'butterfly_weights': self.butterfly_weights}\n",
    "        return transformed_X, Z_\n",
    "        # transformed_X = F.linear(X, self.butterfly_weights.T)\n",
    "        # return transformed_X, {}\n",
    "\n",
    "\n",
    "class ButterflyComputationGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class ButterflyStageGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_params = ButterflyParameterGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X, Z = self.butterfly_params(X, **Z)\n",
    "        assert transformed_X.shape == X.shape, f'Output shape {transformed_X.shape} does not match input shape {X.shape}'\n",
    "        return transformed_X, Z\n",
    "\n",
    "class AFTMechanismGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = PositionBiasGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.element_wise_operation = ElementWiseOperationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.position_bias(X, **Z)\n",
    "        Y, Z = self.element_wise_operation(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ElementWiseOperationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, embed_dim, **self.\n",
    "            factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        queries = Z.get('queries', X)\n",
    "        keys = Z.get('keys', X)\n",
    "        values = Z.get('values', X)\n",
    "        elementwise_product = queries * keys\n",
    "        elementwise_sum = elementwise_product + values + self.position_bias\n",
    "        return elementwise_sum, {}\n",
    "\n",
    "\n",
    "class PositionBiasGAU(GAUBase):\n",
    "    \"\"\"Position Bias Generalized Autoregressive Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, 1, embed_dim, **\n",
    "            self.factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = X + self.position_bias\n",
    "        return Y, {}\n",
    "    \n",
    "\n",
    "class GABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100, **kwargs):\n",
    "        super().__init__()\n",
    "        self.gab = GAB(embed_dim, (0,1), **kwargs)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, Z = self.gab(X, **Z)\n",
    "        return self.lm_head(X),X\n",
    "\n",
    "\n",
    "\n",
    "gab_config = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100):\n",
    "        super().__init__()\n",
    "        self.butterfly_param = ButterflyParameterGAU(embed_dim, (0,1), {})\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, _ = self.butterfly_param(X, **Z)\n",
    "        return self.lm_head(X), X\n",
    "\n",
    "# Test the simplified model\n",
    "simple_model = GABLM(128)\n",
    "mock_input = torch.randn(1, 10, 128)\n",
    "mock_y = torch.randint(0, 100, (1, 10))\n",
    "\n",
    "optimizer = optim.Adam(simple_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for _ in range(5):  # Run for a few iterations\n",
    "    optimizer.zero_grad()\n",
    "    y, X = simple_model(mock_input)\n",
    "    loss = criterion(y.view(-1, 100), mock_y.view(-1))\n",
    "    loss.backward()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    for name, param in simple_model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"Gradient for {name}: {param.grad.abs().mean()}\")\n",
    "        else:\n",
    "            print(f\"No gradient for {name}\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim \n",
    "\n",
    "model=GABLM(128, **gab_config)\n",
    "vocab_size=100\n",
    "DEFAULT_CONTEXT_LENGTH=10\n",
    "\n",
    "mock_input=torch.randn(1, DEFAULT_CONTEXT_LENGTH, 128)\n",
    "mock_y=torch.randint(0, 100, (1, 10))\n",
    "mock_Z={}\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "y,X=model(mock_input, **mock_Z)\n",
    "loss=nn.CrossEntropyLoss()(y.view(-1, 100), mock_y.view(-1))\n",
    "loss.backward()\n",
    "\n",
    "used_params = set()\n",
    "\n",
    "# Hook to register which parameters are used during the forward pass\n",
    "def hook_fn(module, input, output):\n",
    "    for param in module.parameters():\n",
    "        used_params.add(param)\n",
    "\n",
    "# Register hooks for all submodules\n",
    "hooks = []\n",
    "for submodule in model.modules():\n",
    "    hook = submodule.register_forward_hook(hook_fn)\n",
    "    hooks.append(hook)\n",
    "\n",
    "# Forward pass again to collect used parameters\n",
    "model(mock_input)\n",
    "\n",
    "# Remove all hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "all_gradients_present = True\n",
    "\n",
    "# Check gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if param in used_params and param.grad is None:\n",
    "            # Used parameter without gradient -> Error\n",
    "            print(f\"Error: Used parameter {name} requires gradients but has none.\")\n",
    "            all_gradients_present = False\n",
    "        elif param not in used_params:\n",
    "            # Unused parameter -> Warning\n",
    "            print(f\"Warning: Parameter {name} was not used in the forward pass.\")\n",
    "\n",
    "if all_gradients_present:\n",
    "    print('Differentiability test passed')\n",
    "else:\n",
    "    print('Differentiability test failed due to missing gradients.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INIT_DESIGN_INPUT='''\n",
    "Below is the specification for the GAU you need to refine:\n",
    "Specification: Unit Name: GatedMLP The DynamicGatedMLP is an enhanced version of the GatedMLP that introduces a dynamic gating mechanism capable of selecting between multiple activation functions. This design aims to improve the expressiveness and adaptability of the model by allowing it to dynamically choose the most suitable activation function for different inputs. The unit maintains the same input-output interface as the original GatedMLP, ensuring seamless integration with the existing GPT-2 architecture.\n",
    "\n",
    "\n",
    "Inputs: X\n",
    "Outputs: Y\n",
    "Children list: []\n",
    "\n",
    "Current Implementation: import torch import torch.nn as nn from model_discovery.model.utils.modules import GAUBase, gau_test import torch.nn.functional as F\n",
    "\n",
    "class GatedMLP(GAUBase): \"\"\"Generalized Autoregressive Block Unit Input: X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of new intermediate variables to update the current Z} Constraints: Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "embed_dim:    The dimension of the input embeddings\n",
    "    block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "    kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "\"\"\"\n",
    "\n",
    "def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "    device=None, dtype=None, hidden_features=None, out_features=None,\n",
    "    bias=False, multiple_of=128, **kwargs):\n",
    "    self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "    out_features = out_features if out_features is not None else embed_dim\n",
    "    hidden_features = (hidden_features if hidden_features is not None else\n",
    "        int(8 * embed_dim / 3))\n",
    "    hidden_features = (hidden_features + multiple_of - 1\n",
    "        ) // multiple_of * multiple_of\n",
    "    self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n",
    "        self.factory_kwargs)\n",
    "    self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n",
    "        self.factory_kwargs)\n",
    "    self.activations = [F.relu, F.tanh, F.gelu]\n",
    "    self.gate_weights = nn.Parameter(torch.randn(len(self.activations),\n",
    "        hidden_features, **self.factory_kwargs))\n",
    "\n",
    "def _forward(self, X, **Z):\n",
    "    y = self.fc1(X)\n",
    "    y, gate = y.chunk(2, dim=-1)\n",
    "    gate_scores = F.softmax(self.gate_weights, dim=0)\n",
    "    y = sum(gate_scores[i] * self.activations[i](gate) for i in range(\n",
    "        len(self.activations)))\n",
    "    y = self.fc2(y)\n",
    "    return y, {}\n",
    "\n",
    "Review: ### Highlights:\n",
    "\n",
    "Successful Implementation Fix: The designer has successfully addressed the previous implementation error by initializing the activations attribute before its use, resolving the AttributeError.\n",
    "Dynamic Gating Mechanism: The design continues to leverage a dynamic gating mechanism, which is a novel approach that enhances the model's adaptability and expressiveness by selecting between multiple activation functions.\n",
    "Checker Validation: The updated implementation has passed all checker tests, including forward pass, causality, and differentiability, indicating that the GAU is functioning correctly within the model.\n",
    "Concerns:\n",
    "Efficiency Considerations: While the implementation is now functional, the potential computational overhead of evaluating multiple activation functions simultaneously remains a concern. This could impact efficiency, especially in larger models or datasets.\n",
    "Justification of Activation Functions: The choice of activation functions (ReLU, Tanh, GELU) is reasonable, but further theoretical justification or empirical validation would strengthen the design.\n",
    "Accuracy, Robustness, Efficiency, and Scalability:\n",
    "Accuracy and Robustness: The dynamic selection of activation functions should enhance the model's ability to adapt to various input patterns, potentially improving accuracy and robustness.\n",
    "Efficiency: The design could still benefit from optimizations to reduce computational overhead, ensuring it remains efficient in practice.\n",
    "Scalability: The design appears scalable, as indicated by the successful checker tests, but empirical validation on larger datasets would be beneficial.\n",
    "Novelty:\n",
    "The design introduces a novel dynamic gating mechanism, which is a significant improvement over static activation functions.\n",
    "Clarity and Completeness:\n",
    "The design is now clear and complete, with the previous implementation error resolved.\n",
    "Theoretical Soundness:\n",
    "The theoretical foundation of using dynamic gating is sound, and the implementation aligns well with the proposal's objectives.\n",
    "Implementation Feasibility:\n",
    "The current implementation is feasible and has been validated by the checker, indicating it can be integrated into the broader model.\n",
    "Suggestions:\n",
    "Efficiency Optimization: Consider exploring methods to optimize the dynamic gating mechanism to reduce computational overhead, such as selectively applying it or using more efficient computation techniques.\n",
    "Empirical Validation: Plan for empirical validation to confirm the theoretical benefits of the dynamic gating mechanism and the choice of activation functions in practice.\n",
    "Documentation: Ensure thorough documentation of the design choices and their theoretical justifications to aid future refinements and empirical testing.\n",
    "Rating: 4 out of 5 (Passing score >3)\n",
    "\n",
    "Reviewer Suggestions: 1. Efficiency Optimization: Consider exploring methods to optimize the dynamic gating mechanism to reduce computational overhead, such as selectively applying it or using more efficient computation techniques. 2. Empirical Validation: Plan for empirical validation to confirm the theoretical benefits of the dynamic gating mechanism and the choice of activation functions in practice.\n",
    "\n",
    "Documentation: Ensure thorough documentation of the design choices and their theoretical justifications to aid future refinements and empirical testing.\n",
    "Refinement Process\n",
    "If there is a review provided, you should start by reflecting on the feedback. Otherwise, leave reflection empty. The, proceed with the following:\n",
    "\n",
    "New Analysis and Design: - Provide an updated detailed analysis based on the feedback, including your new design direction and justifications. - Include a high-level pseudocode that captures the core of the new design. You should also provide the updated document of the GAU that allows other people to understand the design and implementation without the need to read the code.\n",
    "\n",
    "Implementation: - Provide the full updated implementation of the GAU, following the specified format and templates.\n",
    "\n",
    "Children list: - Provide the list of the children GAUs that are declared in the current GAU. You can declare new children GAUs or preserve the existing ones. If you do not declare any new children GAUs, you should provide the original children GAUs.\n",
    "\n",
    "Log of Changes: - Summarize the key changes you made during the refinement process. Including all code snippets where you made a change wrapped in python .\n",
    "\n",
    "Key Points to Remember:\n",
    "The bug or issue must always be resolved within the current GAU, as other units are either fully implemented and tested or placeholders that do not perform any computation.\n",
    "Ensure the GAU is self-contained, so you won't need to adjust it later when working on other units.\n",
    "The design must align with the original proposal and follow all instructions, templates, and format requirements.\n",
    "Use a top-down approach: break down complex operations into smaller tasks where necessary and declare each of them as a child GAU. Do not make a single unit overly complex.\n",
    "Remember your final goal is to refine the GAU in a way that enhances the overall design, ensuring both correctness and innovation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.prompts.prompts as P\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DESIGNER_SYSTEM=P.GUE_DESIGNER_SYSTEM(GAB_BASE=GAB_BASE,GAU_BASE=GAU_BASE,GAU_TEMPLATE=GAU_TEMPLATE)\n",
    "system=[{\"type\": \"text\", \"text\": DESIGNER_SYSTEM, \"cache_control\": {\"type\": \"ephemeral\"}}]\n",
    "\n",
    "# messages = [\n",
    "#     SystemMessage(system),\n",
    "#     HumanMessage(TEST_INIT_DESIGN_INPUT)\n",
    "# ]\n",
    "\n",
    "message=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": TEST_INIT_DESIGN_INPUT,\n",
    "            \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "\n",
    "SELECTIONS=['GPT2','RMSNorm','GatedMLP','MHA','RotaryPositionalEmbeddings']\n",
    "lc_model = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20240620\", temperature=0,\n",
    "    extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    ")\n",
    "structured_llm = lc_model.with_structured_output(P.gen_GUE_DESIGN_PROPOSAL(SELECTIONS).format,include_raw=True)\n",
    "tools_args=structured_llm.dict()['first']['steps__']['raw']['kwargs']\n",
    "        # tools[0]['cache_control']={\"type\": \"ephemeral\"}\n",
    "\n",
    "RET=anthropic.Anthropic().messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\", # model in config is ignored\n",
    "    max_tokens=8192,\n",
    "    messages=message, \n",
    "    temperature=0,\n",
    "    system=system, # claude does not has system role, system prompt must be passed separately\n",
    "    extra_headers=extra_headers,\n",
    "    **tools_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RET.dict()['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.gen_GUE_DESIGN_PROPOSAL(SELECTIONS).format.model_validate(RET.content[0].input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RET.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm.dict()['first']['steps__']['raw']['kwargs']['tools'][0]['input_schema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Message(id='msg_01Nvch8XBwzwrrMBETHpch49', content=[TextBlock(text=\"Thank you for providing the seed design and references. I'll now propose a design improvement for the GPT2 model, focusing on modifying one of its GAUs.\", type='text'), ToolUseBlock(id='toolu_01PpbFTQ6NDwJJCfj1cRDvFS', input={'selection': 'RotaryPositionalEmbeddings', 'modelname': 'GPT2-RoPE', 'proposal': \"\\n# GPT2 with Enhanced Rotary Position Embeddings (GPT2-RoPE)\\n\\n## Motivation\\n\\nThe current GPT2 model uses a basic implementation of Rotary Positional Embeddings (RoPE). While this is already an improvement over traditional positional encodings, there's room for enhancement, particularly in handling longer sequences and improving the model's ability to capture relative positions more effectively.\\n\\n## Problem Analysis\\n\\n1. **Limited sequence length handling**: The current implementation may not scale well to very long sequences, which is increasingly important in modern language models.\\n2. **Lack of flexibility**: The existing RoPE implementation doesn't allow for easy adjustment of its properties, such as the rate of position information decay.\\n3. **Potential for improved performance**: While RoPE is effective, recent research suggests that more sophisticated implementations can lead to better model performance, especially in tasks requiring understanding of long-range dependencies.\\n\\n## Core Idea and Philosophy\\n\\nWe propose to enhance the RotaryPositionalEmbeddings GAU by incorporating ideas from the RoFormer paper and the more recent developments in rotary embeddings. The core philosophy is to create a more flexible and powerful positional embedding system that can:\\n\\n1. Handle longer sequences more effectively\\n2. Allow for fine-tuning of the position encoding properties\\n3. Potentially improve the model's performance on tasks requiring long-range understanding\\n\\n## Design Plan\\n\\nWe will modify the RotaryPositionalEmbeddings GAU as follows:\\n\\n1. **Implement a more sophisticated RoPE class**: Based on the RoFormer paper and recent developments, we'll create a new RotaryEmbedding class that offers more flexibility and features.\\n\\n2. **Add support for learned frequencies**: This will allow the model to potentially learn better position representations for specific tasks or domains.\\n\\n3. **Implement interpolation factor**: This will enable the model to handle sequences longer than those seen during training more effectively.\\n\\n4. **Add support for XPos (Extrapolatable Position Embeddings)**: This will further enhance the model's ability to handle longer sequences and improve its understanding of relative positions.\\n\\n5. **Implement caching mechanism**: To improve efficiency, especially for autoregressive generation tasks.\\n\\n6. **Add support for different frequency types**: This will allow the embeddings to be optimized for different types of data (e.g., language, images).\\n\\nHere's a high-level overview of the proposed changes:\\n\\n```python\\nclass RotaryEmbedding(nn.Module):\\n    def __init__(\\n        self,\\n        dim,\\n        custom_freqs = None,\\n        freqs_for = 'lang',\\n        theta = 10000,\\n        max_freq = 10,\\n        num_freqs = 1,\\n        learned_freq = False,\\n        use_xpos = False,\\n        xpos_scale_base = 512,\\n        interpolate_factor = 1.,\\n        theta_rescale_factor = 1.,\\n        seq_before_head_dim = False,\\n        cache_if_possible = True\\n    ):\\n        super().__init__()\\n        # Implementation details...\\n\\n    def rotate_queries_or_keys(self, t, seq_dim = None, offset = 0, scale = None):\\n        # Implementation details...\\n\\n    def rotate_queries_and_keys(self, q, k, seq_dim = None):\\n        # Implementation details...\\n\\n    def get_scale(self, t, seq_len = None, offset = 0):\\n        # Implementation details...\\n\\n    def forward(self, t, seq_len = None, offset = 0):\\n        # Implementation details...\\n```\\n\\nThis new implementation will replace the current RotaryPositionalEmbeddings GAU in the GPT2 model.\\n\\n## Conclusion\\n\\nBy enhancing the RotaryPositionalEmbeddings GAU with these improvements, we expect to see several benefits:\\n\\n1. **Improved handling of long sequences**: The model should be able to maintain performance even on sequences longer than those seen during training.\\n2. **Better capture of relative positions**: The enhanced RoPE implementation should allow the model to better understand and utilize relative position information.\\n3. **Increased flexibility**: The new implementation allows for fine-tuning of various aspects of the positional embeddings, which can be optimized for specific tasks or domains.\\n4. **Potential performance improvements**: These enhancements may lead to improved performance on tasks requiring long-range understanding and more nuanced use of positional information.\\n\\nThis modification maintains the overall structure of the GPT2 model while significantly enhancing its ability to handle and utilize positional information, potentially leading to improved performance across a wide range of natural language processing tasks.\\n\\n## References\\n\\n1. Su, J., Lu, Y., Pan, S., Wen, B., & Liu, Y. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv preprint arXiv:2104.09864.\\n2. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint arXiv:2401.04088.\\n\"}, name='GUE_DESIGN_PROPOSAL_format', type='tool_use')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='tool_use', stop_sequence=None, type='message', usage=Usage(input_tokens=16630, output_tokens=1325))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content='Reflection on Feedback:\\nThe reviewer\\'s suggestions highlight the need for efficiency optimization, empirical validation, and thorough documentation. I\\'ll address these points in the refined design.\\n\\nNew Analysis and Design:\\n\\nThe current GatedMLP design introduces a dynamic gating mechanism that allows the model to select between multiple activation functions. While this approach enhances adaptability, it may introduce computational overhead. To address this, we\\'ll implement a more efficient gating mechanism that maintains the benefits of dynamic activation selection while reducing computational costs.\\n\\nUpdated Design Direction:\\n1. Efficient Gating: Instead of applying all activation functions and then combining them, we\\'ll use a lightweight gating network to predict weights for each activation function. This approach reduces the number of computations while still allowing dynamic selection.\\n\\n2. Sparse Activation: We\\'ll introduce a sparsity factor in the gating mechanism, allowing the model to focus on the most relevant activation functions for each input.\\n\\n3. Adaptive Computation: We\\'ll implement a mechanism to dynamically adjust the number of active gates based on the input complexity, further optimizing computational resources.\\n\\nHigh-level Pseudocode:\\n```\\nfunction GatedMLP(X):\\n    hidden = linear_projection(X)\\n    gate_weights = lightweight_gate_network(hidden)\\n    sparse_gate_weights = apply_sparsity(gate_weights)\\n    activated = sum(sparse_gate_weights[i] * activation[i](hidden) for i in range(num_activations))\\n    output = linear_projection(activated)\\n    return output\\n```\\n\\nUpdated GAU Document:\\nThe GatedMLP unit now incorporates an efficient dynamic gating mechanism that selects between multiple activation functions. Key features include:\\n- Lightweight gating network for predicting activation weights\\n- Sparse activation selection to focus on the most relevant functions\\n- Adaptive computation based on input complexity\\n- Maintains the same input-output interface as the original GatedMLP\\n\\nThis design aims to balance expressiveness and computational efficiency, allowing the model to adapt to various input patterns while managing resource usage effectively.\\n\\nImplementation:\\n\\n```python\\n# gau.py\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test\\n\\nclass GatedMLP(GAUBase):\\n    \"\"\"\\n    Generalized Autoregressive Block Unit with efficient dynamic gating\\n    Input:  X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \\n    Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of new intermediate variables to update the current Z}\\n    \"\"\"\\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\\n                 device=None, dtype=None, hidden_features=None, out_features=None,\\n                 bias=False, multiple_of=128, num_activations=3, sparsity_factor=0.5, **kwargs):\\n        self.factory_kwargs = {\\'device\\': device, \\'dtype\\': dtype}\\n        super().__init__(embed_dim, block_loc, kwarg_all)\\n        \\n        out_features = out_features if out_features is not None else embed_dim\\n        hidden_features = hidden_features if hidden_features is not None else int(8 * embed_dim / 3)\\n        hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of\\n        \\n        self.fc1 = nn.Linear(embed_dim, hidden_features, bias=bias, **self.factory_kwargs)\\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **self.factory_kwargs)\\n        \\n        self.gate_network = nn.Sequential(\\n            nn.Linear(hidden_features, 64, bias=False, **self.factory_kwargs),\\n            nn.ReLU(),\\n            nn.Linear(64, num_activations, bias=False, **self.factory_kwargs)\\n        )\\n        \\n        self.activations = [F.relu, F.tanh, F.gelu]\\n        self.sparsity_factor = sparsity_factor\\n\\n    def _forward(self, X, **Z):\\n        hidden = self.fc1(X)\\n        gate_weights = self.gate_network(hidden)\\n        \\n        # Apply softmax and sp'\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from core library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import importlib\n",
    "import model_discovery.model.composer \n",
    "# importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUBase, GAUTree, check_tree_name, GABComposer\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "import model_discovery.utils as U\n",
    "\n",
    "# dir='/home/junyanc/model_discovery/model_discovery/model/library/core/gpt2/units'\n",
    "dir='C://ChengJunyan1//Research//model_discovery//model_discovery//model//library//core//gpt2//units'\n",
    "tree=GAUTree.load_from_base(dir)\n",
    "\n",
    "print(tree.view()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "gam_prompt_path = 'C://ChengJunyan1//Research//model_discovery//model_discovery//agents//prompts//gam_prompt.py'\n",
    "gau_template_path = 'C://ChengJunyan1//Research//model_discovery//model_discovery//agents//prompts//gau_template.py'\n",
    "GAM_TEMPLATE=open(gam_prompt_path).read()\n",
    "GAU_TEMPLATE=open(gau_template_path).read()\n",
    "\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n",
    "GAB_BASE=inspect.getsource(GABBase)\n",
    "GAB_COMPOSER=inspect.getsource(GABComposer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GAM_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:19:13:54,888 INFO     [model_agent.py:184] Agent name=`designer`, model_details={\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"temperature\": 0.3\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized dataset wikitext-2 from C:\\ChengJunyan1\\Research\\model_discovery\\data\\wikitext-2\\tokenized\\meta-llama/Llama-2-7b-hf\\2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:19:13:56,847 INFO     [model_agent.py:184] Agent name=`debugger`, model_details={\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-12:19:13:57,209 INFO     [model_agent.py:184] Agent name=`claude`, model_details={\n",
      "    \"model_name\": \"gpt-4o\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evoname': 'test_evo_000', 'scales': '14M,31M,70M', 'selection_ratio': '0.25', 'select_method': 'random', 'design_budget': '0'}\n",
      "Evolution system initialized with scales: ['14M', '31M', '70M']\n",
      "Current scale: 0\n",
      "Budgets remaining: {'70M': 1, '31M': 4, '14M': 16}\n",
      "Checkpoint directory: C:\\ChengJunyan1\\Research\\model_discovery\\ckpt\\test_evo_000\n",
      "gpt2 tree loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:19:13:58,936 INFO     [model_agent.py:184] Agent name=`designer`, model_details={\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"temperature\": 0.3\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized dataset wikitext-2 from C:\\ChengJunyan1\\Research\\model_discovery\\data\\wikitext-2\\tokenized\\meta-llama/Llama-2-7b-hf\\2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:19:13:59,744 INFO     [model_agent.py:184] Agent name=`debugger`, model_details={\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-12:19:14:00,132 INFO     [model_agent.py:184] Agent name=`claude`, model_details={\n",
      "    \"model_name\": \"gpt-4o\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "import importlib\n",
    "import arxiv\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.utils as U\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "headers={\"X-API-KEY\": os.environ['S2_API_KEY']}\n",
    "paper_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}'\n",
    "\n",
    "\n",
    "strparams=[\n",
    "    f\"evoname=test_evo_000\",\n",
    "    \"scales=14M,31M,70M\",\n",
    "    \"selection_ratio=0.25\",\n",
    "    \"select_method=random\",\n",
    "    \"design_budget=0\",\n",
    "]\n",
    "evo_system = BuildEvolution(\n",
    "    strparams=';'.join(strparams),\n",
    "    do_cache=False,\n",
    "    # cache_type='diskcache',\n",
    ")\n",
    "\n",
    "ptree = evo_system.ptree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:19:14:02,041 INFO     [_client.py:1038] HTTP Request: GET https://api.openai.com/v1/vector_stores/vs_a2L6ajjzEki2N1krUFz3JZxz \"HTTP/1.1 200 OK\"\n",
      "2024-09-12:19:14:02,213 INFO     [_client.py:1038] HTTP Request: GET https://api.openai.com/v1/vector_stores/vs_BDqAPFMyIPSrZkk657zgNiGY \"HTTP/1.1 200 OK\"\n",
      "2024-09-12:19:14:02,409 INFO     [_client.py:1038] HTTP Request: GET https://api.openai.com/v1/vector_stores/vs_1lc8Tgkl6FTwOzmQ9TAK3ZsO \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.search_utils\n",
    "importlib.reload(model_discovery.agents.search_utils)\n",
    "\n",
    "from model_discovery.agents.search_utils import SuperScholarSearcher, pwc_search_patched\n",
    "\n",
    "sss=SuperScholarSearcher(ptree,evo_system.stream)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sss._load_libs()\n",
    "# sss._load_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "\n",
    "co = cohere.Client(os.environ['COHERE_API_KEY'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:22:04:18,912 INFO     [_client.py:1038] HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='e8633431-293e-405d-82f3-4b47d524a89f' results=[RerankResponseResultsItem(document=None, index=5, relevance_score=0.001012286), RerankResponseResultsItem(document=None, index=12, relevance_score=0.000628784), RerankResponseResultsItem(document=None, index=14, relevance_score=0.00040448149)] meta=ApiMeta(api_version=ApiMetaApiVersion(version='1', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(input_tokens=None, output_tokens=None, search_units=1.0, classifications=None), tokens=None, warnings=None)\n"
     ]
    }
   ],
   "source": [
    "_docs=[i.page_content for i in docs]\n",
    "\n",
    "response = co.rerank(\n",
    "    model=\"rerank-english-v3.0\",\n",
    "    query=\"What is the capital of the United States?\",\n",
    "    documents=_docs,\n",
    "    top_n=3,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[i.index for i in response.results]\n",
    "relevance_scores=[i.relevance_score for i in response.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43m_docs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "_docs[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:20:48:52,900 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "e=embedding.embed_documents([docs[0].page_content,docs[1].page_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12:20:03:49,202 INFO     [_client.py:1038] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "The context size is $L$, and the chunk size is $c$. The vectors $\\left[\\boldsymbol{h}_{j, m}^{1}, \\boldsymbol{h}_{j, m}^{2}\\right]^{T}$ and $\\left[-\\boldsymbol{h}_{j, m}^{2}, \\boldsymbol{h}_{j, m}^{1}\\right]^{T}$ form an orthogonal basis, corresponding to the $|1\\rangle$ and $|0\\rangle$ states in Eq. 11. The components $\\boldsymbol{h}_{j, m}^{1}$ and $\\boldsymbol{h}_{j, m}^{2}$ represent the first and second dimensions of the state vector $\\boldsymbol{h}_{j, m}$, which is the $m_{t h}$ token in the $j_{t h}$ chunk. i is the imaginary unit. $\\boldsymbol{h}_{j, m}^{\\perp}$ equals to $\\left[-\\boldsymbol{h}_{j, m}^{2}, \\boldsymbol{h}_{j, m}^{1}\\right]^{T}$, where $\\boldsymbol{h}_{j, m}^{1} \\in \\mathbb{R}^{d / 2}$ and $\\boldsymbol{h}_{j, m}^{2} \\in \\mathbb{R}^{d / 2}$ is the first and second halves of the state vector $\\boldsymbol{h}_{j, m}$. In transformer-based LLMs, after applying position encoding to the state vectors from Query and Key, it is essential to compute their attention scores. For the sake of clarity and formalization, we denote the position encoding of the state vector from Query as 3d-PE $(\\boldsymbol{q}, i, m)$ and from Key as $3 \\mathrm{~d}-\\mathrm{PE}(\\boldsymbol{k}, j, n)$, where $i$ and $j$ range from 0 to $\\lceil L / c\\rceil-1$, and $m$ and $n$ range from 0 to $c-1$. The self-attention score can be obtained through the conjugate symmetric inner product of $\\boldsymbol{q}_{i, m}$ and $\\boldsymbol{k}_{j, n}$, which are the state vectors from Query and Key,\n",
      "\n",
      "$$\n",
      "s\\left(\\boldsymbol{q}_{i, m}, \\boldsymbol{k}_{j, n}, \\varphi_{i}-\\varphi_{j}, m-n\\right)=\\operatorname{Re}\\left[e^{\\mathrm{i}\\left(\\varphi_{i}-\\varphi_{j}\\right)} \\sum_{l=0}^{d / 2-1} e^{\\mathrm{i}(m-n) \\theta_{l}}\\left(\\boldsymbol{q}_{l} \\boldsymbol{k}_{l}+\\boldsymbol{q}_{d / 2+l} \\boldsymbol{k}_{d / 2+l}\\right)\\right]\n",
      "$$\n",
      "\n",
      "where $l \\in\\left[0, \\frac{d}{2}-1\\right], \\varphi_{i}=b a s e^{-i}$ and $\\varphi_{j}=$ base ${ }^{-j}$. Let $\\{\\boldsymbol{q}, \\boldsymbol{k}\\}_{l}$ denote the $l$-th components of $\\{\\boldsymbol{q}, \\boldsymbol{k}\\}$. In experiments using the LLaMA2 models, the base is generally set to 10, 000. The self-attention score computed after applying 3d-PE is a function of both the relative position between chunks $\\left(\\varphi_{i}-\\varphi_{j}\\right)$ and the relative position $(m-n)$. Consequently, the self-attention score relying on 3d-PE is influenced by the relative positions at both the chunk and token levels. It is important to highlight that when $\\boldsymbol{q}_{i, m}$ and $\\boldsymbol{k}_{j, n}$ reside within the same chunk (i.e., $i=j$ ), Eq. (5) simplifies to the standard RoPE formulation as depicted in Eq. (3). For a detailed derivation and computation process of Eq. (5), as well as the complete formulation of Eq. 4, please refer to Supplementary Materials B.2. ### 3.2 Benefits of 3D-RPE\n",
      "\n",
      "In this section, we delve into two benefits offered by 3D-RPE: the ability to control long-term decay and mitigate the reduction in positional resolution caused by position interpolation. ![](https://cdn.mathpix.com/cropped/2024_09_12_d426bc005c38a790628bg-05.jpg?height=709&width=858&top_left_y=296&top_left_x=517)\n",
      "\n",
      "Figure 3: Visualization of the Relative Position Matrix $\\boldsymbol{A}$ employing 3D-RPE, with chunk size $c=4$, and sequence size $L=12$. The matrix elements $A_{i, j}$ represents the relative position between the $i_{t h}$ query vector $\\boldsymbol{q}$ and the $j_{t h}$ key vector $\\boldsymbol{k}$. ### 3.2.1 Controllable Long-term Decay\n",
      "\n",
      "3D-RPE has the property of controllable long-term decay. Like RoPE, taking the absolute value $s$ in Eq (5) and applying the Abel transformation, we derive the upper bound of the correlation coefficients related to term dependencies as follows:\n",
      "\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\left|s\\left(\\boldsymbol{q}_{i, m}, \\boldsymbol{k}_{j, n}, \\varphi_{i}-\\varphi_{j}, m-n\\right)\\right| & \\leq\\left|e^{\\mathrm{i}\\left(\\varphi_{i}-\\varphi_{j}\\right)} \\| \\sum_{l=0}^{d / 2-1} E_{l+1}\\left(h_{l+1}-h_{l}\\right)\\right| \\\\\n",
      "& \\leq\\left(\\max _{l}\\left|h_{l+1}-h_{l}\\right|\\right) \\sum_{l=0}^{d / 2-1}\\left|E_{l+1}\\right|\n",
      "\\end{aligned}\n",
      "$$\n",
      "\n",
      "where $E_{l}=\\sum_{k=0}^{l-1} e^{\\mathrm{i}(m-n) \\theta_{k}}$ and $E_{0}=0$. For RoPE [23], the relative upper bound $E_{\\text {rope }}$ is given by $\\frac{1}{d / 2} \\sum_{j=1}^{d / 2}\\left|S_{j}\\right|$, where $S_{j}=\\sum_{t=0}^{j-1} e^{i(m-n) \\theta_{t}}$ (see the section 3.4.3 of RoPE [23]). By setting $\\theta_{t}=10000^{\\frac{-2 t}{d}}$, the value decays as the relative position $(m-n)$ increases. For the upper bound $E_{3 d-r p e}$ of 3D-RPE, it is formalized as follows:\n",
      "\n",
      "$$\n",
      "E_{3 d-r p e}=\\frac{1}{d / 2} \\sum_{j=1}^{d / 2}\\left|E_{l}\\right|\n",
      "$$\n",
      "\n",
      "The domains of the relative position $(m-n)$ differ between $E_{3 d-r p e}$ and $E_{\\text {rope }}$. In $E_{\\text {rope }},(m-n)$ is in the range $[0, L-1]$, while in $E_{3 d-r p e}$, it is in $[0, c-1]$. The relative positions between tokens exceeding the chunk size $c$ are constructed collaboratively using positional encoding within and across chunks. The Relative Position Matrix $\\boldsymbol{A}$ using 3D-RPE is shown in Figure 3. To illustrate the advantage of controllable long-term decay, we present the results in Figure 1(c) and Figure 1/d). As shown in Figure 1 (c), when the relative position $(m-n)$ exceeds approximately 1000, $E_{\\text {rope }}$ begins to significantly decrease to below 5 . This limitation of $E_{\\text {rope }} \\leq 5$ poses challenges for RoPE in modeling attention scores between tokens with longer relative distances (greater than 4000). In contrast, as shown in Figure 11d), 3D-RPE employs both $(m-n)$ and $\\left(\\varphi_{i}-\\varphi_{j}\\right)$, setting $c=1000$ to keep $(m-n)$ within 1000, thereby preventing decay over longer distances. This method ensures $E_{3 d-r p e}$ stays at or above 5 for all relative positions. ### 3.2.2 Enhanced Positional Resolution\n",
      "\n",
      "Position Interpolation (PI) [4] has been introduced to scale down the position indices to align with the original window size, resulting in enhanced outcomes for context extension. However, as the extension length and interpolation increase, PI can lead to a reduction in relative positional resolution. 3D-RPE can be used alongside PI for long-context extensions. Compared to RoPE combined with PI, 3D-RPE has the advantage of mitigating the reduction in positional resolution caused by positional interpolation, as demonstrated in Theorem 1 . Theorem 1 (Enhanced Position Resolution). For a pre-trained language model with a length of $L_{p}$ and an extension length requirement of L, employing linear position interpolation extension methods $\\mathcal{I}$ based on Rotary Position Encoding (RoPE) can elevate the relative positional resolution from $\\mathcal{E}_{\\text {rope }}$ to $\\mathcal{E}_{\\text {rope }}^{\\prime}$. Let $\\mathcal{E}_{3 d-r p e}^{\\prime}$ denote the relative positional encoding resolution achieved by the method $\\mathcal{I}$ based on $3 D-R P E$, with chunk size $c \\geq 3$, there is:\n",
      "\n",
      "$$\n",
      "\\mathcal{E}_{3 d-r p e}^{\\prime}>\\mathcal{E}_{\\text {rope }}^{\\prime}\n",
      "$$\n",
      "\n",
      "The Proof of Theorem 1 is provided in Supplementary Materials C. To empirically validate the superior performance of this benefit in a training-free setting, it has been observed that methods combining RoPE with interpolation lead to a significant increase in Perplexity as the modeling length increases in language modeling tasks. Conversely, the increase in Perplexity is substantially smaller when employing 3D-RPE with linear interpolation (Refer to Table 4 in Section 5). This phenomenon indicates that this benefit has led to an improvement in the performance of long sequence language modeling. ## 4 Related Work\n",
      "\n",
      "This section provides an overview of the extensive literature on position encoding in Transformers [26] and discusses context extending capabilities based on RoPE. Position Encoding (PE): PE is important for Transformer-based language models. Earlier studies [22, 21, 28, 23] have focused on enhancing the original absolute position encoding to develop better relative position encoding, thereby improving the text modeling capabilities of language models. These works [22, 21, 28] utilized trainable position vector encoding to directly incorporate positional information into context representations. Although effective, these methods typically add positional information to contextual representations, making them unsuitable for linear self-attention architectures. RoFormer [23] introduced relative position information by rotating context representations, known as RoPE. Transformers utilizing RoPE have become a prevalent backbone in various LLM designs [24, 8, 27, 16]. Our proposed 3D-RPE differs from the two-dimensional space of RoPE by modeling the relative position information of tokens through rotation on the Bloch Sphere. Long-context LLMs based on RoPE: To enhance the contextual capabilities of Large Language Models (LLMs) using RoPE, several positional encoding interpolation techniques have been developed. These include Linear Position Interpolation (LPI) [4], Neural Tangent Kernel (NTK) [17], and Yet Another Recurrent Network (YaRN) [18] interpolation. Position Sequence Tuning (PoSE) [31] has notably increased sequence lengths to $128 k$ by amalgamating these positional interpolation strategies. Additionally, LongLora [5] introduced the shift-short attention mechanism, allowing for effective emulation of full attention and extending sequences up to $100 k$, leveraging the LLMa-2-7B model and LoRA's fine-tuning approach [12]. 3D-RPE further strengthens the positional relationships between distant tokens by capturing inter-chunk positional information and is compatible with existing fine-tuning techniques like LoRA to bolster long-context representation. The Dual Chunk Attention (DCA) [2] method, which enhances the use of pre-trained integer-based parameters, splits query and key sequences into chunks and uses three specialized matrices to capture the relative positions within and between these chunks. This method enhances the model's ability to process longer sequences, but it is unable to model the relative positions within distant chunks. In our work, we employ rotating positional encoding to link attention across different chunks. ## 5 Experiments\n",
      "\n",
      "We evaluate the method of position encoding, 3D-RPE, on LLaMA2 [24] models (specifically, LLaMA-2-7B and LLaMA-2-7B-chat), which have a $4 k$ pre-training context, and LLaMA-3-8B-\n",
      "\n",
      "Instruct ${ }^{2}$, which has an $8 k$ pre-training context. Our experiments aim to explore the following aspects: 1) The effect of 3D-RPE on long-context generation can be assessed using Perplexity. 2) The impact of 3D-RPE on long-context understanding and generation tasks, can be reflected by the accuracy of long sequence natural language tasks, e.g., multiply documents QA. 3) Ablation studies to confirm the advantages of 3D-RPE in position interpolation. ### 5.1 Experimental Settings\n",
      "\n",
      "In this section, we elaborate on the experimental setup by introducing two types of tasks (i.e., longcontext language understanding and long sequence language modeling) and detailing three aspects of the configuration (i.e., training parameters, training and evaluation datasets, and baseline models). Training Setting: For long-context Natural Language Understanding (NLU) tasks, we have finetuned LLaMA-2-7B-chat and LLaMA-3-8B-Instruct. The context length for these models has been extended from $4 k$ to $16 k$ and from $8 k$ to $16 k$, respectively. The fine-tuning method follows the fine-tuning strategy of LongChat [13]. The training step is 3, 000. For the long-sequence Language Modeling (LM) tasks, we have fine-tuned LLaMA-2-7B to support extended context length of $32 k$ tokens. The training step is 1,000 . We set the per-device batch size as 1 , and gradient accumulation step as 8 , which means that the batch size is 8 . We train the model with the next token prediction objective with LoRA [12]. We employed the AdamW optimizer [15] with $\\beta_{1}=0.9$ and $\\beta_{2}=0.95$ for all fine-tuned models. Chunk size is set to $3 k$. The learning rate was set to $2 \\times 10^{-5}$, and a linear learning rate warmup was applied. Training was conducted on a single 4xA800 GPU machine using FlashAttention-2 [10]. Datasets: In the context of long-context NLU tasks, we employ the LongAlpaca-12k dataset, which contains 9,000 LongQA and 3,000 short QA entries [6], and the LongAlpace-16k-length datase ${ }^{3}$ To evaluate the performance of 3D-RPE for long-context extension, we use the LongBench [3], which includes 13 English tasks, 5 Chinese tasks and 2 code tasks, with most tasks having an average context length of $5 k$ to $15 k$ tokens. We focus on the English and code tasks to evaluate our method, 3D-RPE. Additionally, the LEval [1] evaluation set, which also consists of long-context datasets, is used to verify the effectiveness of 3D-RPE. The five datasets annotated from scratch in LEval, namely Coursera, QuALiTY, CodeU, GSM,and TOEFL, are utilized. For long-sequence LM tasks, we use the RedPajama-Data [9] for fine-tuning training. The dataset is a large-scale pre-training dataset (the size reaches 1.2 trillion tokens) designed to provide high-quality training data for language models, and contains multiple data sources (i.e., github, arxiv, book, c4 and Wikipedia, etc.). We sample 20,000 samples from these data sources for training. For evaluation, we utilize the PG19 book corpus dataset [20], which includes 100 documents, and the Arxiv Math Proof-pile dataset (test split). Additionally, all methods evaluate perplexity by using a sliding window following [19]. Baseline models: For long-context NLU tasks, the fine-tuned models, including LongAlpace-16k [5], LongChat-32k [14] LongLlama [25] and ChatGLM [11] are used as the baseline models. Models of fine-tuning free in language modeling tasks are also used in long-context NLU tasks. In long sequence LM tasks, the methods of LongLoRA [5], StreamingLLM [29], Positional Interpolation(PI) [4] and the NTK-Aware Scale RoPE(NTK) [17] are selected as the baselines, all based on the LLaMA-2-7B-base model. Among these baseline models, PI, NTK and StreamingLLM are fine-tuning-free methods. The fine-tuned models include LongLoRA and Activation Beacon [30]. In Ablation experiments, interpolation methods without training are used as baseline models, which are PI and NTK. ### 5.2 Long-Context Natural Language Understanding\n",
      "\n",
      "In this task, the LongBench [3] evaluation set was initially utilized. Five categories of tasks were included: single-document QA (3 tasks), multi-document QA (3 tasks), summarization (3 tasks), few-shot learning ( 3 tasks), and code completion ( 2 tasks). The average score for each type is reported in Table 1]. The evaluation metrics followed those specified in LongBench [3], which differ across\n",
      "\n",
      "[^1]Table 1: Comparison between open-source based models on long-context NLU tasks. Our model, 3D-RPE-LlaMA2-7B-Chat is fine-tuning on LLaMA-2-7b-chat, which is extended from $4 k$ to $16 k$ context lengths. Baseline models can be categorized into two groups: those that necessitate finetuning during training (such as LongAlpaca [5] and LongLLaMA [25]), and those that do not require it (including PI, NTK, StreamingLLM [29], and ChunkLLaMA-16k [2]). The experimental results for each specific evaluation set in Supplementary Material D.2. | METHODS | Single-Doc QA | Multi-Doc QA | Summarization | Few-shot | Code |\n",
      "| :--- | :---: | :---: | :---: | :---: | :---: |\n",
      "| LLaMA-2-7B-chat | 24.90 | 22.60 | 24.70 | 60.01 | 48.10 |\n",
      "| LLaMA-2-7B-chat-PI | 18.98 | 17.16 | 25.03 | 49.43 | 52.73 |\n",
      "| LLaMA-2-7B-chat-NTK | 23.21 | 23.34 | 24.40 | 59.29 | 49.28 |\n",
      "| StreamingLLM | 21.47 | 22.22 | 22.20 | 50.05 | 48.00 |\n",
      "| ChunkLLaMA-16 $k$ | 24.04 | 22.98 | 21.52 | 46.31 | 49.73 |\n",
      "| LongChat-32 $k$ | 31.58 | 23.50 | 26.70 | 64.02 | 54.10 |\n",
      "| LongAlpaca-16 $k$ | 28.70 | 28.10 | 27.80 | 63.70 | 56.00 |\n",
      "| LongLLaMA | 30.12 | 16.37 | 24.19 | 60.31 | 66.05 |\n",
      "| Vicuna-v1.5-7B-16 $k$ | 28.01 | 18.63 | 26.01 | 66.20 | 47.30 |\n",
      "| ChatGLM3-6B-32 $k$ | 40.30 | 46.60 | $\\mathbf{2 9 . 5 0}$ | 68.10 | 56.20 |\n",
      "| 3D-RPE-LLaMA2-7B-Chat | $\\mathbf{4 7 . 4 0}$ | $\\mathbf{6 0 . 1 0}$ | 28.99 | $\\mathbf{7 3 . 1 6}$ | $\\mathbf{7 6 . 5 0}$ |\n",
      "\n",
      "Table 2: Comparison with open-source models, LLaMA-2-7B-chat, LLaMA3-8B-Instruct, on 5 closed-ended-ended tasks with various input length from LEval [1]. The evaluation metric \"EM,\" which represents the exact match score, is adopted. $*$ indicates the model is train-free. | MODELS | Coursera | QuALiTY | CodeU | GSM | TOEFL |\n",
      "| :--- | :---: | :---: | :---: | :---: | :---: |\n",
      "| LLaMA-2-7B-Chat | 29.21 | 37.62 | 1.11 | 19.00 | 51.67 |\n",
      "| LongChat-7B-16K | 29.74 | 33.66 | 3.33 | 10.00 | 47.95 |\n",
      "| LLaMA2-7B-NTK | 32.71 | 33.16 | 0.00 | 19.00 | 52.78 |\n",
      "| Vicuna1.5-7B-16k | 38.66 | $\\mathbf{3 9 . 6 0}$ | $\\mathbf{5 . 5 5}$ | 19.00 | 55.39 |\n",
      "| 3D-RPE-LLaMA2-7B-Chat(ours) | $\\mathbf{3 9 . 3 8}$ | 38.11 | 2.22 | $\\mathbf{2 1 . 0 1}$ | $\\mathbf{5 7 . 9 9}$ |\n",
      "| LLaMA3-8B-Instruct* | 51.45 | $\\mathbf{6 4 . 3 4}$ | 4.44 | 76.00 | 82.89 |\n",
      "| 3D-RPE-LLaMA3-8B-Instruct* | $\\mathbf{5 1 . 8 9}$ | 61.38 | 4.44 | $\\mathbf{8 0 . 0 0}$ | 82.89 |\n",
      "\n",
      "tasks and are detailed in Supplementary Material D.1. The results in Table 1 highlight our model's significant performance advantages over baseline models in four tasks, both for models without training and those with fine-tuning. In summarization tasks, our model also achieved performance comparable to ChatGLM3-6B-32k. These experimental outcomes indicate that our model enhances the correlation between tokens with distant relative positions in long contexts through 3D-RPE, resulting in improved performance. Subsequently, the LEval Benchmark [1] was employed. Table 2 reveals that our model, 3D-RPELLaMA2-7B-Chat, outperformed LLaMA2-7B-NTK and LongChat-7B-16K. Although it did not surpass Vicuna1.5-7B-16K in Quality and CodeU tasks, it excelled in the Coursera, GSM, and TOEFL tasks. Additionally, we conducted experiments on LLaMA3-8B-Instruct using a 16k context window with 3D-RPE. The 3D-RPE-LLaMA3-8B-Instruct* showed performance improvements in the Coursera and GSM tasks. While 3D-RPE did not enhance performance in the CodeU, TOEFL, and QuALiTY tasks, there was no significant performance decline either. These experimental results demonstrate the effectiveness of the 3D-RPE method. ### 5.3 Long-Sequence Language Modeling\n",
      "\n",
      "In Table 3, we present the perplexity scores for our model, 3D-RPE-LLaMA-2-7B and baseline models on the proof-pile and PG19 test datasets. 3D-RPE-LLaMA-2-7B was fine-tuned from the LLaMA2-7B-Base model using a dataset with a $32 k$ context window. To evaluate performance, we set sequence lengths of $8 k, 16 k$, and $32 k$. We extended our model's sequence length from $32 k$ to $100 k$ using the position extending method from PoSE [31]. The results indicate that our method outperforms train-free sequence extending models. Compared to fine-tuned models, our model shows better performance at $8 k$ and $16 k$ sequence lengths. This suggests that the new positional encoding,\n",
      "\n",
      "Table 3: Perplexity evaluation on different extending methods. We conduct evaluation on the Proof-pile and PG-19 test datasets, varying evaluation context window size from $8 k$ to $100 k$. | METHODS | PG-19 |  |  |  | Proof-Pile |  |  |  |\n",
      "| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
      "|  | $8 k$ | $16 k$ | $32 k$ | $100 k$ | $8 k$ | $16 k$ | $32 k$ | $100 k$ |\n",
      "| LLaMA2-7B-Base | 131.09 | $>10^{2}$ | $>10^{2}$ | OOM | 16.79 | $>10^{2}$ | $>10^{2}$ | OOM |\n",
      "| LLama2-7B-PI | 11.32 | 19.5 | $>10^{2}$ | OOM | 3.86 | 5.94 | 33.7 | OOM |\n",
      "| LLama2-7B-NTK | 10.28 | 11.5 | 37.8 | OOM | 3.98 | 5.94 | 33.7 | OOM |\n",
      "| StreamingLLM | 9.23 | 9.25 | 9.24 | 9.32 | 3.47 | 3.51 | 3.50 | 3.55 |\n",
      "| LongLoRA-32k | 7.33 | 7.16 | $\\mathbf{7 . 0 4}$ | - | 2.78 | 2.61 | $\\mathbf{2 . 5 0}$ | - |\n",
      "| LongLoRA-100k | 7.57 | 7.33 | 7.16 | $\\mathbf{7 . 0 4}$ | 2.78 | $\\mathbf{2 . 6 0}$ | 2.58 | $\\mathbf{2 . 5 2}$ |\n",
      "| LongChat-32k | 8.92 | 8.85 | 8.81 | OOM | 2.98 | 2.70 | 2.65 | OOM |\n",
      "| Activation Beacon | 8.52 | 8.54 | 8.56 | 8.68 | 3.45 | 3.42 | 3.39 | 3.35 |\n",
      "| 3D-RPE-LLaMA2-7B | $\\mathbf{7 .\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.create_documents([sss.texts['3drpe']])\n",
    "print(len(docs))\n",
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3}$ | $\\mathbf{7 . 1 0}$ | 8.09 | 8.12 | $\\mathbf{2 . 7 2}$ | 2.93 | 2.89 | 3.05 |\n",
      "\n",
      "Table 4: Results are evaluated in Perplexity on PG19 validation split. '*' denotes train-free, implementing 3D-RPE directly on the LLaMA2-7B-Base model without additional fine-tuning, utilizing a chunk size of $3 k$. The context length of $8 k$ is extended directly with 3D-RPE. Achieving $16 k$ and $32 k$ is accomplished through linear positional interpolation with chunks based on the $8 k$ context length. | MODELS | $4 k$ | $8 k$ | $16 k$ | $32 k$ |\n",
      "| :--- | :---: | :---: | :---: | :---: |\n",
      "| LLaMA2-7B-PI | 7.94 | 9.19 | 15.11 | $>10^{2}$ |\n",
      "| LLaMA2-7B-NTK | 7.87 | 11.98 | 26.12 | 58.91 |\n",
      "| LLaMA2-7B-Yarn | 7.87 | 8.06 | 9.82 | 11.74 |\n",
      "| 3D-RPE-LLaMA2-7B* | 7.87 | $\\mathbf{7 .\n"
     ]
    }
   ],
   "source": [
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retry when recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 265/4573 [00:00<00:01, 2348.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4573/4573 [10:54<00:00,  6.99it/s]  \n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "pdfs2_dir=U.pjoin(sss.files_dir,'pdfs2')\n",
    "\n",
    "for i in tqdm(sss.lib2):\n",
    "    if 'externalIds' in sss.lib2[i]:\n",
    "        if 'ArXiv' in sss.lib2[i]['externalIds']:\n",
    "            arxiv_id=sss.lib2[i]['externalIds']['ArXiv']\n",
    "            path=U.pjoin(pdfs2_dir,f'{i}.pdf')\n",
    "            arxiv_pdf=f'https://arxiv.org/pdf/{arxiv_id}'\n",
    "            if U.pexists(path):\n",
    "                continue\n",
    "            try:\n",
    "                pdf=urllib.request.urlretrieve(arxiv_pdf, path)\n",
    "                time.sleep(0.01)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
