{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place for experimenting the progressive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.utils as U\n",
    "from model_discovery.configs.gam_config import GAMConfig, GAMConfig_14M\n",
    "# from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "from model_discovery.model.composer import GAUBase, GAUTree, check_tree_name, GABComposer\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "from model_discovery.evolution import BuildEvolution\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "\n",
    "prompts_dir='../model_discovery/agents/prompts/'\n",
    "gab_py = U.read_file(U.pjoin(prompts_dir,'gab_template.py'))\n",
    "gam_prompt_path = os.path.join(prompts_dir,'gam_prompt.py')\n",
    "gau_template_path = os.path.join(prompts_dir,'gau_template.py')\n",
    "GAM_TEMPLATE=open(gam_prompt_path).read()\n",
    "GAU_TEMPLATE=open(gau_template_path).read()\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n",
    "\n",
    "GAB_BASE=inspect.getsource(GABBase)\n",
    "GAB_COMPOSER=inspect.getsource(GABComposer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os,sys\n",
    "import importlib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "import model_discovery.agents.prompts.prompts as P\n",
    "import model_discovery.model.composer\n",
    "importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUTree,GAUBase,GAUNode\n",
    "import model_discovery.utils\n",
    "importlib.reload(model_discovery.utils)\n",
    "import model_discovery.utils as U\n",
    "\n",
    "\n",
    "\n",
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "# Placeholder classes for future implementation\n",
    "class MemoryAccessUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, memory_size, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "class DownsamplingUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, downsample_factor, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "\n",
    "class XXX(GAUBase,object):\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "\n",
    "class GAU(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
    "        # argv: list of hyperparameters\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\n",
    "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        self.a1 = kwarg_all.get('a1', 1)\n",
    "        self.a2: int = kwarg_all.get('a2', 2)\n",
    "\n",
    "        # COMPLETING THE CODE HERE #\n",
    "        self.token_scorer = TokenScoringGAU(embed_dim, **factory_kwargs)\n",
    "        self.dual_path: GAUBase = DualPathGAU(embed_dim, **factory_kwargs)\n",
    "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim, **factory_kwargs)\n",
    "\n",
    "        self.stages: nn.ModuleList = nn.ModuleList([CompressionStageGAU(embed_dim, device=\n",
    "            device, dtype=dtype, **kwargs) for _ in range(2)])\n",
    "\n",
    "        self.modelx=nn.ModuleList([\n",
    "            nn.Conv2d(1,20,5),\n",
    "            nn.ReLU(),\n",
    "            LatentAttentionGAU(),\n",
    "            nn.Conv2d(20,64,5),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "            \n",
    "\n",
    "        self.model1 = (\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU(),\n",
    "            'gau3': CompressionStageGAU()\n",
    "        })\n",
    "\n",
    "        self.model2 = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('gau2', DualPathGAU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "    def init_something(self):\n",
    "        self.modelx[3]=LatentAttentionGAU()\n",
    "        self.modelx.append(LatentAttentionGAU())\n",
    "        self.modelx.insert(0,LatentAttentionGAU())\n",
    "        self.activations['gau1']=LatentAttentionGAU()\n",
    "        self.model2.add_module('gau4',LatentAttentionGAU())\n",
    "\n",
    "        X=self.dual_path(X)\n",
    "    \n",
    "    def _forward(self, X, **Z): \n",
    "\n",
    "        # THE CODE HERE MUST BE COMPLETED #\n",
    "        # Step 1: Score tokens\n",
    "        X, Z = self.token_scorer(X, **Z)\n",
    "        # Step 2: Route through dual paths\n",
    "        # Step 3: Apply latent attention\n",
    "        Y, Z = self.latent_attention(X, A=A, B=B, **Z)\n",
    "        for stage in self.stages:\n",
    "            X = stage(X, **Z)\n",
    "        \n",
    "        X, Z = self.dual_path(X, **Z)\n",
    "        X=self.activations['gau3'](X)\n",
    "\n",
    "        return Y, Z\n",
    "\n",
    "@gau_test\n",
    "def test1(a=1,device=None):\n",
    "    print('test1')\n",
    "\n",
    "    \n",
    "@gau_test\n",
    "def test2(device=None):\n",
    "    print('test2')\n",
    "\n",
    "'''\n",
    "children = ['TokenScoringGAU', 'DualPathGAU', 'LatentAttentionGAU','CompressionStageGAU'] # solve redundancy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "class RelevanceScoringGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT KEEP THE ORIGINAL ONES #\n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        # Initialize a linear projection layer\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim // 2, **self.factory_kwargs)\n",
    "        self.projection.weight.requires_grad = True\n",
    "        self.projection.bias.requires_grad = True\n",
    "\n",
    "        # Initialize a learnable query vector\n",
    "        self.query_vector = nn.Parameter(torch.randn(embed_dim // 2, requires_grad=True, **self.factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        # Project the input embeddings\n",
    "        projected_X = self.projection(X)\n",
    "\n",
    "        # Compute similarity scores\n",
    "        similarity_scores = torch.matmul(projected_X, self.query_vector)\n",
    "\n",
    "        # Normalize the similarity scores\n",
    "        relevance_scores = F.softmax(similarity_scores, dim=-1)\n",
    "\n",
    "        # Weight the input embeddings with relevance scores\n",
    "        Y = X * relevance_scores.unsqueeze(-1)\n",
    "\n",
    "        # Return weighted embeddings and updated Z\n",
    "        return Y, {}\n",
    "\n",
    "\n",
    "# WRITE YOUR UNIT TEST FUNCTIONS HERE #\n",
    "\n",
    "@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON'T BE RECOGNIZED AS A UNIT TEST #\n",
    "def test_relevance_scoring_gau(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\n",
    "    # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\n",
    "\n",
    "    # Initialize the GAU\n",
    "    embed_dim = 512\n",
    "    block_loc = (0, 6)\n",
    "    relevance_scoring_gau = RelevanceScoringGAU(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={}, device=device, dtype=dtype)\n",
    "\n",
    "    # Mock input\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "\n",
    "    # Forward pass\n",
    "    Y, Z_ = relevance_scoring_gau(X, **Z)\n",
    "\n",
    "    # Assertions\n",
    "    assert Y.shape == (batch_size, seq_len, embed_dim), \"Output shape mismatch\"\n",
    "    assert isinstance(Z_, dict), \"Z_ should be a dictionary\"\n",
    "\n",
    "    # Print for debugging\n",
    "    print(\"Output embeddings:\", Y)\n",
    "    print(\"Updated Z:\", Z_)\n",
    "\n",
    "    # Additional checks\n",
    "    assert torch.allclose(Y.sum(dim=-1), X.sum(dim=-1), atol=1e-6), \"Weighted embeddings should preserve the sum along the embedding dimension\"\n",
    "\n",
    "    print(\"RelevanceScoringGAU test passed!\")\n",
    "'''\n",
    "children=['ProjectionGAU','SimilarityComputationGAU','NormalizationGAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "class EnhancedGatedMLP(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, hidden_features=None, out_features=None, activation=None, bias=False, multiple_of=128, **kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT KEEP THE ORIGINAL ONES #\n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        out_features = out_features if out_features is not None else embed_dim\n",
    "        hidden_features = (hidden_features if hidden_features is not None else int(8 * embed_dim / 3))\n",
    "        hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of\n",
    "\n",
    "        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **self.factory_kwargs)\n",
    "        self.activation = activation if activation is not None else F.silu\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **self.factory_kwargs)\n",
    "\n",
    "        # Feedback loop components\n",
    "        self.feedback_fc = nn.Linear(out_features, hidden_features, bias=bias, **self.factory_kwargs)\n",
    "        self.memory_buffer = nn.Parameter(torch.zeros(hidden_features, **self.factory_kwargs))\n",
    "        self.layer_norm = nn.LayerNorm(hidden_features, **self.factory_kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        # Forward pass through the first linear layer\n",
    "        y = self.fc1(X)\n",
    "        y, gate = y.chunk(2, dim=-1)\n",
    "        y = y * self.activation(gate)\n",
    "\n",
    "        # Feedback loop\n",
    "        feedback = self.feedback_fc(y)\n",
    "        feedback = self.layer_norm(feedback + self.memory_buffer)\n",
    "\n",
    "        # Update memory buffer\n",
    "        self.memory_buffer.data = feedback.detach()\n",
    "\n",
    "        # Forward pass through the second linear layer\n",
    "        y = self.fc2(feedback)\n",
    "        return y, {}\n",
    "\n",
    "\n",
    "# WRITE YOUR UNIT TEST FUNCTIONS HERE #\n",
    "\n",
    "@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON'T BE RECOGNIZED AS A UNIT TEST #\n",
    "def test_enhanced_gated_mlp(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\n",
    "    # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\n",
    "\n",
    "    embed_dim = 64\n",
    "    block_loc = (0, 1)\n",
    "    kwarg_all = {}\n",
    "    model = EnhancedGatedMLP(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, device=device, dtype=dtype)\n",
    "\n",
    "    X = torch.randn(2, 10, embed_dim, device=device, dtype=dtype)\n",
    "    Y, Z = model(X)\n",
    "\n",
    "    assert Y.shape == X.shape, f\"Expected output shape {X.shape}, but got {Y.shape}\"\n",
    "    assert isinstance(Z, dict), \"Expected Z to be a dictionary\"\n",
    "\n",
    "    print(\"EnhancedGatedMLP test passed!\")\n",
    "\n",
    "    # WRITE ASSERTIONS TO PERFORM THE TEST, USE PRINT TO DEBUG #\n",
    "\n",
    "    # raise NotImplementedError # YOU MUST IMPLEMENT THIS FUNCTION #\n",
    "'''\n",
    "\n",
    "children=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_name = \"MHA\"  # Provide the unit_name to rename GAU class\n",
    "reformatted_code, new_args, gau_tests, errors, warnings, fetal_errors = check_and_reformat_gau_code(code, unit_name,children)\n",
    "print(\"Reformatted Code:\\n\" + reformatted_code)\n",
    "print(\"Errors:\\n\", '\\n\\t'.join(errors+fetal_errors))\n",
    "print(\"Warnings:\\n\", '\\n\\t'.join(warnings))\n",
    "print(\"New Arguments:\\n\", new_args)\n",
    "\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "spec=P.UnitSpec(\n",
    "   unitname = unit_name,\n",
    "   document = 'Test',\n",
    "   inputs = [],\n",
    "   outputs = [],\n",
    ")\n",
    "test_tree.add_unit(\n",
    "    spec, reformatted_code, new_args, None, None, None, children, gau_tests, None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[{'role': 'system', 'content': '\\nYou are a professional AI researcher focusing on discovering the best\\nautoregressive language model block. Your goal is to design a novel block\\nfollowing the Generalized Autoregressive Block (GAB) structure defined in the\\nfollowing base class:\\n\\n```python class GABBase(nn.Module):\\n \"\"\" Base class for Generalized Autoregressive Block \"\"\"\\n def __init__(self,embed_dim: int, block_loc: tuple):\\n super().__init__()\\n self.embed_dim = embed_dim\\n self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\\n\\n def _forward(self, X, **Z): \\n raise NotImplementedError\\n \\n # YOU ARE NOT ALLOW TO OVERRIDE THIS METHOD #\\n def forward(self, X, **Z): # kwargs not parsable by torchscript but more flexible\\n \"\"\"Forward pass of the model\"\"\"\\n assert len(X.shape) == 3, f\"Input shape must be (batch, seqlen, embed_dim), got {len(X.shape)}D instead\"\\n assert X.shape[-1] == self.embed_dim, f\"Input shape must be (batch, seqlen, embed_dim), got {X.shape} instead\"\\n Y = self._forward(X, **Z)\\n if isinstance(Y, tuple):\\n Y, Z_ = Y\\n else:\\n Z_ = {}\\n assert Y.shape == X.shape, f\"GAB Output shape must be the same as input shape of {X.shape}, got {Y.shape} instead\"\\n assert isinstance(Z, dict), \"Intermediate variables must be stored in a dict\"\\n Z.update(Z_) # the new intermediate variables are updated to the current Z\\n return Y, Z\\n ```\\n\\nThe GAB will be used to construct a Generalized Autoregressive Model (GAM)\\ndefined as follows:\\n\\n```python from transformers.modeling_outputs import CausalLMOutput\\nfrom transformers import PreTrainedModel, PretrainedConfig\\nfrom dataclasses import dataclass\\n\\nimport torch\\nfrom torch import nn\\n\\nfrom gab import GAB, gab_config\\n\\n\\n@dataclass\\nclass GAMConfig(PretrainedConfig):\\n \\'\\'\\'Configurations for Generalized Autoregressive Models.\\'\\'\\'\\n\\n d_model: int\\n n_block: int\\n batch_tokens: int \\n vocab_size: int = None\\n\\n\\nclass GAM(nn.Module):\\n \\'\\'\\' Generalized Autoregressive Models\\n Input: X: (batch, seqlen, embed_dim)\\n Output: Y: (batch, seqlen, embed_dim)\\n \\'\\'\\'\\n def __init__(\\n self,\\n d_model: int,\\n n_block: int,\\n vocab_size: int = 50277,\\n norm_epsilon: float = 1e-5,\\n device = None,\\n dtype = None,\\n ) -> None:\\n self.factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n super().__init__()\\n self.d_model = d_model\\n self.embedding = nn.Embedding(vocab_size, d_model, **self.factory_kwargs)\\n\\n block_config = gab_config()\\n self.blocks = nn.ModuleList(\\n [\\n GAB(\\n embed_dim=d_model, \\n block_loc=(layer_idx,n_block),\\n device=device, \\n dtype=dtype, \\n **block_config\\n )\\n for layer_idx in range(n_block)\\n ]\\n )\\n self.norm_out = nn.LayerNorm(\\n d_model, eps=norm_epsilon, **self.factory_kwargs\\n )\\n\\n def forward(self, input_ids):\\n hidden_states = self.embedding(input_ids)\\n intermediate_vars = {}\\n for block in self.blocks:\\n hidden_states, intermediate_vars = block(\\n hidden_states,\\n **intermediate_vars\\n )\\n hidden_states = self.norm_out(hidden_states)\\n return hidden_states\\n\\n\\nclass GLMHeadModel(PreTrainedModel):\\n \\'\\'\\' Generalized Autoregressive Models with LM Head \\'\\'\\'\\n config_class = GAMConfig\\n\\n def __init__(\\n self,\\n config: GAMConfig,\\n device=None,\\n dtype=None,\\n ) -> None:\\n super().__init__(config)\\n factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n self.backbone = GAM(\\n d_model=config.d_model,\\n n_block=config.n_block,\\n vocab_size=config.vocab_size,\\n **factory_kwargs,\\n )\\n self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False, **factory_kwargs)\\n\\n def forward(self, input_ids, **gab_kwargs):\\n hidden_states = self.backbone(input_ids, **gab_kwargs)\\n lm_logits = self.lm_head(hidden_states)\\n return CausalLMOutput(logits=lm_logits)\\n ```\\n\\nThe produced language model will be pretrained with the corpus and then be\\napplied for downstream tasks. The new model is expected to have a low\\nperplexity, high accuracy, robustness, efficiency, and most importantly, good\\nscalability. \\n\\nSince the autoregressive model design is complicated, so we will break it down\\ninto smaller parts. We represent a block as multiple nested units, the\\nGeneralized Autoregressive Unit (GAU). Each GAU accepts a sequence of embeddings\\nX and a dictionary of intermediate variables Z as input, and outputs a sequence\\nof embeddings Y and a dictionary of new or updated intermediate variables Z_. Z_\\nis optional, when it is provided, it will be used to update Z for the next unit\\nby Z.update(Z_). A GAU is defined in the following base class:\\n\\n```python class GAUBase(nn.Module): \\n \"\"\" \\n Instead of directly giving the full implementation of a GAB block, the agent need to \\n design a series of nested GAB units and construct the full GAB block as a pipeline of these units.\\n\\n GAB is fractal, like GAB itself, each GAB unit accepts X and Z as input and returns Y and Z as output.\\n \"\"\" \\n def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict):\\n super().__init__()\\n self.embed_dim = embed_dim\\n self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\\n self.kwarg_all = kwarg_all # dict of all kwargs, use it to initialize the children units\\n\\n def _forward(self, X, **Z): # you can define the arguments instead of using **Z\\n raise NotImplementedError\\n \\n def forward(self, X, **Z):\\n assert len(X.shape) == 3 and X.shape[-1] == self.embed_dim, f\"Input X must be a sequence of shape (batch, seqlen, embed_dim), all other kinds of variables should be passed by Z. Got {X.shape} instead.\"\\n _params = inspect.signature(self._forward).parameters\\n X=X.to(**self.factory_kwargs)\\n _Z = {k: v for k, v in Z.items() if k in _params}\\n Y = self._forward(X, **_Z)\\n if isinstance(Y, tuple):\\n Y, Z_ = Y\\n else:\\n Z_ = {}\\n assert Y.shape == X.shape, f\"GAU output Y must be a sequence with the same shape as input of {X.shape}, got {Y.shape} instead, all other kinds of variables should be passed by Z.\"\\n assert isinstance(Z_, dict), \"Intermediate variables must be stored in a dict\"\\n Z.update(Z_) # the new intermediate variables are updated to the current Z\\n return Y, Z\\n ```\\n\\nYou will design a GAU by completing the blanks marked in this template, which\\nincludes the initialization where you can define your custom arguments with\\noptional default values, the forward function where you can define convenient\\nfunctions or classes in the GAB class such as caches, notice that you are only\\nallowed to have only one GAU which inherited from the GAUBase class in the file:\\n \\n```python # gau.py # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\\n\\n\\n# YOU CAN IMPORT MORE MODULES HERE #\\n\\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\\n\\n\\nclass UnitName(GAUBase):\\n \"\"\"Generalized Autoregressive Block Unit\\n Input: X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \\n Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\\n Constraints: Causal, differentiable, parameter number, complexity, parallelizable...\\n\\n embed_dim: The dimension of the input embeddings\\n block_loc: The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\\n kwarg_all: A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\\n \"\"\"\\n def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT KEEP THE ORIGINAL ONES #\\n self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\\n super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\\n \\n # COMPLETING THE CODE HERE #\\n\\n raise NotImplementedError\\n\\n\\n # YOU CAN ADD MORE FUNCTIONS HERE #\\n\\n\\n def _forward(self, X, **Z): \\n \\n # THE CODE HERE MUST BE COMPLETED #\\n\\n raise NotImplementedError\\n\\n\\n# WRITE YOUR UNIT TEST FUNCTIONS HERE #\\n\\n@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON\\'T BE RECOGNIZED AS A UNIT TEST #\\ndef unit_test_name(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\\n # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\\n\\n # WRITE ASSERTIONS TO PERFORM THE TEST, USE PRINT TO DEBUG #\\n \\n raise NotImplementedError # YOU MUST IMPLEMENT THIS FUNCTION #\\n ```\\n\\nIn a GAU, you can call other GAUs, as such, you can create a complicated GAB\\nblock by nesting multiple GAUs. However, each GAU should be not too complex, if\\nyou want to create complex block, you should break it down into smaller GAUs and\\nnest them. As such, you should design a GAB block in a top-down manner. \\n\\nInstead of starting from scratch, you will start from an existing design and\\nimprove it. You will be provided with the full information of the design,\\nincluding the proposal, the tree structure, the implementations of the GAUs. You\\nare only allowed to modify *one GAU* from the existing design. \\n\\nYou need to select one GAU to modify, you can define new children GAUs, however\\nyou need to guarantee that your modification wont affect the correctness of the\\noverall design. \\n\\nYou will start by writing down an overal proposal for the design you want to\\nhave, the proposal decides a direction, phylosophy and the plan of the design,\\nand the analysis of the problem and how you gonna solve it by modifying one GAU\\nfrom the existing design. You will be provided with one or multiple references\\nto consider that may inspire you if there are references provided.\\n\\nYour response should include: \\n\\n1. The proposal, it should include but not restrict to the following parts: a. A\\n title with the name of the design in the level 1 header format. You shuld\\n have only one level 1 header in your response which is the name of the\\n design.\\n\\n b. Your motivation of the design. What problem you want to solve based on the\\n insights or observations you have about the autoregressive models today,\\n and any inspirations you may have from the references. \\n\\n c. The analysis of the problem.\\n\\n d. The core idea and phylosophy behind of your design that may solve the\\n problem you proposed. \\n\\n e. The plan of the design. You should include subsections of that describe\\n the details of each part of the design with the justifications. The\\n selection of the GAU to modify and the reasoning of the selection. \\n\\n f. A conclution of the proposal. \\n\\n g. Optional, the references you used in your proposal, should be in the right\\n format.\\n2. The name of the variant of the model you are going to design.\\n3. The selection of the GAU to modify.\\n\\nThe proposal will be reviewed and you will be asked to modify it if it is not\\npassed. You can start to implement the design after the proposal is passed. \\n\\nThe proposal should be as detailed as possible, DO NOT WORRY IF THE PROPOSAL IS\\nTOO LONG, BUT ALSO DO NOT FILL IN BY REDUNDANT WORDS, USE PRECISE AND CONCRETE\\nLANGUAGE, the proposal will be the guideline for the entire design process so it\\nshould be clear and detailed. \\n'}]\n",
    "print(x[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code=test_tree.compose_unit('XAU')\n",
    "code=U.add_line_num(code)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class DilatedReformerRootGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.dilated_attention = DilatedAttentionGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.reversible_residual = ReversibleResidualGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **\n",
    "            self.factory_kwargs, **kwarg_all)\n",
    "        self.lsh_attention = LSHAttentionGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.dilated_attention(X, **Z)\n",
    "        X, Z = self.reversible_residual(X, **Z)\n",
    "        X, Z = self.lsh_attention(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class LSHAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class DilatedAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class ReversibleResidualGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "@gau_test\n",
    "def test_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau(device=\n",
    "    None, dtype=None) ->None:\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    embed_dim = 16\n",
    "    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "    gau = DilatedReformerRootGAU(embed_dim=embed_dim, block_loc=(0, 6), kwarg_all={}, device=\n",
    "        device, dtype=dtype)\n",
    "    Y, Z_ = gau(X, **Z)\n",
    "    assert Y.shape == X.shape, 'Output shape should match input shape'\n",
    "    assert isinstance(Z_, dict), 'Z_ should be a dictionary'\n",
    "    print('Input X:', X)\n",
    "    print('Output Y:', Y)\n",
    "    print('Intermediate variables Z_:', Z_)\n",
    "\n",
    "\n",
    "def run_DilatedReformerRootGAU_tests():\n",
    "\ttry:\n",
    "\t\ttest_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"Error in running unit_test_dilated_reformer_root_gau:\", e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\trun_DilatedReformerRootGAU_tests()\n",
    "'''\n",
    "\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "output=io.StringIO()\n",
    "with redirect_stdout(output):\n",
    "    exec(code)\n",
    "captured = str(output.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ResonanceLOCOSTGAMRoot(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class ResonanceLOCOSTGAMRoot(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.resonance_rope = ResonanceRoPEUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.state_space = StateSpaceUnit(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all)\n",
    "        self.inter_block_comm = InterBlockCommUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y, Z_ = self.resonance_rope(X, **Z)\n",
    "        Y, Z_ = self.state_space(Y, **Z_)\n",
    "        Y, Z_ = self.inter_block_comm(Y, **Z_)\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class ResonanceRoPEUnit(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.freqs = self._init_frequencies(embed_dim)\n",
    "\n",
    "    def _init_frequencies(self, embed_dim):\n",
    "        half_dim = embed_dim // 2\n",
    "        freqs = torch.exp(-torch.arange(half_dim, dtype=torch.float32,\n",
    "            device=self.factory_kwargs['device']) * (math.log(10000.0) / (\n",
    "            half_dim - 1)))\n",
    "        return freqs\n",
    "\n",
    "    def _apply_rope(self, X):\n",
    "        batch_size, seqlen, _ = X.size()\n",
    "        position_ids = torch.arange(seqlen, dtype=X.dtype, device=X.device)\n",
    "        sinusoid_inp = torch.einsum('i,j->ij', position_ids, self.freqs)\n",
    "        sin, cos = sinusoid_inp.sin(), sinusoid_inp.cos()\n",
    "        X1, X2 = X[..., ::2], X[..., 1::2]\n",
    "        Y1 = X1 * cos - X2 * sin\n",
    "        Y2 = X1 * sin + X2 * cos\n",
    "        return torch.stack([Y1, Y2], dim=-1).flatten(-2)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = self._apply_rope(X)\n",
    "        Z_ = Z\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "class StateSpaceUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class InterBlockCommUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "    \n",
    "\n",
    "\n",
    "gab_config = {}\n",
    "\n",
    "\n",
    "# Check if to device still work!\n",
    "\n",
    "gab = GAB(128, (0, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_device(model):\n",
    "    \"\"\"\n",
    "    Returns the device of the first parameter found in the given nn.Module.\n",
    "    If the module has no parameters, returns None.\n",
    "    \"\"\"\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "get_module_device(gab.root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gab.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children={\n",
    "    'unit1': 'MockUnit1',\n",
    "    'unit2': 'MockUnit2',\n",
    "}\n",
    "called=['unit1', 'unit2', 'unit1']\n",
    "test_tree.units['DualPathGAU'] = GAUNode('DualPathGAU', None, None, None, called, None, None, children, None)\n",
    "children={\n",
    "    'unit1': 'MockUnit3',\n",
    "    'unit2': 'MockUnit4',\n",
    "}\n",
    "called=['unit2', 'unit2']\n",
    "test_tree.units['MockUnit1'] = GAUNode('MockUnit1', None, None, None, called, None, None, children, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self,r,d,e,**Z):\n",
    "        print('Args in A:',d,e)\n",
    "        print('Kwargs in A:',Z)\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self,_Z,c=1,d=2,e=3,**Z):\n",
    "        args2={'d':d,'e':e}\n",
    "        super().__init__(c,**_Z,**args2)\n",
    "        print('Kwargs in B:',Z)\n",
    "\n",
    "cfg={\n",
    "    'a':1,\n",
    "    'b':2,\n",
    "    'c':3,\n",
    "}\n",
    "\n",
    "B(cfg,**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(X,Z,**kwargs):\n",
    "    print(X)\n",
    "    print(Z)\n",
    "    print(kwargs)\n",
    "\n",
    "def B(X,Z,a,**kwargs):\n",
    "    A(X,Z,**Z)\n",
    "\n",
    "Z={'a':3}\n",
    "B(1,Z,**Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "    '''\n",
    "    GAU Tree Map of Butterfly-AFT Generalized Autoregressive Block (BAGAB):\n",
    "    ButterflyAFTGAU (Rating: 3.5/5)\n",
    "        |- AFTMechanismGAU (Rating: 3.5/5)\n",
    "            |- PositionBiasGAU (Rating: 3.5/5)\n",
    "            |- ElementWiseOperationGAU (Rating: 3.5/5)\n",
    "        |- ButterflyTransformGAU (Rating: 3.5/5)\n",
    "            |- ButterflyLayerGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "            |- ButterflyInitializationGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "                |- ButterflyParameterGAU (Unimplemented)\n",
    "\n",
    "    Implemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ElementWiseOperationGAU, ButterflyAFTGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    Unimplemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ButterflyParameterGAU, ElementWiseOperationGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    '''\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ButterflyAFTGAU(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase\n",
    "\n",
    "\n",
    "class ButterflyAFTGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.aft_mechanism = AFTMechanismGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.butterfly_transform = ButterflyTransformGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        aft_output, Z = self.aft_mechanism(X, **Z)\n",
    "        butterfly_output, Z = self.butterfly_transform(aft_output, **Z)\n",
    "        return butterfly_output, Z\n",
    "\n",
    "\n",
    "class ButterflyTransformGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_layer = ButterflyLayerGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_init = ButterflyInitializationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.butterfly_init(X, **Z)\n",
    "        Y, Z = self.butterfly_layer(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ButterflyLayerGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.stages = nn.ModuleList([ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all) for _ in range(kwarg_all.get('num_stages', 4))])\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for stage in self.stages:\n",
    "            X, Z = stage(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "class ButterflyMatrixGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ButterflyInitializationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_size = kwarg_all.get('butterfly_size', embed_dim)\n",
    "        self.butterfly_matrices = nn.ParameterList([nn.Parameter(torch.\n",
    "            randn(self.butterfly_size, self.butterfly_size, **self.\n",
    "            factory_kwargs)) for _ in range(int(torch.log2(torch.tensor(\n",
    "            self.butterfly_size))))])\n",
    "        self.butterfly_stage = ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_parameter = ButterflyParameterGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for matrix in self.butterfly_matrices:\n",
    "            X = F.linear(X, matrix)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class ButterflyParameterGAU(GAUBase):\n",
    "    \"\"\"\n",
    "    Generalized Autoregressive Block Unit for learning parameters of Butterfly Factorization.\n",
    "    Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "    Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "    Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "    embed_dim:    The dimension of the input embeddings\n",
    "    block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "    kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.param_size = kwarg_all.get('param_size', 128)\n",
    "        self.butterfly_weights = nn.Parameter(torch.randn(self.param_size,\n",
    "            embed_dim, **self.factory_kwargs))\n",
    "        nn.init.xavier_uniform_(self.butterfly_weights)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X = torch.matmul(X, self.butterfly_weights)\n",
    "        Z_ = {'butterfly_weights': self.butterfly_weights}\n",
    "        return transformed_X, Z_\n",
    "        # transformed_X = F.linear(X, self.butterfly_weights.T)\n",
    "        # return transformed_X, {}\n",
    "\n",
    "\n",
    "class ButterflyComputationGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class ButterflyStageGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_params = ButterflyParameterGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X, Z = self.butterfly_params(X, **Z)\n",
    "        assert transformed_X.shape == X.shape, f'Output shape {transformed_X.shape} does not match input shape {X.shape}'\n",
    "        return transformed_X, Z\n",
    "\n",
    "class AFTMechanismGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = PositionBiasGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.element_wise_operation = ElementWiseOperationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.position_bias(X, **Z)\n",
    "        Y, Z = self.element_wise_operation(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ElementWiseOperationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, embed_dim, **self.\n",
    "            factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        queries = Z.get('queries', X)\n",
    "        keys = Z.get('keys', X)\n",
    "        values = Z.get('values', X)\n",
    "        elementwise_product = queries * keys\n",
    "        elementwise_sum = elementwise_product + values + self.position_bias\n",
    "        return elementwise_sum, {}\n",
    "\n",
    "\n",
    "class PositionBiasGAU(GAUBase):\n",
    "    \"\"\"Position Bias Generalized Autoregressive Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, 1, embed_dim, **\n",
    "            self.factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = X + self.position_bias\n",
    "        return Y, {}\n",
    "    \n",
    "\n",
    "class GABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100, **kwargs):\n",
    "        super().__init__()\n",
    "        self.gab = GAB(embed_dim, (0,1), **kwargs)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, Z = self.gab(X, **Z)\n",
    "        return self.lm_head(X),X\n",
    "\n",
    "\n",
    "\n",
    "gab_config = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100):\n",
    "        super().__init__()\n",
    "        self.butterfly_param = ButterflyParameterGAU(embed_dim, (0,1), {})\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, _ = self.butterfly_param(X, **Z)\n",
    "        return self.lm_head(X), X\n",
    "\n",
    "# Test the simplified model\n",
    "simple_model = GABLM(128)\n",
    "mock_input = torch.randn(1, 10, 128)\n",
    "mock_y = torch.randint(0, 100, (1, 10))\n",
    "\n",
    "optimizer = optim.Adam(simple_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for _ in range(5):  # Run for a few iterations\n",
    "    optimizer.zero_grad()\n",
    "    y, X = simple_model(mock_input)\n",
    "    loss = criterion(y.view(-1, 100), mock_y.view(-1))\n",
    "    loss.backward()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    for name, param in simple_model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"Gradient for {name}: {param.grad.abs().mean()}\")\n",
    "        else:\n",
    "            print(f\"No gradient for {name}\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim \n",
    "\n",
    "model=GABLM(128, **gab_config)\n",
    "vocab_size=100\n",
    "DEFAULT_CONTEXT_LENGTH=10\n",
    "\n",
    "mock_input=torch.randn(1, DEFAULT_CONTEXT_LENGTH, 128)\n",
    "mock_y=torch.randint(0, 100, (1, 10))\n",
    "mock_Z={}\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "y,X=model(mock_input, **mock_Z)\n",
    "loss=nn.CrossEntropyLoss()(y.view(-1, 100), mock_y.view(-1))\n",
    "loss.backward()\n",
    "\n",
    "used_params = set()\n",
    "\n",
    "# Hook to register which parameters are used during the forward pass\n",
    "def hook_fn(module, input, output):\n",
    "    for param in module.parameters():\n",
    "        used_params.add(param)\n",
    "\n",
    "# Register hooks for all submodules\n",
    "hooks = []\n",
    "for submodule in model.modules():\n",
    "    hook = submodule.register_forward_hook(hook_fn)\n",
    "    hooks.append(hook)\n",
    "\n",
    "# Forward pass again to collect used parameters\n",
    "model(mock_input)\n",
    "\n",
    "# Remove all hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "all_gradients_present = True\n",
    "\n",
    "# Check gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if param in used_params and param.grad is None:\n",
    "            # Used parameter without gradient -> Error\n",
    "            print(f\"Error: Used parameter {name} requires gradients but has none.\")\n",
    "            all_gradients_present = False\n",
    "        elif param not in used_params:\n",
    "            # Unused parameter -> Warning\n",
    "            print(f\"Warning: Parameter {name} was not used in the forward pass.\")\n",
    "\n",
    "if all_gradients_present:\n",
    "    print('Differentiability test passed')\n",
    "else:\n",
    "    print('Differentiability test failed due to missing gradients.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INIT_DESIGN_INPUT='''\n",
    "Below is the specification for the GAU you need to refine:\n",
    "Specification: Unit Name: GatedMLP The DynamicGatedMLP is an enhanced version of the GatedMLP that introduces a dynamic gating mechanism capable of selecting between multiple activation functions. This design aims to improve the expressiveness and adaptability of the model by allowing it to dynamically choose the most suitable activation function for different inputs. The unit maintains the same input-output interface as the original GatedMLP, ensuring seamless integration with the existing GPT-2 architecture.\n",
    "\n",
    "\n",
    "Inputs: X\n",
    "Outputs: Y\n",
    "Children list: []\n",
    "\n",
    "Current Implementation: import torch import torch.nn as nn from model_discovery.model.utils.modules import GAUBase, gau_test import torch.nn.functional as F\n",
    "\n",
    "class GatedMLP(GAUBase): \"\"\"Generalized Autoregressive Block Unit Input: X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of new intermediate variables to update the current Z} Constraints: Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "embed_dim:    The dimension of the input embeddings\n",
    "    block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "    kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "\"\"\"\n",
    "\n",
    "def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "    device=None, dtype=None, hidden_features=None, out_features=None,\n",
    "    bias=False, multiple_of=128, **kwargs):\n",
    "    self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "    out_features = out_features if out_features is not None else embed_dim\n",
    "    hidden_features = (hidden_features if hidden_features is not None else\n",
    "        int(8 * embed_dim / 3))\n",
    "    hidden_features = (hidden_features + multiple_of - 1\n",
    "        ) // multiple_of * multiple_of\n",
    "    self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n",
    "        self.factory_kwargs)\n",
    "    self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n",
    "        self.factory_kwargs)\n",
    "    self.activations = [F.relu, F.tanh, F.gelu]\n",
    "    self.gate_weights = nn.Parameter(torch.randn(len(self.activations),\n",
    "        hidden_features, **self.factory_kwargs))\n",
    "\n",
    "def _forward(self, X, **Z):\n",
    "    y = self.fc1(X)\n",
    "    y, gate = y.chunk(2, dim=-1)\n",
    "    gate_scores = F.softmax(self.gate_weights, dim=0)\n",
    "    y = sum(gate_scores[i] * self.activations[i](gate) for i in range(\n",
    "        len(self.activations)))\n",
    "    y = self.fc2(y)\n",
    "    return y, {}\n",
    "\n",
    "Review: ### Highlights:\n",
    "\n",
    "Successful Implementation Fix: The designer has successfully addressed the previous implementation error by initializing the activations attribute before its use, resolving the AttributeError.\n",
    "Dynamic Gating Mechanism: The design continues to leverage a dynamic gating mechanism, which is a novel approach that enhances the model's adaptability and expressiveness by selecting between multiple activation functions.\n",
    "Checker Validation: The updated implementation has passed all checker tests, including forward pass, causality, and differentiability, indicating that the GAU is functioning correctly within the model.\n",
    "Concerns:\n",
    "Efficiency Considerations: While the implementation is now functional, the potential computational overhead of evaluating multiple activation functions simultaneously remains a concern. This could impact efficiency, especially in larger models or datasets.\n",
    "Justification of Activation Functions: The choice of activation functions (ReLU, Tanh, GELU) is reasonable, but further theoretical justification or empirical validation would strengthen the design.\n",
    "Accuracy, Robustness, Efficiency, and Scalability:\n",
    "Accuracy and Robustness: The dynamic selection of activation functions should enhance the model's ability to adapt to various input patterns, potentially improving accuracy and robustness.\n",
    "Efficiency: The design could still benefit from optimizations to reduce computational overhead, ensuring it remains efficient in practice.\n",
    "Scalability: The design appears scalable, as indicated by the successful checker tests, but empirical validation on larger datasets would be beneficial.\n",
    "Novelty:\n",
    "The design introduces a novel dynamic gating mechanism, which is a significant improvement over static activation functions.\n",
    "Clarity and Completeness:\n",
    "The design is now clear and complete, with the previous implementation error resolved.\n",
    "Theoretical Soundness:\n",
    "The theoretical foundation of using dynamic gating is sound, and the implementation aligns well with the proposal's objectives.\n",
    "Implementation Feasibility:\n",
    "The current implementation is feasible and has been validated by the checker, indicating it can be integrated into the broader model.\n",
    "Suggestions:\n",
    "Efficiency Optimization: Consider exploring methods to optimize the dynamic gating mechanism to reduce computational overhead, such as selectively applying it or using more efficient computation techniques.\n",
    "Empirical Validation: Plan for empirical validation to confirm the theoretical benefits of the dynamic gating mechanism and the choice of activation functions in practice.\n",
    "Documentation: Ensure thorough documentation of the design choices and their theoretical justifications to aid future refinements and empirical testing.\n",
    "Rating: 4 out of 5 (Passing score >3)\n",
    "\n",
    "Reviewer Suggestions: 1. Efficiency Optimization: Consider exploring methods to optimize the dynamic gating mechanism to reduce computational overhead, such as selectively applying it or using more efficient computation techniques. 2. Empirical Validation: Plan for empirical validation to confirm the theoretical benefits of the dynamic gating mechanism and the choice of activation functions in practice.\n",
    "\n",
    "Documentation: Ensure thorough documentation of the design choices and their theoretical justifications to aid future refinements and empirical testing.\n",
    "Refinement Process\n",
    "If there is a review provided, you should start by reflecting on the feedback. Otherwise, leave reflection empty. The, proceed with the following:\n",
    "\n",
    "New Analysis and Design: - Provide an updated detailed analysis based on the feedback, including your new design direction and justifications. - Include a high-level pseudocode that captures the core of the new design. You should also provide the updated document of the GAU that allows other people to understand the design and implementation without the need to read the code.\n",
    "\n",
    "Implementation: - Provide the full updated implementation of the GAU, following the specified format and templates.\n",
    "\n",
    "Children list: - Provide the list of the children GAUs that are declared in the current GAU. You can declare new children GAUs or preserve the existing ones. If you do not declare any new children GAUs, you should provide the original children GAUs.\n",
    "\n",
    "Log of Changes: - Summarize the key changes you made during the refinement process. Including all code snippets where you made a change wrapped in python .\n",
    "\n",
    "Key Points to Remember:\n",
    "The bug or issue must always be resolved within the current GAU, as other units are either fully implemented and tested or placeholders that do not perform any computation.\n",
    "Ensure the GAU is self-contained, so you won't need to adjust it later when working on other units.\n",
    "The design must align with the original proposal and follow all instructions, templates, and format requirements.\n",
    "Use a top-down approach: break down complex operations into smaller tasks where necessary and declare each of them as a child GAU. Do not make a single unit overly complex.\n",
    "Remember your final goal is to refine the GAU in a way that enhances the overall design, ensuring both correctness and innovation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05:20:45:46,951 INFO     [_client.py:1026] HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Reflection on Feedback:\\nThe reviewer\\'s suggestions highlight the need for efficiency optimization, empirical validation, and thorough documentation. I\\'ll address these points in the refined design.\\n\\nNew Analysis and Design:\\n\\nThe current GatedMLP design introduces a dynamic gating mechanism that allows the model to select between multiple activation functions. While this approach enhances adaptability, it may introduce computational overhead. To address this, we\\'ll implement a more efficient gating mechanism that maintains the benefits of dynamic activation selection while reducing computational costs.\\n\\nUpdated Design Direction:\\n1. Efficient Gating: Instead of applying all activation functions and then combining them, we\\'ll use a lightweight gating network to predict weights for each activation function. This approach reduces the number of computations while still allowing dynamic selection.\\n\\n2. Sparse Activation: We\\'ll introduce a sparsity factor in the gating mechanism, allowing the model to focus on the most relevant activation functions for each input.\\n\\n3. Adaptive Computation: We\\'ll implement a mechanism to dynamically adjust the number of active gates based on the input complexity, further optimizing computational resources.\\n\\nHigh-level Pseudocode:\\n```\\nfunction GatedMLP(X):\\n    hidden = linear_projection(X)\\n    gate_weights = lightweight_gate_network(hidden)\\n    sparse_gate_weights = apply_sparsity(gate_weights)\\n    activated = sum(sparse_gate_weights[i] * activation[i](hidden) for i in range(num_activations))\\n    output = linear_projection(activated)\\n    return output\\n```\\n\\nUpdated GAU Document:\\nThe GatedMLP unit now incorporates an efficient dynamic gating mechanism that selects between multiple activation functions. Key features include:\\n- Lightweight gating network for predicting activation weights\\n- Sparse activation selection to focus on the most relevant functions\\n- Adaptive computation based on input complexity\\n- Maintains the same input-output interface as the original GatedMLP\\n\\nThis design aims to balance expressiveness and computational efficiency, allowing the model to adapt to various input patterns while managing resource usage effectively.\\n\\nImplementation:\\n\\n```python\\n# gau.py\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test\\n\\nclass GatedMLP(GAUBase):\\n    \"\"\"\\n    Generalized Autoregressive Block Unit with efficient dynamic gating\\n    Input:  X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \\n    Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of new intermediate variables to update the current Z}\\n    \"\"\"\\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\\n                 device=None, dtype=None, hidden_features=None, out_features=None,\\n                 bias=False, multiple_of=128, num_activations=3, sparsity_factor=0.5, **kwargs):\\n        self.factory_kwargs = {\\'device\\': device, \\'dtype\\': dtype}\\n        super().__init__(embed_dim, block_loc, kwarg_all)\\n        \\n        out_features = out_features if out_features is not None else embed_dim\\n        hidden_features = hidden_features if hidden_features is not None else int(8 * embed_dim / 3)\\n        hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of\\n        \\n        self.fc1 = nn.Linear(embed_dim, hidden_features, bias=bias, **self.factory_kwargs)\\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **self.factory_kwargs)\\n        \\n        self.gate_network = nn.Sequential(\\n            nn.Linear(hidden_features, 64, bias=False, **self.factory_kwargs),\\n            nn.ReLU(),\\n            nn.Linear(64, num_activations, bias=False, **self.factory_kwargs)\\n        )\\n        \\n        self.activations = [F.relu, F.tanh, F.gelu]\\n        self.sparsity_factor = sparsity_factor\\n\\n    def _forward(self, X, **Z):\\n        hidden = self.fc1(X)\\n        gate_weights = self.gate_network(hidden)\\n        \\n        # Apply softmax and sp', response_metadata={'id': 'msg_01H8nYRncC39P9QXguANNyho', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'max_tokens', 'stop_sequence': None, 'usage': {'input_tokens': 5350, 'output_tokens': 1023}}, id='run-af3a40be-25a6-4d67-90f6-a192a34b24bd-0', usage_metadata={'input_tokens': 5350, 'output_tokens': 1023, 'total_tokens': 6373})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.prompts.prompts as P\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DESIGNER_SYSTEM=P.GUE_DESIGNER_SYSTEM(GAB_BASE=GAB_BASE,GAU_BASE=GAU_BASE,GAU_TEMPLATE=GAU_TEMPLATE)\n",
    "\n",
    "messages = [\n",
    "    (\"system\",DESIGNER_SYSTEM),\n",
    "    (\"user\",TEST_INIT_DESIGN_INPUT)\n",
    "]\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n",
    "# structured_llm = model.with_structured_output(P.GU_IMPLEMENTATION_REFINE_format)\n",
    "res=model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reflection on Feedback:\n",
      "The reviewer's suggestions highlight the need for efficiency optimization, empirical validation, and thorough documentation. I'll address these points in the refined design.\n",
      "\n",
      "New Analysis and Design:\n",
      "\n",
      "The current GatedMLP design introduces a dynamic gating mechanism that allows the model to select between multiple activation functions. While this approach enhances adaptability, it may introduce computational overhead. To address this, we'll implement a more efficient gating mechanism that maintains the benefits of dynamic activation selection while reducing computational costs.\n",
      "\n",
      "Updated Design Direction:\n",
      "1. Efficient Gating: Instead of applying all activation functions and then combining them, we'll use a lightweight gating network to predict weights for each activation function. This approach reduces the number of computations while still allowing dynamic selection.\n",
      "\n",
      "2. Sparse Activation: We'll introduce a sparsity factor in the gating mechanism, allowing the model to focus on the most relevant activation functions for each input.\n",
      "\n",
      "3. Adaptive Computation: We'll implement a mechanism to dynamically adjust the number of active gates based on the input complexity, further optimizing computational resources.\n",
      "\n",
      "High-level Pseudocode:\n",
      "```\n",
      "function GatedMLP(X):\n",
      "    hidden = linear_projection(X)\n",
      "    gate_weights = lightweight_gate_network(hidden)\n",
      "    sparse_gate_weights = apply_sparsity(gate_weights)\n",
      "    activated = sum(sparse_gate_weights[i] * activation[i](hidden) for i in range(num_activations))\n",
      "    output = linear_projection(activated)\n",
      "    return output\n",
      "```\n",
      "\n",
      "Updated GAU Document:\n",
      "The GatedMLP unit now incorporates an efficient dynamic gating mechanism that selects between multiple activation functions. Key features include:\n",
      "- Lightweight gating network for predicting activation weights\n",
      "- Sparse activation selection to focus on the most relevant functions\n",
      "- Adaptive computation based on input complexity\n",
      "- Maintains the same input-output interface as the original GatedMLP\n",
      "\n",
      "This design aims to balance expressiveness and computational efficiency, allowing the model to adapt to various input patterns while managing resource usage effectively.\n",
      "\n",
      "Implementation:\n",
      "\n",
      "```python\n",
      "# gau.py\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
      "\n",
      "class GatedMLP(GAUBase):\n",
      "    \"\"\"\n",
      "    Generalized Autoregressive Block Unit with efficient dynamic gating\n",
      "    Input:  X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
      "    Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of new intermediate variables to update the current Z}\n",
      "    \"\"\"\n",
      "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
      "                 device=None, dtype=None, hidden_features=None, out_features=None,\n",
      "                 bias=False, multiple_of=128, num_activations=3, sparsity_factor=0.5, **kwargs):\n",
      "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "        \n",
      "        out_features = out_features if out_features is not None else embed_dim\n",
      "        hidden_features = hidden_features if hidden_features is not None else int(8 * embed_dim / 3)\n",
      "        hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of\n",
      "        \n",
      "        self.fc1 = nn.Linear(embed_dim, hidden_features, bias=bias, **self.factory_kwargs)\n",
      "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **self.factory_kwargs)\n",
      "        \n",
      "        self.gate_network = nn.Sequential(\n",
      "            nn.Linear(hidden_features, 64, bias=False, **self.factory_kwargs),\n",
      "            nn.ReLU(),\n",
      "            nn.Linear(64, num_activations, bias=False, **self.factory_kwargs)\n",
      "        )\n",
      "        \n",
      "        self.activations = [F.relu, F.tanh, F.gelu]\n",
      "        self.sparsity_factor = sparsity_factor\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        hidden = self.fc1(X)\n",
      "        gate_weights = self.gate_network(hidden)\n",
      "        \n",
      "        # Apply softmax and sp\n"
     ]
    }
   ],
   "source": [
    "content='Reflection on Feedback:\\nThe reviewer\\'s suggestions highlight the need for efficiency optimization, empirical validation, and thorough documentation. I\\'ll address these points in the refined design.\\n\\nNew Analysis and Design:\\n\\nThe current GatedMLP design introduces a dynamic gating mechanism that allows the model to select between multiple activation functions. While this approach enhances adaptability, it may introduce computational overhead. To address this, we\\'ll implement a more efficient gating mechanism that maintains the benefits of dynamic activation selection while reducing computational costs.\\n\\nUpdated Design Direction:\\n1. Efficient Gating: Instead of applying all activation functions and then combining them, we\\'ll use a lightweight gating network to predict weights for each activation function. This approach reduces the number of computations while still allowing dynamic selection.\\n\\n2. Sparse Activation: We\\'ll introduce a sparsity factor in the gating mechanism, allowing the model to focus on the most relevant activation functions for each input.\\n\\n3. Adaptive Computation: We\\'ll implement a mechanism to dynamically adjust the number of active gates based on the input complexity, further optimizing computational resources.\\n\\nHigh-level Pseudocode:\\n```\\nfunction GatedMLP(X):\\n    hidden = linear_projection(X)\\n    gate_weights = lightweight_gate_network(hidden)\\n    sparse_gate_weights = apply_sparsity(gate_weights)\\n    activated = sum(sparse_gate_weights[i] * activation[i](hidden) for i in range(num_activations))\\n    output = linear_projection(activated)\\n    return output\\n```\\n\\nUpdated GAU Document:\\nThe GatedMLP unit now incorporates an efficient dynamic gating mechanism that selects between multiple activation functions. Key features include:\\n- Lightweight gating network for predicting activation weights\\n- Sparse activation selection to focus on the most relevant functions\\n- Adaptive computation based on input complexity\\n- Maintains the same input-output interface as the original GatedMLP\\n\\nThis design aims to balance expressiveness and computational efficiency, allowing the model to adapt to various input patterns while managing resource usage effectively.\\n\\nImplementation:\\n\\n```python\\n# gau.py\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test\\n\\nclass GatedMLP(GAUBase):\\n    \"\"\"\\n    Generalized Autoregressive Block Unit with efficient dynamic gating\\n    Input:  X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \\n    Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of new intermediate variables to update the current Z}\\n    \"\"\"\\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\\n                 device=None, dtype=None, hidden_features=None, out_features=None,\\n                 bias=False, multiple_of=128, num_activations=3, sparsity_factor=0.5, **kwargs):\\n        self.factory_kwargs = {\\'device\\': device, \\'dtype\\': dtype}\\n        super().__init__(embed_dim, block_loc, kwarg_all)\\n        \\n        out_features = out_features if out_features is not None else embed_dim\\n        hidden_features = hidden_features if hidden_features is not None else int(8 * embed_dim / 3)\\n        hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of\\n        \\n        self.fc1 = nn.Linear(embed_dim, hidden_features, bias=bias, **self.factory_kwargs)\\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **self.factory_kwargs)\\n        \\n        self.gate_network = nn.Sequential(\\n            nn.Linear(hidden_features, 64, bias=False, **self.factory_kwargs),\\n            nn.ReLU(),\\n            nn.Linear(64, num_activations, bias=False, **self.factory_kwargs)\\n        )\\n        \\n        self.activations = [F.relu, F.tanh, F.gelu]\\n        self.sparsity_factor = sparsity_factor\\n\\n    def _forward(self, X, **Z):\\n        hidden = self.fc1(X)\\n        gate_weights = self.gate_network(hidden)\\n        \\n        # Apply softmax and sp'\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from core library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import importlib\n",
    "import model_discovery.model.composer \n",
    "# importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUBase, GAUTree, check_tree_name, GABComposer\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "import model_discovery.utils as U\n",
    "\n",
    "# dir='/home/junyanc/model_discovery/model_discovery/model/library/core/gpt2/units'\n",
    "dir='C://ChengJunyan1//Research//model_discovery//model_discovery//model//library//core//gpt2//units'\n",
    "tree=GAUTree.load_from_base(dir)\n",
    "\n",
    "print(tree.view()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "gam_prompt_path = 'C://ChengJunyan1//Research//model_discovery//model_discovery//agents//prompts//gam_prompt.py'\n",
    "gau_template_path = 'C://ChengJunyan1//Research//model_discovery//model_discovery//agents//prompts//gau_template.py'\n",
    "GAM_TEMPLATE=open(gam_prompt_path).read()\n",
    "GAU_TEMPLATE=open(gau_template_path).read()\n",
    "\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n",
    "GAB_BASE=inspect.getsource(GABBase)\n",
    "GAB_COMPOSER=inspect.getsource(GABComposer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GAM_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
