{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place for experimenting the progressive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.utils as U\n",
    "from model_discovery.configs.gam_config import GAMConfig, GAMConfig_14M\n",
    "from model_discovery.model.composer import GAUTree,GAUBase,GAUNode\n",
    "# from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "\n",
    "prompts_dir='../model_discovery/agents/prompts/'\n",
    "gab_py = U.read_file(U.pjoin(prompts_dir,'gab_template.py'))\n",
    "gam_py = U.read_file(U.pjoin(prompts_dir,'gam_prompt.py'))\n",
    "GAU_TEMPLATE = U.read_file(U.pjoin(prompts_dir,'gau_template.py'))\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os,sys\n",
    "import importlib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "import model_discovery.agents.prompts.prompts as P\n",
    "import model_discovery.model.composer\n",
    "importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUTree,GAUBase,GAUNode\n",
    "import model_discovery.utils\n",
    "importlib.reload(model_discovery.utils)\n",
    "import model_discovery.utils as U\n",
    "\n",
    "\n",
    "\n",
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "# Placeholder classes for future implementation\n",
    "class MemoryAccessUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, memory_size, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "class DownsamplingUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, downsample_factor, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "class GAU(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
    "        # argv: list of hyperparameters\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\n",
    "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        self.a1 = kwarg_all.get('a1', 1)\n",
    "        self.a2: int = kwarg_all.get('a2', 2)\n",
    "\n",
    "        # COMPLETING THE CODE HERE #\n",
    "        self.token_scorer = TokenScoringGAU(embed_dim, **factory_kwargs)\n",
    "        self.dual_path: GAUBase = DualPathGAU(embed_dim, **factory_kwargs)\n",
    "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim, **factory_kwargs)\n",
    "\n",
    "        self.stages: nn.ModuleList = nn.ModuleList([CompressionStageGAU(embed_dim, device=\n",
    "            device, dtype=dtype, **kwargs) for _ in range(2)])\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU(),\n",
    "            'gau3': CompressionStageGAU()\n",
    "        })\n",
    "\n",
    "        self.model2 = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('gau2', DualPathGAU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "\n",
    "        # THE CODE HERE MUST BE COMPLETED #\n",
    "        # Step 1: Score tokens\n",
    "        X, Z = self.token_scorer(X, **Z)\n",
    "        # Step 2: Route through dual paths\n",
    "        # Step 3: Apply latent attention\n",
    "        Y, Z = self.latent_attention(X, **Z)\n",
    "        X, Z = self.dual_path(X, **Z)\n",
    "\n",
    "        return Y, Z\n",
    "\n",
    "@gau_test\n",
    "def test1(a=1,device=None):\n",
    "    print('test1')\n",
    "\n",
    "    \n",
    "@gau_test\n",
    "def test2(device=None):\n",
    "    print('test2')\n",
    "\n",
    "'''\n",
    "children = ['TokenScoringGAU', 'DualPathGAU', 'LatentAttentionGAU','CompressionStageGAU'] # solve redundancy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "class RelevanceScoringGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT KEEP THE ORIGINAL ONES #\n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        # Initialize a linear projection layer\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim // 2, **self.factory_kwargs)\n",
    "        self.projection.weight.requires_grad = True\n",
    "        self.projection.bias.requires_grad = True\n",
    "\n",
    "        # Initialize a learnable query vector\n",
    "        self.query_vector = nn.Parameter(torch.randn(embed_dim // 2, requires_grad=True, **self.factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        # Project the input embeddings\n",
    "        projected_X = self.projection(X)\n",
    "\n",
    "        # Compute similarity scores\n",
    "        similarity_scores = torch.matmul(projected_X, self.query_vector)\n",
    "\n",
    "        # Normalize the similarity scores\n",
    "        relevance_scores = F.softmax(similarity_scores, dim=-1)\n",
    "\n",
    "        # Weight the input embeddings with relevance scores\n",
    "        Y = X * relevance_scores.unsqueeze(-1)\n",
    "\n",
    "        # Return weighted embeddings and updated Z\n",
    "        return Y, {}\n",
    "\n",
    "\n",
    "# WRITE YOUR UNIT TEST FUNCTIONS HERE #\n",
    "\n",
    "@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON'T BE RECOGNIZED AS A UNIT TEST #\n",
    "def test_relevance_scoring_gau(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\n",
    "    # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\n",
    "\n",
    "    # Initialize the GAU\n",
    "    embed_dim = 512\n",
    "    block_loc = (0, 6)\n",
    "    relevance_scoring_gau = RelevanceScoringGAU(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={}, device=device, dtype=dtype)\n",
    "\n",
    "    # Mock input\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "\n",
    "    # Forward pass\n",
    "    Y, Z_ = relevance_scoring_gau(X, **Z)\n",
    "\n",
    "    # Assertions\n",
    "    assert Y.shape == (batch_size, seq_len, embed_dim), \"Output shape mismatch\"\n",
    "    assert isinstance(Z_, dict), \"Z_ should be a dictionary\"\n",
    "\n",
    "    # Print for debugging\n",
    "    print(\"Output embeddings:\", Y)\n",
    "    print(\"Updated Z:\", Z_)\n",
    "\n",
    "    # Additional checks\n",
    "    assert torch.allclose(Y.sum(dim=-1), X.sum(dim=-1), atol=1e-6), \"Weighted embeddings should preserve the sum along the embedding dimension\"\n",
    "\n",
    "    print(\"RelevanceScoringGAU test passed!\")\n",
    "'''\n",
    "children=['ProjectionGAU','SimilarityComputationGAU','NormalizationGAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Attribute' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m unit_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXAU\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Provide the unit_name to rename GAU class\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m reformatted_code, new_args, gau_tests, errors, warnings \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_and_reformat_gau_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReformatted Code:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m reformatted_code)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErrors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, errors)\n",
      "File \u001b[1;32mc:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\flow\\gau_utils.py:552\u001b[0m, in \u001b[0;36mcheck_and_reformat_gau_code\u001b[1;34m(source_code, unit_name, children)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# Step 4: Run the InitChecker\u001b[39;00m\n\u001b[0;32m    551\u001b[0m init_checker \u001b[38;5;241m=\u001b[39m InitChecker(unit_name)\n\u001b[1;32m--> 552\u001b[0m \u001b[43minit_checker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;66;03m# Step 5: Run the AttributeChecker\u001b[39;00m\n\u001b[0;32m    555\u001b[0m attribute_checker \u001b[38;5;241m=\u001b[39m AttributeChecker(unit_name, children)\n",
      "File \u001b[1;32mc:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\ast.py:407\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    405\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    406\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit)\n\u001b[1;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\ast.py:415\u001b[0m, in \u001b[0;36mNodeVisitor.generic_visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m value:\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, AST):\n\u001b[1;32m--> 415\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, AST):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(value)\n",
      "File \u001b[1;32mc:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\ast.py:407\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    405\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    406\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit)\n\u001b[1;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\flow\\gau_utils.py:154\u001b[0m, in \u001b[0;36mInitChecker.visit_ClassDef\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_name:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minside_gau_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Start processing only when inside GAU class\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneric_visit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minside_gau_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Reset after processing GAU class\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Skip other classes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\ast.py:415\u001b[0m, in \u001b[0;36mNodeVisitor.generic_visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m value:\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, AST):\n\u001b[1;32m--> 415\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, AST):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(value)\n",
      "File \u001b[1;32mc:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\ast.py:407\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    405\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    406\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit)\n\u001b[1;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\flow\\gau_utils.py:171\u001b[0m, in \u001b[0;36mInitChecker.visit_FunctionDef\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stmt, ast\u001b[38;5;241m.\u001b[39mAssign) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stmt\u001b[38;5;241m.\u001b[39mtargets[\u001b[38;5;241m0\u001b[39m], ast\u001b[38;5;241m.\u001b[39mAttribute):\n\u001b[0;32m    170\u001b[0m     target \u001b[38;5;241m=\u001b[39m stmt\u001b[38;5;241m.\u001b[39mtargets[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m target\u001b[38;5;241m.\u001b[39mattr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactory_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    172\u001b[0m         factory_kwargs_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Attribute' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "\n",
    "unit_name = \"XAU\"  # Provide the unit_name to rename GAU class\n",
    "reformatted_code, new_args, gau_tests, errors, warnings = check_and_reformat_gau_code(code, unit_name,children)\n",
    "print(\"Reformatted Code:\\n\" + reformatted_code)\n",
    "print(\"Errors:\\n\", errors)\n",
    "print(\"Warnings:\\n\", warnings)\n",
    "print(\"New Arguments:\\n\", new_args)\n",
    "\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "spec=P.UnitSpec(\n",
    "   unitname = unit_name,\n",
    "   document = 'Test',\n",
    "   inputs = [],\n",
    "   outputs = [],\n",
    ")\n",
    "test_tree.add_unit(\n",
    "    spec, reformatted_code, new_args, None, None, None, children, gau_tests, None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 1: import torch\n",
      "line 2: import torch.nn as nn\n",
      "line 3: from model_discovery.model.utils.modules import GAUBase, gau_test\n",
      "line 4: import torch.nn.functional as F\n",
      "line 5: \n",
      "line 6: \n",
      "line 7: class MemoryAccessUnit(nn.Module):\n",
      "line 8: \n",
      "line 9:     def __init__(self, embed_dim, memory_size, device=None, dtype=None):\n",
      "line 10:         super().__init__(embed_dim)\n",
      "line 11: \n",
      "line 12:     def _forward(self, X, **Z):\n",
      "line 13:         return X, {}\n",
      "line 14: \n",
      "line 15: \n",
      "line 16: class DownsamplingUnit(nn.Module):\n",
      "line 17: \n",
      "line 18:     def __init__(self, embed_dim, downsample_factor, device=None, dtype=None):\n",
      "line 19:         super().__init__(embed_dim)\n",
      "line 20: \n",
      "line 21:     def _forward(self, X, **Z):\n",
      "line 22:         return X, {}\n",
      "line 23: \n",
      "line 24: \n",
      "line 25: class XAU(GAUBase):\n",
      "line 26:     \"\"\"Generalized Autoregressive Block Unit\n",
      "line 27:         Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "line 28:         Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "line 29:         Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "line 30:     \"\"\"\n",
      "line 31: \n",
      "line 32:     def __init__(self, embed_dim: int, block_loc, kwarg_all, device=None,\n",
      "line 33:         dtype=None, a1=1, a2=2, **kwargs):\n",
      "line 34:         self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "line 35:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 36:         factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "line 37:         self.token_scorer = TokenScoringGAU(embed_dim=embed_dim, block_loc=\n",
      "line 38:             block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all)\n",
      "line 39:         self.dual_path: GAUBase = DualPathGAU(embed_dim=embed_dim,\n",
      "line 40:             block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
      "line 41:             **kwarg_all)\n",
      "line 42:         self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim=\n",
      "line 43:             embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **\n",
      "line 44:             self.factory_kwargs, **kwarg_all)\n",
      "line 45:         self.stages: nn.ModuleList = nn.ModuleList([CompressionStageGAU(\n",
      "line 46:             embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all,\n",
      "line 47:             **self.factory_kwargs, **kwarg_all) for _ in range(2)])\n",
      "line 48:         self.model = nn.Sequential(nn.Conv2d(1, 20, 5), nn.ReLU(),\n",
      "line 49:             LatentAttentionGAU(), nn.Conv2d(20, 64, 5), nn.ReLU(), nn.\n",
      "line 50:             Sequential(OrderedDict([('conv1', nn.Conv2d(1, 20, 5)), (\n",
      "line 51:             'relu1', nn.ReLU()), ('gau2', DualPathGAU()), ('conv2', nn.\n",
      "line 52:             Conv2d(20, 64, 5)), ('relu2', nn.ReLU())])))\n",
      "line 53:         self.activations = nn.ModuleDict({'relu': nn.ReLU(), 'lrelu': nn.\n",
      "line 54:             LeakyReLU(), 'gau3': CompressionStageGAU(embed_dim=embed_dim,\n",
      "line 55:             block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
      "line 56:             **kwarg_all)})\n",
      "line 57:         self.model2 = nn.Sequential(OrderedDict([('conv1', nn.Conv2d(1, 20,\n",
      "line 58:             5)), ('relu1', nn.ReLU()), ('gau2', DualPathGAU()), ('conv2',\n",
      "line 59:             nn.Conv2d(20, 64, 5)), ('relu2', nn.ReLU())]))\n",
      "line 60:         self.a1 = a1\n",
      "line 61:         self.a2: int = a2\n",
      "line 62: \n",
      "line 63:     def _forward(self, X, **Z):\n",
      "line 64:         X, Z = self.token_scorer(X, **Z)\n",
      "line 65:         Y, Z = self.latent_attention(X, **Z)\n",
      "line 66:         X, Z = self.dual_path(X, **Z)\n",
      "line 67:         return Y, Z\n",
      "line 68: \n",
      "line 69: \n",
      "line 70: class CompressionStageGAU(GAUBase): \n",
      "line 71:     def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
      "line 72:         self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "line 73:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 74:         \n",
      "line 75:     def _forward(self, X, **Z): \n",
      "line 76:         return X\n",
      "line 77: \n",
      "line 78: \n",
      "line 79: class DualPathGAU(GAUBase): \n",
      "line 80:     def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
      "line 81:         self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "line 82:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 83:         \n",
      "line 84:     def _forward(self, X, **Z): \n",
      "line 85:         return X\n",
      "line 86: \n",
      "line 87: \n",
      "line 88: class TokenScoringGAU(GAUBase): \n",
      "line 89:     def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
      "line 90:         self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "line 91:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 92:         \n",
      "line 93:     def _forward(self, X, **Z): \n",
      "line 94:         return X\n",
      "line 95: \n",
      "line 96: \n",
      "line 97: class LatentAttentionGAU(GAUBase): \n",
      "line 98:     def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
      "line 99:         self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "line 100:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 101:         \n",
      "line 102:     def _forward(self, X, **Z): \n",
      "line 103:         return X\n",
      "line 104: \n",
      "line 105: \n",
      "line 106: @gau_test\n",
      "line 107: def test_XAU_test1(a=1, device=None, dtype=None):\n",
      "line 108:     print('test1')\n",
      "line 109: \n",
      "line 110: \n",
      "line 111: @gau_test\n",
      "line 112: def test_XAU_test2(device=None, dtype=None):\n",
      "line 113:     print('test2')\n",
      "line 114: \n",
      "line 115: \n",
      "line 116: def run_XAU_tests():\n",
      "line 117: \ttry:\n",
      "line 118: \t\ttest_XAU_test1()\n",
      "line 119: \texcept Exception as e:\n",
      "line 120: \t\tprint(\"Error in running test1:\", e)\n",
      "line 121: \ttry:\n",
      "line 122: \t\ttest_XAU_test2()\n",
      "line 123: \texcept Exception as e:\n",
      "line 124: \t\tprint(\"Error in running test2:\", e)\n",
      "line 125: \n",
      "line 126: \n",
      "line 127: if __name__ == \"__main__\":\n",
      "line 128: \trun_XAU_tests()\n"
     ]
    }
   ],
   "source": [
    "code=test_tree.compose_unit('XAU')\n",
    "code=U.add_line_num(code)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class DilatedReformerRootGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.dilated_attention = DilatedAttentionGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.reversible_residual = ReversibleResidualGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **\n",
    "            self.factory_kwargs, **kwarg_all)\n",
    "        self.lsh_attention = LSHAttentionGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.dilated_attention(X, **Z)\n",
    "        X, Z = self.reversible_residual(X, **Z)\n",
    "        X, Z = self.lsh_attention(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class LSHAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class DilatedAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class ReversibleResidualGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "@gau_test\n",
    "def test_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau(device=\n",
    "    None, dtype=None) ->None:\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    embed_dim = 16\n",
    "    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "    gau = DilatedReformerRootGAU(embed_dim=embed_dim, block_loc=(0, 6), kwarg_all={}, device=\n",
    "        device, dtype=dtype)\n",
    "    Y, Z_ = gau(X, **Z)\n",
    "    assert Y.shape == X.shape, 'Output shape should match input shape'\n",
    "    assert isinstance(Z_, dict), 'Z_ should be a dictionary'\n",
    "    print('Input X:', X)\n",
    "    print('Output Y:', Y)\n",
    "    print('Intermediate variables Z_:', Z_)\n",
    "\n",
    "\n",
    "def run_DilatedReformerRootGAU_tests():\n",
    "\ttry:\n",
    "\t\ttest_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"Error in running unit_test_dilated_reformer_root_gau:\", e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\trun_DilatedReformerRootGAU_tests()\n",
    "'''\n",
    "\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "output=io.StringIO()\n",
    "with redirect_stdout(output):\n",
    "    exec(code)\n",
    "captured = str(output.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Input X: tensor([[[-2.3012e-01,  2.3788e+00, -4.7254e-01, -1.9962e+00,  8.4011e-01,\\n          -1.3151e+00, -7.6405e-01, -4.4511e-01,  1.2063e+00, -7.1245e-01,\\n          -4.3916e-01,  1.1784e+00, -1.7686e-01,  1.0315e-01,  1.3303e+00,\\n           3.2917e-01],\\n         [ 2.8818e-01, -1.5537e-01, -1.3801e+00, -1.4679e+00,  1.4772e+00,\\n           1.6958e+00,  1.3572e+00, -4.7153e-01, -1.7442e+00, -6.8832e-01,\\n           6.4387e-01, -1.7276e+00,  1.9131e+00, -2.7023e-01, -1.6691e+00,\\n          -5.1265e-01],\\n         [ 1.0191e+00, -5.3503e-01,  1.0546e+00,  3.6731e+00,  1.0722e+00,\\n           1.7628e-02, -1.0685e+00, -2.2400e-01,  9.5643e-01,  1.6189e+00,\\n           2.5520e-01,  8.3835e-01,  7.8880e-01,  3.5786e-02, -6.9103e-01,\\n          -6.8120e-01],\\n         [-8.3853e-01,  1.4766e-01, -1.7785e+00, -9.1734e-01,  2.0629e+00,\\n           1.7541e-01, -1.0523e+00,  1.0929e+00,  1.2211e+00,  4.3829e-02,\\n          -1.3583e+00,  6.1974e-01,  1.3561e+00, -8.1625e-01,  7.7795e-01,\\n          -8.4910e-01],\\n         [-2.9532e-01, -1.6850e-01,  4.6396e-01,  3.9504e-01, -3.0020e-01,\\n          -9.0898e-01, -7.5458e-01, -4.6768e-01, -1.2290e+00,  1.0436e+00,\\n          -1.0790e+00, -2.1275e+00,  1.9250e+00,  3.2639e-01, -1.5225e+00,\\n          -2.3875e-01],\\n         [-2.8734e-01, -7.6125e-01,  6.6165e-01,  1.9117e-01, -1.3853e+00,\\n          -6.5434e-01,  1.2841e+00, -3.8892e-01, -3.6391e-02, -5.6071e-01,\\n           1.0487e+00,  5.2577e-01, -8.7046e-02, -1.9185e+00, -2.7701e-01,\\n          -5.4837e-01],\\n         [-1.8271e+00, -2.2989e-01, -4.1457e-02, -3.8908e-01, -1.1506e+00,\\n          -1.7726e+00,  7.0311e-01, -8.8569e-01,  2.3374e-01, -7.2847e-01,\\n          -1.3759e+00,  9.9713e-01, -1.1464e+00,  1.4911e+00, -6.3035e-01,\\n          -1.3787e+00],\\n         [-1.9567e-01, -6.7656e-01, -4.1577e-01, -7.9093e-01,  1.0846e+00,\\n           1.2831e+00,  4.3176e-01, -1.2018e+00, -3.9761e-01,  1.0631e+00,\\n           3.5137e-01, -1.1726e+00, -1.7690e+00, -6.9135e-01,  1.5616e+00,\\n           4.3905e-01],\\n         [ 1.5721e+00,  8.7833e-02, -1.7725e-01,  1.5154e-01,  2.5378e-01,\\n           1.0671e+00,  8.5926e-01, -7.1906e-01,  3.0424e+00,  9.4212e-01,\\n           2.8739e-01, -4.2988e-01,  8.2752e-01,  6.5587e-02,  1.2562e+00,\\n          -1.6802e+00],\\n         [ 7.3228e-01, -8.1188e-01,  9.7737e-01,  1.6016e-01, -9.4219e-01,\\n           7.5512e-01,  8.8544e-01, -4.1118e-01, -6.9975e-01,  2.2230e-01,\\n          -1.2710e+00,  6.1737e-01, -7.4645e-01,  5.2663e-01, -8.4917e-01,\\n           2.4869e-03]],\\n\\n        [[-6.4365e-01, -5.9676e-01, -2.3407e+00, -1.5412e-01, -1.7030e-01,\\n           1.9732e+00, -7.4000e-01,  2.4362e-01, -1.8887e-01, -3.1853e-01,\\n          -1.4202e+00, -2.0449e-02,  4.4784e-01,  4.5520e-03,  1.0186e+00,\\n           3.2213e-02],\\n         [ 1.2003e+00, -1.0516e+00, -3.0509e-01,  3.9276e-02, -6.7694e-01,\\n           1.1425e+00, -2.0662e-01, -1.6321e-01,  1.7266e+00,  2.0474e+00,\\n          -3.6312e-01, -2.6017e+00, -1.1199e+00, -8.0636e-01, -1.6534e+00,\\n           2.5151e-01],\\n         [-4.8930e-01, -6.3702e-01, -1.3206e+00,  1.3525e+00, -9.4968e-01,\\n          -9.8064e-01,  1.5565e+00,  6.3228e-01, -8.1683e-01,  8.2618e-01,\\n           5.4623e-01, -8.3067e-01,  4.3650e-01, -1.1956e+00,  1.9162e+00,\\n          -4.5455e-01],\\n         [ 1.1713e+00, -4.0826e-01, -1.2673e+00, -5.6142e-03, -1.6909e+00,\\n          -1.1453e-01,  6.4751e-01, -6.9049e-01,  1.2243e+00, -2.9153e-01,\\n           8.1508e-01, -6.0720e-01, -1.3485e+00,  2.2795e+00,  1.4592e+00,\\n           4.4274e-01],\\n         [-4.0948e-01, -3.4183e-01,  2.1905e+00,  1.0580e+00, -1.3685e-01,\\n           7.1237e-01,  3.7976e-01, -1.9973e-01,  1.6704e+00,  9.0320e-02,\\n          -1.1018e-01,  2.6214e-01, -1.4356e-01, -8.2408e-01,  4.7039e-02,\\n          -1.9377e+00],\\n         [ 3.7131e-02, -1.6384e+00, -3.9555e-01,  9.3109e-02, -1.1891e+00,\\n          -3.1702e-01, -3.6400e-01, -2.6217e-01,  2.5587e-01, -1.6727e+00,\\n           6.7207e-01,  1.4594e-01, -1.5137e+00, -1.0944e+00,  2.1928e-01,\\n          -1.9515e-02],\\n         [ 5.0375e-02,  9.6348e-01,  1.4848e+00,  9.8551e-01,  7.0022e-01,\\n           1.4012e+00, -1.1533e+00, -1.6070e+00,  6.7859e-01, -7.7775e-01,\\n           5.3878e-01,  3.4698e-02,  1.5986e+00,  1.8547e+00, -6.3990e-01,\\n           1.8998e+00],\\n         [-3.7459e-01, -9.4590e-01,  8.2438e-01, -5.7372e-01, -2.3196e-01,\\n           7.8973e-01,  1.1412e-01, -2.3676e+00, -1.4919e-01,  1.4760e+00,\\n          -2.1196e-02,  1.2369e-03, -1.0607e+00, -2.2453e-01, -1.4187e+00,\\n           2.6583e-01],\\n         [-1.0107e-01, -6.3206e-01, -4.1987e-01, -8.7557e-01, -1.3291e-01,\\n          -3.4582e-01, -2.1534e+00, -2.5991e-01, -6.3077e-01, -4.7854e-01,\\n           7.5332e-01,  4.2013e-01,  5.6524e-01, -1.3121e+00, -4.6274e-01,\\n          -2.2559e-01],\\n         [ 1.1817e+00, -1.0153e+00, -7.1745e-01, -5.6217e-01,  1.1542e+00,\\n          -3.2691e-01, -1.4837e-01, -7.1832e-01, -9.6574e-02, -1.3602e+00,\\n           8.5652e-01, -1.0386e+00,  1.0605e+00,  5.9364e-01,  8.5413e-01,\\n          -3.1080e-01]]])\\nOutput Y: tensor([[[-2.3012e-01,  2.3788e+00, -4.7254e-01, -1.9962e+00,  8.4011e-01,\\n          -1.3151e+00, -7.6405e-01, -4.4511e-01,  1.2063e+00, -7.1245e-01,\\n          -4.3916e-01,  1.1784e+00, -1.7686e-01,  1.0315e-01,  1.3303e+00,\\n           3.2917e-01],\\n         [ 2.8818e-01, -1.5537e-01, -1.3801e+00, -1.4679e+00,  1.4772e+00,\\n           1.6958e+00,  1.3572e+00, -4.7153e-01, -1.7442e+00, -6.8832e-01,\\n           6.4387e-01, -1.7276e+00,  1.9131e+00, -2.7023e-01, -1.6691e+00,\\n          -5.1265e-01],\\n         [ 1.0191e+00, -5.3503e-01,  1.0546e+00,  3.6731e+00,  1.0722e+00,\\n           1.7628e-02, -1.0685e+00, -2.2400e-01,  9.5643e-01,  1.6189e+00,\\n           2.5520e-01,  8.3835e-01,  7.8880e-01,  3.5786e-02, -6.9103e-01,\\n          -6.8120e-01],\\n         [-8.3853e-01,  1.4766e-01, -1.7785e+00, -9.1734e-01,  2.0629e+00,\\n           1.7541e-01, -1.0523e+00,  1.0929e+00,  1.2211e+00,  4.3829e-02,\\n          -1.3583e+00,  6.1974e-01,  1.3561e+00, -8.1625e-01,  7.7795e-01,\\n          -8.4910e-01],\\n         [-2.9532e-01, -1.6850e-01,  4.6396e-01,  3.9504e-01, -3.0020e-01,\\n          -9.0898e-01, -7.5458e-01, -4.6768e-01, -1.2290e+00,  1.0436e+00,\\n          -1.0790e+00, -2.1275e+00,  1.9250e+00,  3.2639e-01, -1.5225e+00,\\n          -2.3875e-01],\\n         [-2.8734e-01, -7.6125e-01,  6.6165e-01,  1.9117e-01, -1.3853e+00,\\n          -6.5434e-01,  1.2841e+00, -3.8892e-01, -3.6391e-02, -5.6071e-01,\\n           1.0487e+00,  5.2577e-01, -8.7046e-02, -1.9185e+00, -2.7701e-01,\\n          -5.4837e-01],\\n         [-1.8271e+00, -2.2989e-01, -4.1457e-02, -3.8908e-01, -1.1506e+00,\\n          -1.7726e+00,  7.0311e-01, -8.8569e-01,  2.3374e-01, -7.2847e-01,\\n          -1.3759e+00,  9.9713e-01, -1.1464e+00,  1.4911e+00, -6.3035e-01,\\n          -1.3787e+00],\\n         [-1.9567e-01, -6.7656e-01, -4.1577e-01, -7.9093e-01,  1.0846e+00,\\n           1.2831e+00,  4.3176e-01, -1.2018e+00, -3.9761e-01,  1.0631e+00,\\n           3.5137e-01, -1.1726e+00, -1.7690e+00, -6.9135e-01,  1.5616e+00,\\n           4.3905e-01],\\n         [ 1.5721e+00,  8.7833e-02, -1.7725e-01,  1.5154e-01,  2.5378e-01,\\n           1.0671e+00,  8.5926e-01, -7.1906e-01,  3.0424e+00,  9.4212e-01,\\n           2.8739e-01, -4.2988e-01,  8.2752e-01,  6.5587e-02,  1.2562e+00,\\n          -1.6802e+00],\\n         [ 7.3228e-01, -8.1188e-01,  9.7737e-01,  1.6016e-01, -9.4219e-01,\\n           7.5512e-01,  8.8544e-01, -4.1118e-01, -6.9975e-01,  2.2230e-01,\\n          -1.2710e+00,  6.1737e-01, -7.4645e-01,  5.2663e-01, -8.4917e-01,\\n           2.4869e-03]],\\n\\n        [[-6.4365e-01, -5.9676e-01, -2.3407e+00, -1.5412e-01, -1.7030e-01,\\n           1.9732e+00, -7.4000e-01,  2.4362e-01, -1.8887e-01, -3.1853e-01,\\n          -1.4202e+00, -2.0449e-02,  4.4784e-01,  4.5520e-03,  1.0186e+00,\\n           3.2213e-02],\\n         [ 1.2003e+00, -1.0516e+00, -3.0509e-01,  3.9276e-02, -6.7694e-01,\\n           1.1425e+00, -2.0662e-01, -1.6321e-01,  1.7266e+00,  2.0474e+00,\\n          -3.6312e-01, -2.6017e+00, -1.1199e+00, -8.0636e-01, -1.6534e+00,\\n           2.5151e-01],\\n         [-4.8930e-01, -6.3702e-01, -1.3206e+00,  1.3525e+00, -9.4968e-01,\\n          -9.8064e-01,  1.5565e+00,  6.3228e-01, -8.1683e-01,  8.2618e-01,\\n           5.4623e-01, -8.3067e-01,  4.3650e-01, -1.1956e+00,  1.9162e+00,\\n          -4.5455e-01],\\n         [ 1.1713e+00, -4.0826e-01, -1.2673e+00, -5.6142e-03, -1.6909e+00,\\n          -1.1453e-01,  6.4751e-01, -6.9049e-01,  1.2243e+00, -2.9153e-01,\\n           8.1508e-01, -6.0720e-01, -1.3485e+00,  2.2795e+00,  1.4592e+00,\\n           4.4274e-01],\\n         [-4.0948e-01, -3.4183e-01,  2.1905e+00,  1.0580e+00, -1.3685e-01,\\n           7.1237e-01,  3.7976e-01, -1.9973e-01,  1.6704e+00,  9.0320e-02,\\n          -1.1018e-01,  2.6214e-01, -1.4356e-01, -8.2408e-01,  4.7039e-02,\\n          -1.9377e+00],\\n         [ 3.7131e-02, -1.6384e+00, -3.9555e-01,  9.3109e-02, -1.1891e+00,\\n          -3.1702e-01, -3.6400e-01, -2.6217e-01,  2.5587e-01, -1.6727e+00,\\n           6.7207e-01,  1.4594e-01, -1.5137e+00, -1.0944e+00,  2.1928e-01,\\n          -1.9515e-02],\\n         [ 5.0375e-02,  9.6348e-01,  1.4848e+00,  9.8551e-01,  7.0022e-01,\\n           1.4012e+00, -1.1533e+00, -1.6070e+00,  6.7859e-01, -7.7775e-01,\\n           5.3878e-01,  3.4698e-02,  1.5986e+00,  1.8547e+00, -6.3990e-01,\\n           1.8998e+00],\\n         [-3.7459e-01, -9.4590e-01,  8.2438e-01, -5.7372e-01, -2.3196e-01,\\n           7.8973e-01,  1.1412e-01, -2.3676e+00, -1.4919e-01,  1.4760e+00,\\n          -2.1196e-02,  1.2369e-03, -1.0607e+00, -2.2453e-01, -1.4187e+00,\\n           2.6583e-01],\\n         [-1.0107e-01, -6.3206e-01, -4.1987e-01, -8.7557e-01, -1.3291e-01,\\n          -3.4582e-01, -2.1534e+00, -2.5991e-01, -6.3077e-01, -4.7854e-01,\\n           7.5332e-01,  4.2013e-01,  5.6524e-01, -1.3121e+00, -4.6274e-01,\\n          -2.2559e-01],\\n         [ 1.1817e+00, -1.0153e+00, -7.1745e-01, -5.6217e-01,  1.1542e+00,\\n          -3.2691e-01, -1.4837e-01, -7.1832e-01, -9.6574e-02, -1.3602e+00,\\n           8.5652e-01, -1.0386e+00,  1.0605e+00,  5.9364e-01,  8.5413e-01,\\n          -3.1080e-01]]])\\nIntermediate variables Z_: {}\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "test2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "output=io.StringIO()\n",
    "with redirect_stdout(output):\n",
    "    exec(test_tree.compose_unit('XAU'))\n",
    "\n",
    "print(output.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "children={\n",
    "    'unit1': 'MockUnit1',\n",
    "    'unit2': 'MockUnit2',\n",
    "}\n",
    "called=['unit1', 'unit2', 'unit1']\n",
    "test_tree.units['DualPathGAU'] = GAUNode('DualPathGAU', None, None, None, called, None, None, children, None)\n",
    "children={\n",
    "    'unit1': 'MockUnit3',\n",
    "    'unit2': 'MockUnit4',\n",
    "}\n",
    "called=['unit2', 'unit2']\n",
    "test_tree.units['MockUnit1'] = GAUNode('MockUnit1', None, None, None, called, None, None, children, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DualPathGAU': GAUNode(name='DualPathGAU', code=None, args=None, desc=None, path=['unit1', 'unit2', 'unit1'], review=None, rating=None, children={'unit1': 'MockUnit1', 'unit2': 'MockUnit2'}, suggestions=None),\n",
       " 'MockUnit1': GAUNode(name='MockUnit1', code=None, args=None, desc=None, path=['unit2', 'unit2'], review=None, rating=None, children={'unit1': 'MockUnit3', 'unit2': 'MockUnit4'}, suggestions=None)}"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tree.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in A: 2 3\n",
      "Kwargs in A: {'a': 1, 'b': 2, 'c': 3}\n",
      "Kwargs in B: {'a': 1, 'b': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.B at 0x2dfbf677b30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class A:\n",
    "    def __init__(self,r,d,e,**Z):\n",
    "        print('Args in A:',d,e)\n",
    "        print('Kwargs in A:',Z)\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self,_Z,c=1,d=2,e=3,**Z):\n",
    "        args2={'d':d,'e':e}\n",
    "        super().__init__(c,**_Z,**args2)\n",
    "        print('Kwargs in B:',Z)\n",
    "\n",
    "cfg={\n",
    "    'a':1,\n",
    "    'b':2,\n",
    "    'c':3,\n",
    "}\n",
    "\n",
    "B(cfg,**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'a': 3}\n",
      "{'a': 3}\n"
     ]
    }
   ],
   "source": [
    "def A(X,Z,**kwargs):\n",
    "    print(X)\n",
    "    print(Z)\n",
    "    print(kwargs)\n",
    "\n",
    "def B(X,Z,a,**kwargs):\n",
    "    A(X,Z,**Z)\n",
    "\n",
    "Z={'a':3}\n",
    "B(1,Z,**Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "    '''\n",
    "    GAU Tree Map of Butterfly-AFT Generalized Autoregressive Block (BAGAB):\n",
    "    ButterflyAFTGAU (Rating: 3.5/5)\n",
    "        |- AFTMechanismGAU (Rating: 3.5/5)\n",
    "            |- PositionBiasGAU (Rating: 3.5/5)\n",
    "            |- ElementWiseOperationGAU (Rating: 3.5/5)\n",
    "        |- ButterflyTransformGAU (Rating: 3.5/5)\n",
    "            |- ButterflyLayerGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "            |- ButterflyInitializationGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "                |- ButterflyParameterGAU (Unimplemented)\n",
    "\n",
    "    Implemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ElementWiseOperationGAU, ButterflyAFTGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    Unimplemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ButterflyParameterGAU, ElementWiseOperationGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    '''\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ButterflyAFTGAU(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase\n",
    "\n",
    "\n",
    "class ButterflyAFTGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.aft_mechanism = AFTMechanismGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.butterfly_transform = ButterflyTransformGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        aft_output, Z = self.aft_mechanism(X, **Z)\n",
    "        butterfly_output, Z = self.butterfly_transform(aft_output, **Z)\n",
    "        return butterfly_output, Z\n",
    "\n",
    "\n",
    "class ButterflyTransformGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_layer = ButterflyLayerGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_init = ButterflyInitializationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.butterfly_init(X, **Z)\n",
    "        Y, Z = self.butterfly_layer(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ButterflyLayerGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.stages = nn.ModuleList([ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all) for _ in range(kwarg_all.get('num_stages', 4))])\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for stage in self.stages:\n",
    "            X, Z = stage(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "class ButterflyMatrixGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ButterflyInitializationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_size = kwarg_all.get('butterfly_size', embed_dim)\n",
    "        self.butterfly_matrices = nn.ParameterList([nn.Parameter(torch.\n",
    "            randn(self.butterfly_size, self.butterfly_size, **self.\n",
    "            factory_kwargs)) for _ in range(int(torch.log2(torch.tensor(\n",
    "            self.butterfly_size))))])\n",
    "        self.butterfly_stage = ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_parameter = ButterflyParameterGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for matrix in self.butterfly_matrices:\n",
    "            X = F.linear(X, matrix)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class ButterflyParameterGAU(GAUBase):\n",
    "    \"\"\"\n",
    "    Generalized Autoregressive Block Unit for learning parameters of Butterfly Factorization.\n",
    "    Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "    Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "    Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "    embed_dim:    The dimension of the input embeddings\n",
    "    block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "    kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.param_size = kwarg_all.get('param_size', 128)\n",
    "        self.butterfly_weights = nn.Parameter(torch.randn(self.param_size,\n",
    "            embed_dim, **self.factory_kwargs))\n",
    "        nn.init.xavier_uniform_(self.butterfly_weights)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X = torch.matmul(X, self.butterfly_weights)\n",
    "        Z_ = {'butterfly_weights': self.butterfly_weights}\n",
    "        return transformed_X, Z_\n",
    "        # transformed_X = F.linear(X, self.butterfly_weights.T)\n",
    "        # return transformed_X, {}\n",
    "\n",
    "\n",
    "class ButterflyComputationGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class ButterflyStageGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_params = ButterflyParameterGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X, Z = self.butterfly_params(X, **Z)\n",
    "        assert transformed_X.shape == X.shape, f'Output shape {transformed_X.shape} does not match input shape {X.shape}'\n",
    "        return transformed_X, Z\n",
    "\n",
    "class AFTMechanismGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = PositionBiasGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.element_wise_operation = ElementWiseOperationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.position_bias(X, **Z)\n",
    "        Y, Z = self.element_wise_operation(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ElementWiseOperationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, embed_dim, **self.\n",
    "            factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        queries = Z.get('queries', X)\n",
    "        keys = Z.get('keys', X)\n",
    "        values = Z.get('values', X)\n",
    "        elementwise_product = queries * keys\n",
    "        elementwise_sum = elementwise_product + values + self.position_bias\n",
    "        return elementwise_sum, {}\n",
    "\n",
    "\n",
    "class PositionBiasGAU(GAUBase):\n",
    "    \"\"\"Position Bias Generalized Autoregressive Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, 1, embed_dim, **\n",
    "            self.factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = X + self.position_bias\n",
    "        return Y, {}\n",
    "    \n",
    "\n",
    "class GABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100, **kwargs):\n",
    "        super().__init__()\n",
    "        self.gab = GAB(embed_dim, (0,1), **kwargs)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, Z = self.gab(X, **Z)\n",
    "        return self.lm_head(X),X\n",
    "\n",
    "\n",
    "\n",
    "gab_config = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 54495796.0\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 899675.4375\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 475023.78125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 810216.875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 777344.75\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 783032.0625\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 771978.3125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 62701.796875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 66148.203125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 67731.2734375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 69984.171875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 74028.8046875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 70948.921875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 78776.75\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 608838.3125\n",
      "Loss: 18159550.0\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 680764.9375\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 377442.28125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 609263.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 591532.1875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 592608.125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 558789.9375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 48893.484375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 53887.46875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 54524.15625\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 57871.140625\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 63347.60546875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 56995.46875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 57605.421875\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 362053.15625\n",
      "Loss: 2057852.75\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 380637.28125\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 183177.609375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 376077.53125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 359930.09375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 367729.90625\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 371797.71875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 24987.49609375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 30496.25\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 29164.060546875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 32006.0234375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 32426.490234375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 33316.421875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 33375.78125\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 196014.078125\n",
      "Loss: 0.0\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 0.0\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 0.0\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 0.0\n",
      "Loss: 0.0\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 0.0\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 0.0\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 0.0\n"
     ]
    }
   ],
   "source": [
    "class SimpleGABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100):\n",
    "        super().__init__()\n",
    "        self.butterfly_param = ButterflyParameterGAU(embed_dim, (0,1), {})\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, _ = self.butterfly_param(X, **Z)\n",
    "        return self.lm_head(X), X\n",
    "\n",
    "# Test the simplified model\n",
    "simple_model = GABLM(128)\n",
    "mock_input = torch.randn(1, 10, 128)\n",
    "mock_y = torch.randint(0, 100, (1, 10))\n",
    "\n",
    "optimizer = optim.Adam(simple_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for _ in range(5):  # Run for a few iterations\n",
    "    optimizer.zero_grad()\n",
    "    y, X = simple_model(mock_input)\n",
    "    loss = criterion(y.view(-1, 100), mock_y.view(-1))\n",
    "    loss.backward()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    for name, param in simple_model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"Gradient for {name}: {param.grad.abs().mean()}\")\n",
    "        else:\n",
    "            print(f\"No gradient for {name}\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Used parameter gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights requires gradients but has none.\n",
      "Error: Used parameter gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights requires gradients but has none.\n",
      "Differentiability test failed due to missing gradients.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim \n",
    "\n",
    "model=GABLM(128, **gab_config)\n",
    "vocab_size=100\n",
    "DEFAULT_CONTEXT_LENGTH=10\n",
    "\n",
    "mock_input=torch.randn(1, DEFAULT_CONTEXT_LENGTH, 128)\n",
    "mock_y=torch.randint(0, 100, (1, 10))\n",
    "mock_Z={}\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "y,X=model(mock_input, **mock_Z)\n",
    "loss=nn.CrossEntropyLoss()(y.view(-1, 100), mock_y.view(-1))\n",
    "loss.backward()\n",
    "\n",
    "used_params = set()\n",
    "\n",
    "# Hook to register which parameters are used during the forward pass\n",
    "def hook_fn(module, input, output):\n",
    "    for param in module.parameters():\n",
    "        used_params.add(param)\n",
    "\n",
    "# Register hooks for all submodules\n",
    "hooks = []\n",
    "for submodule in model.modules():\n",
    "    hook = submodule.register_forward_hook(hook_fn)\n",
    "    hooks.append(hook)\n",
    "\n",
    "# Forward pass again to collect used parameters\n",
    "model(mock_input)\n",
    "\n",
    "# Remove all hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "all_gradients_present = True\n",
    "\n",
    "# Check gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if param in used_params and param.grad is None:\n",
    "            # Used parameter without gradient -> Error\n",
    "            print(f\"Error: Used parameter {name} requires gradients but has none.\")\n",
    "            all_gradients_present = False\n",
    "        elif param not in used_params:\n",
    "            # Unused parameter -> Warning\n",
    "            print(f\"Warning: Parameter {name} was not used in the forward pass.\")\n",
    "\n",
    "if all_gradients_present:\n",
    "    print('Differentiability test passed')\n",
    "else:\n",
    "    print('Differentiability test failed due to missing gradients.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
