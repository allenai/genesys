{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place for experimenting the progressive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "c:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\library\\core\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.utils as U\n",
    "from model_discovery.configs.gam_config import GAMConfig, GAMConfig_14M\n",
    "# from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "from model_discovery.model.composer import GAUBase, GAUTree, GABComposer\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "from model_discovery.evolution import BuildEvolution\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "\n",
    "tree_dir='../model_discovery/model/library/core/gpt2/units'\n",
    "gpt2_tree=GAUTree.load_from_base(tree_dir)\n",
    "\n",
    "prompts_dir='../model_discovery/agents/prompts/'\n",
    "gab_py = U.read_file(U.pjoin(prompts_dir,'gab_template.py'))\n",
    "gam_prompt_path = os.path.join(prompts_dir,'gam_prompt.py')\n",
    "gau_template_path = os.path.join(prompts_dir,'gau_template.py')\n",
    "GAM_TEMPLATE=open(gam_prompt_path).read()\n",
    "GAU_TEMPLATE=open(gau_template_path).read()\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n",
    "\n",
    "GAB_BASE=inspect.getsource(GABBase)\n",
    "GAB_COMPOSER=inspect.getsource(GABComposer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os,sys\n",
    "import importlib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "import model_discovery.agents.prompts.prompts as P\n",
    "import model_discovery.model.composer\n",
    "importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUTree,GAUBase,GAUNode\n",
    "import model_discovery.utils\n",
    "importlib.reload(model_discovery.utils)\n",
    "import model_discovery.utils as U\n",
    "\n",
    "\n",
    "\n",
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "# Placeholder classes for future implementation\n",
    "class MemoryAccessUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, memory_size, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "class DownsamplingUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, downsample_factor, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "\n",
    "class XXX(GAUBase,object):\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "\n",
    "class XGAU(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
    "        # argv: list of hyperparameters\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\n",
    "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        self.a1 = kwarg_all.get('a1', 1)\n",
    "        self.a2: int = kwarg_all.get('a2', 2)\n",
    "\n",
    "        # COMPLETING THE CODE HERE #\n",
    "        self.token_scorer = TokenScoringGAU(embed_dim, **factory_kwargs)\n",
    "        self.dual_path: GAUBase = DualPathGAU(embed_dim, **factory_kwargs)\n",
    "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim, **factory_kwargs)\n",
    "\n",
    "        self.stages: nn.ModuleList = nn.ModuleList([CompressionStageGAU(embed_dim, device=\n",
    "            device, dtype=dtype, **kwargs) for _ in range(2)])\n",
    "\n",
    "        self.modelx=nn.ModuleList([\n",
    "            nn.Conv2d(1,20,5),\n",
    "            nn.ReLU(),\n",
    "            LatentAttentionGAU(),\n",
    "            nn.Conv2d(20,64,5),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "            \n",
    "\n",
    "        self.model1 = (\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU(),\n",
    "            'gau3': CompressionStageGAU()\n",
    "        })\n",
    "\n",
    "        self.model2 = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('gau2', DualPathGAU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "    def init_something(self):\n",
    "        self.modelx[3]=LatentAttentionGAU()\n",
    "        self.modelx.append(LatentAttentionGAU())\n",
    "        self.modelx.insert(0,LatentAttentionGAU())\n",
    "        self.activations['gau1']=LatentAttentionGAU()\n",
    "        self.model2.add_module('gau4',LatentAttentionGAU())\n",
    "\n",
    "        X=self.dual_path(X)\n",
    "    \n",
    "    def _forward(self, X, **Z): \n",
    "\n",
    "        # THE CODE HERE MUST BE COMPLETED #\n",
    "        # Step 1: Score tokens\n",
    "        X, Z = self.token_scorer(X, **Z)\n",
    "        # Step 2: Route through dual paths\n",
    "        # Step 3: Apply latent attention\n",
    "        Y, Z = self.latent_attention(X, A=A, B=B, **Z)\n",
    "        for stage in self.stages:\n",
    "            X = stage(X, **Z)\n",
    "        \n",
    "        X, Z = self.dual_path(X, **Z)\n",
    "        X=self.activations['gau3'](X)\n",
    "\n",
    "        return Y, Z\n",
    "\n",
    "@gau_test\n",
    "def test1(a=1,device=None):\n",
    "    print('test1')\n",
    "\n",
    "    \n",
    "@gau_test\n",
    "def test2(device=None):\n",
    "    print('test2')\n",
    "\n",
    "\n",
    "CHILDREN_DECLARATIONS = {\n",
    "    'XGAU':[\n",
    "        UnitDecl(\n",
    "            unitname='TokenScoringGAU', \n",
    "            requirements='Token scoring GAU',\n",
    "            inputs=[],\n",
    "            outputs=[],\n",
    "        ),\n",
    "        UnitDecl(\n",
    "            unitname='DualPathGAU',\n",
    "            requirements='Dual path GAU',\n",
    "            inputs=[],\n",
    "            outputs=[],\n",
    "        ),\n",
    "        UnitDecl(\n",
    "            unitname='LatentAttentionGAU',\n",
    "            requirements='Latent attention GAU',\n",
    "            inputs=[],\n",
    "            outputs=[],\n",
    "        ),\n",
    "        UnitDecl(\n",
    "            unitname='CompressionStageGAU',\n",
    "            requirements='Compression stage GAU',\n",
    "            inputs=[],\n",
    "            outputs=[],\n",
    "        ),\n",
    "    ]\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "class MHA(GAUBase):\n",
    "    \"\"\"\n",
    "    MHA combines hierarchical gating mechanisms from HGRN2 with state space modeling (SSM) to improve long-range dependency capture and computational efficiency.\n",
    "\n",
    "    This class replaces the traditional multi-head attention mechanism with a more efficient approach that maintains linear complexity with respect to sequence length.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): The embedding dimension of the input sequence.\n",
    "        block_loc (tuple): The location of the block within the network.\n",
    "        kwarg_all (dict): Dictionary of additional keyword arguments for initialization.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        expand_ratio (int): The ratio for expansion layers.\n",
    "        d_state (int): The dimension of the state in the state space model.\n",
    "\n",
    "    Returns:\n",
    "        Y (Tensor): The output sequence with the same shape as the input.\n",
    "\n",
    "    Example:\n",
    "        >>> mha = MHA(embed_dim=512, block_loc=(0, 0), kwarg_all={}, num_heads=8, expand_ratio=4, d_state=64)\n",
    "        >>> X = torch.randn(2, 10, 512)\n",
    "        >>> Y, Z = mha(X)\n",
    "\n",
    "    Note:\n",
    "        This implementation maintains interpretability through the `get_attention_map` method, which provides a way to visualize effective attention patterns.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, num_heads: int, expand_ratio: int, d_state: int, device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.d_state = d_state\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # HGRN2-inspired projections\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "        # SSM components\n",
    "        self.ssm_A = nn.Parameter(torch.randn(num_heads, d_state, d_state))\n",
    "        self.ssm_B = nn.Parameter(torch.randn(num_heads, d_state, 1))\n",
    "        self.ssm_C = nn.Parameter(torch.randn(num_heads, 1, d_state))\n",
    "\n",
    "        # Expansion layers\n",
    "        self.expand = nn.Linear(embed_dim, embed_dim * expand_ratio, bias=True)\n",
    "        self.contract = nn.Linear(embed_dim * expand_ratio, embed_dim, bias=True)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "        # Layernorm for stability\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize SSM components\n",
    "        nn.init.orthogonal_(self.ssm_A)\n",
    "        nn.init.normal_(self.ssm_B, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.ssm_C, mean=0.0, std=0.02)\n",
    "\n",
    "        # Initialize other weights\n",
    "        nn.init.xavier_uniform_(self.q_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.k_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.v_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.expand.weight)\n",
    "        nn.init.xavier_uniform_(self.contract.weight)\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        B, L, D = X.shape\n",
    "        H = self.num_heads\n",
    "\n",
    "        # HGRN2-inspired projections\n",
    "        Q = self.q_proj(X).view(B, L, H, -1)\n",
    "        K = self.k_proj(X).view(B, L, H, -1)\n",
    "        V = self.v_proj(X).view(B, L, H, -1)\n",
    "\n",
    "        # Expansion\n",
    "        X_exp = self.expand(X).view(B, L, H, -1)\n",
    "\n",
    "        # SSM state update\n",
    "        state = torch.zeros(B, H, self.d_state, 1, device=X.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(L):\n",
    "            # Update state\n",
    "            state = torch.matmul(self.ssm_A, state) + self.ssm_B * K[:, t, :, :, None]\n",
    "\n",
    "            # Compute output\n",
    "            output = torch.matmul(self.ssm_C, state).squeeze(-1)\n",
    "            output = output * Q[:, t, :, :]\n",
    "            output = torch.sum(output * V[:, t, :, :], dim=-1)\n",
    "\n",
    "            # Apply expansion\n",
    "            output = output * X_exp[:, t, :, :]\n",
    "\n",
    "            outputs.append(output)\n",
    "\n",
    "        output = torch.stack(outputs, dim=1)\n",
    "        output = output.view(B, L, -1)\n",
    "\n",
    "        # Contraction\n",
    "        output = self.contract(output)\n",
    "\n",
    "        # Output projection and residual connection\n",
    "        output = self.out_proj(output)\n",
    "        output = self.norm(X + output)\n",
    "\n",
    "        return output, Z\n",
    "\n",
    "    def get_attention_map(self, X):\n",
    "        B, L, D = X.shape\n",
    "        H = self.num_heads\n",
    "\n",
    "        Q = self.q_proj(X).view(B, L, H, -1)\n",
    "        K = self.k_proj(X).view(B, L, H, -1)\n",
    "\n",
    "        attention_map = torch.zeros(B, H, L, L, device=X.device)\n",
    "        state = torch.zeros(B, H, self.d_state, 1, device=X.device)\n",
    "\n",
    "        for t in range(L):\n",
    "            state = torch.matmul(self.ssm_A, state) + self.ssm_B * K[:, t, :, :, None]\n",
    "            output = torch.matmul(self.ssm_C, state).squeeze(-1)\n",
    "            attention_map[:, :, t, :t+1] = torch.sum(output * Q[:, :t+1, :, :], dim=-1).transpose(1, 2)\n",
    "\n",
    "        return attention_map\n",
    "\n",
    "\n",
    "# WRITE YOUR UNIT TEST FUNCTIONS HERE #\n",
    "\n",
    "@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON'T BE RECOGNIZED AS A UNIT TEST #\n",
    "def test_mha(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\n",
    "    # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\n",
    "    embed_dim = 128\n",
    "    block_loc = (0, 0)\n",
    "    kwarg_all = {}\n",
    "    num_heads = 8\n",
    "    expand_ratio = 4\n",
    "    d_state = 64\n",
    "    mha = MHA(embed_dim, block_loc, kwarg_all, num_heads, expand_ratio, d_state, device=device, dtype=dtype)\n",
    "    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "    y, Z_ = mha(x, **Z)\n",
    "    assert y.shape == (1, 100, 128)\n",
    "\n",
    "    # WRITE ASSERTIONS TO PERFORM THE TEST, USE PRINT TO DEBUG #\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "    # raise NotImplementedError # YOU MUST IMPLEMENT THIS FUNCTION #\n",
    "'''\n",
    "children=['ProjectionGAU','SimilarityComputationGAU','NormalizationGAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "# GAU_IMPLEMENTATION_FILE   # DO NOT CHANGE OR REMOVE THE MARK HERE, KEEP IT ALWAYS THE FIRST LINE TO ALLOW PARSER DETECT THE GAU IMPLEMENTATION FILES #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "from typing import Dict, Tuple\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n",
    "\n",
    "class DynamicHybridLayer(GAUBase):\n",
    "    \"\"\"\n",
    "    Dynamic Hybrid Layer (DHL)\n",
    "\n",
    "    This GAU integrates a Selective State Space Model (SSM) block, an Attention block,\n",
    "    and combines their outputs using a dynamic gating mechanism informed by a context vector\n",
    "    extracted from the input. The DHL adaptively balances the contributions of SSM and\n",
    "    attention to efficiently handle long sequences while maintaining the ability to capture\n",
    "    complex dependencies.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): Embedding dimension.\n",
    "        block_loc (Tuple[int, int]): Location of the block within the network.\n",
    "        kwarg_all (dict): Dictionary of all keyword arguments.\n",
    "\n",
    "    Inputs:\n",
    "        - X: Input sequence tensor of shape (B, L, D).\n",
    "\n",
    "    Outputs:\n",
    "        - Y: Output sequence tensor of the same shape as X.\n",
    "\n",
    "    Child GAUs:\n",
    "        - SelectiveSSMUnit\n",
    "        - AttentionUnit\n",
    "        - AdvancedContextVectorExtractor\n",
    "        - HierarchicalChunkProcessor\n",
    "        - MemoryCompressor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "                 device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.block_loc = block_loc\n",
    "        self.kwarg_all = kwarg_all\n",
    "\n",
    "        # Parameters for dynamic adaptation\n",
    "        self.sequence_threshold = kwarg_all.get('sequence_threshold', 1024)\n",
    "        ssm_dim_factor = kwarg_all.get('ssm_dim_factor', 1.0)\n",
    "        self.ssm_dim = int(self.embed_dim * ssm_dim_factor)\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.embed_dim, **self.factory_kwargs)\n",
    "\n",
    "        # Project S, A, C to lower dimensions for gating\n",
    "        gate_proj_dim = kwarg_all.get('gate_proj_dim', self.embed_dim // 2)\n",
    "\n",
    "        self.s_proj = nn.Linear(self.embed_dim, gate_proj_dim, **self.factory_kwargs)\n",
    "        self.a_proj = nn.Linear(self.embed_dim, gate_proj_dim, **self.factory_kwargs)\n",
    "        self.c_proj = nn.Linear(self.embed_dim, gate_proj_dim, **self.factory_kwargs)\n",
    "\n",
    "        self.ssm_unit = SelectiveSSMUnit(embed_dim=self.embed_dim, block_loc=self.block_loc,\n",
    "                                         kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n",
    "\n",
    "        self.attention_unit = AttentionUnit(embed_dim=self.embed_dim, block_loc=self.block_loc,\n",
    "                                            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n",
    "\n",
    "        # Using an updated Context Vector Extractor\n",
    "        self.context_unit = AdvancedContextVectorExtractor(embed_dim=self.embed_dim, block_loc=self.block_loc,\n",
    "                                                           kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n",
    "\n",
    "        self.gate = nn.Linear(gate_proj_dim * 3, self.embed_dim, **self.factory_kwargs)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, **self.factory_kwargs)\n",
    "\n",
    "        # For long sequence handling\n",
    "        self.chunk_size = kwarg_all.get('chunk_size', 1024)\n",
    "        self.hierarchical_processor = HierarchicalChunkProcessor(embed_dim=self.embed_dim, block_loc=self.block_loc,\n",
    "                                                                 kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n",
    "        self.memory_compressor = MemoryCompressor(embed_dim=self.embed_dim, block_loc=self.block_loc,\n",
    "                                                  kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        # Input validation\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            raise TypeError(\"Input X must be a torch.Tensor\")\n",
    "        if X.shape[-1] != self.embed_dim:\n",
    "            raise ValueError(f\"Expected input of shape (B, L, {self.embed_dim}), got {X.shape}\")\n",
    "\n",
    "        B, L, D = X.shape\n",
    "        X_norm = self.norm(X)\n",
    "        Z['X_norm'] = X_norm\n",
    "\n",
    "        # Handle long sequences\n",
    "        if L > self.sequence_threshold:\n",
    "            # Prepare inputs for hierarchical processing\n",
    "            Z['chunk_size'] = self.chunk_size\n",
    "            Y_hp, Z_ = self.hierarchical_processor(X_norm, **Z)\n",
    "            X_norm = Y_hp\n",
    "            Z.update(Z_)\n",
    "            # Prepare inputs for memory compression\n",
    "            Y_mc, Z_ = self.memory_compressor(X_norm, **Z)\n",
    "            X_norm = Y_mc\n",
    "            Z.update(Z_)\n",
    "        else:\n",
    "            # For sequences below the threshold, use X_norm as is\n",
    "            pass\n",
    "\n",
    "        # Process SSM Unit\n",
    "        Y_s, Z_ = self.ssm_unit(X_norm, **Z)\n",
    "        S = Y_s\n",
    "        Z.update(Z_)\n",
    "\n",
    "        # Process Attention Unit\n",
    "        Y_a, Z_ = self.attention_unit(X_norm, **Z)\n",
    "        A = Y_a\n",
    "        Z.update(Z_)\n",
    "\n",
    "        # Process Context Vector\n",
    "        Y_c, Z_ = self.context_unit(X_norm, **Z)\n",
    "        C = Y_c\n",
    "        Z.update(Z_)\n",
    "\n",
    "        if S is None:\n",
    "            S = X_norm\n",
    "        if A is None:\n",
    "            A = X_norm\n",
    "        if C is None:\n",
    "            # Compute a default context vector if not provided\n",
    "            C = torch.mean(X_norm, dim=1, keepdim=True).expand(B, L, D)\n",
    "\n",
    "        # Project to lower dimensions\n",
    "        S_proj = self.s_proj(S)\n",
    "        A_proj = self.a_proj(A)\n",
    "        C_proj = self.c_proj(C)\n",
    "\n",
    "        concatenated = torch.cat([S_proj, A_proj, C_proj], dim=-1)\n",
    "\n",
    "        G = torch.sigmoid(self.gate(concatenated))\n",
    "\n",
    "        # Dynamically adjust balance between SSM and Attention based on sequence length\n",
    "        seq_factor = min(L / self.sequence_threshold, 1.0)\n",
    "\n",
    "        G = G * seq_factor + (1 - seq_factor) * (1 - G)\n",
    "\n",
    "        Y = G * S + (1 - G) * A\n",
    "        Y = self.out_proj(Y)\n",
    "        Y = Y + X  # Residual connection\n",
    "\n",
    "        # Logging gate values (for observation, not to be maintained in production)\n",
    "        if self.training and L < 5000:  # Only log for short sequences to avoid too much data\n",
    "            logging.debug(f\"Gating values mean: {G.mean().item()} at block {self.block_loc}\")\n",
    "\n",
    "        # Return output and updated Z\n",
    "        return Y, Z\n",
    "\n",
    "# Unit Tests\n",
    "\n",
    "@gau_test  # DO NOT CHANGE THIS DECORATOR\n",
    "def test_dynamic_hybrid_layer(device=None, dtype=None) -> None:\n",
    "    embed_dim = 64\n",
    "    batch_size = 2\n",
    "    seq_len_short = 512\n",
    "    seq_len_long = 2048\n",
    "\n",
    "    block_loc = (0, 1)\n",
    "    kwarg_all = {'sequence_threshold': 1024}\n",
    "\n",
    "    # Short sequence test\n",
    "    X_short = torch.randn(batch_size, seq_len_short, embed_dim, device=device, dtype=dtype)\n",
    "    dhl = DynamicHybridLayer(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n",
    "\n",
    "    Y_short, Z_short = dhl(X_short)\n",
    "    assert Y_short.shape == X_short.shape, f\"Output shape mismatch for short sequence. Expected {X_short.shape}, got {Y_short.shape}\"\n",
    "\n",
    "    # Long sequence test\n",
    "    X_long = torch.randn(batch_size, seq_len_long, embed_dim, device=device, dtype=dtype)\n",
    "    Y_long, Z_long = dhl(X_long)\n",
    "    assert Y_long.shape == X_long.shape, f\"Output shape mismatch for long sequence. Expected {X_long.shape}, got {Y_long.shape}\"\n",
    "\n",
    "    print(\"DynamicHybridLayer unit test passed.\")\n",
    "\n",
    "# Declare all children GAUs here\n",
    "CHILDREN_DECLARATIONS = [\n",
    "    UnitDecl(\n",
    "        unitname=\"SelectiveSSMUnit\",\n",
    "        requirements=\"Processes the normalized input through a selective SSM.\",\n",
    "        inputs=[\"X_norm\"],\n",
    "        outputs=[\"Y_s\"]\n",
    "    ),\n",
    "    UnitDecl(\n",
    "        unitname=\"AttentionUnit\",\n",
    "        requirements=\"Applies an efficient attention mechanism to the normalized input.\",\n",
    "        inputs=[\"X_norm\"],\n",
    "        outputs=[\"Y_a\"]\n",
    "    ),\n",
    "    UnitDecl(\n",
    "        unitname=\"AdvancedContextVectorExtractor\",\n",
    "        requirements=\"Extracts an advanced context vector from the normalized input.\",\n",
    "        inputs=[\"X_norm\"],\n",
    "        outputs=[\"Y_c\"]\n",
    "    ),\n",
    "    UnitDecl(\n",
    "        unitname=\"HierarchicalChunkProcessor\",\n",
    "        requirements=\"Processes long sequences in hierarchical chunks.\",\n",
    "        inputs=[\"X_norm\", \"chunk_size\"],\n",
    "        outputs=[\"Y_hp\"]\n",
    "    ),\n",
    "    UnitDecl(\n",
    "        unitname=\"MemoryCompressor\",\n",
    "        requirements=\"Compresses less relevant information for very long sequences.\",\n",
    "        inputs=[\"X_norm\"],\n",
    "        outputs=[\"Y_mc\"]\n",
    "    ),\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit_name = \"MHA\"  # Provide the unit_name to rename GAU class\n",
    "reformatted_code, new_args, gau_tests, errors, warnings, fetal_errors,docstring,children_decl, unit_name = check_and_reformat_gau_code(code)#, unit_name)\n",
    "print(\"Reformatted Code:\\n\" + reformatted_code)\n",
    "print(\"Errors:\\n\", '\\n\\t'.join(errors+fetal_errors))\n",
    "print(\"Warnings:\\n\", '\\n\\t'.join(warnings))\n",
    "print(\"New Arguments:\\n\", new_args)\n",
    "print(\"Docstring:\\n\", docstring)\n",
    "print(\"Children Declarations:\\n\", children_decl)\n",
    "\n",
    "\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "spec=P.UnitSpec(\n",
    "   unitname = unit_name,\n",
    "   document = 'Test',\n",
    "   inputs = [],\n",
    "   outputs = [],\n",
    ")\n",
    "# test_tree.add_unit(\n",
    "#     spec, reformatted_code, new_args, None, None, None, children, gau_tests, None\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[{'role': 'system', 'content': '\\nYou are a professional AI researcher focusing on discovering the best\\nautoregressive language model block. Your goal is to design a novel block\\nfollowing the Generalized Autoregressive Block (GAB) structure defined in the\\nfollowing base class:\\n\\n```python class GABBase(nn.Module):\\n \"\"\" Base class for Generalized Autoregressive Block \"\"\"\\n def __init__(self,embed_dim: int, block_loc: tuple):\\n super().__init__()\\n self.embed_dim = embed_dim\\n self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\\n\\n def _forward(self, X, **Z): \\n raise NotImplementedError\\n \\n # YOU ARE NOT ALLOW TO OVERRIDE THIS METHOD #\\n def forward(self, X, **Z): # kwargs not parsable by torchscript but more flexible\\n \"\"\"Forward pass of the model\"\"\"\\n assert len(X.shape) == 3, f\"Input shape must be (batch, seqlen, embed_dim), got {len(X.shape)}D instead\"\\n assert X.shape[-1] == self.embed_dim, f\"Input shape must be (batch, seqlen, embed_dim), got {X.shape} instead\"\\n Y = self._forward(X, **Z)\\n if isinstance(Y, tuple):\\n Y, Z_ = Y\\n else:\\n Z_ = {}\\n assert Y.shape == X.shape, f\"GAB Output shape must be the same as input shape of {X.shape}, got {Y.shape} instead\"\\n assert isinstance(Z, dict), \"Intermediate variables must be stored in a dict\"\\n Z.update(Z_) # the new intermediate variables are updated to the current Z\\n return Y, Z\\n ```\\n\\nThe GAB will be used to construct a Generalized Autoregressive Model (GAM)\\ndefined as follows:\\n\\n```python from transformers.modeling_outputs import CausalLMOutput\\nfrom transformers import PreTrainedModel, PretrainedConfig\\nfrom dataclasses import dataclass\\n\\nimport torch\\nfrom torch import nn\\n\\nfrom gab import GAB, gab_config\\n\\n\\n@dataclass\\nclass GAMConfig(PretrainedConfig):\\n \\'\\'\\'Configurations for Generalized Autoregressive Models.\\'\\'\\'\\n\\n d_model: int\\n n_block: int\\n batch_tokens: int \\n vocab_size: int = None\\n\\n\\nclass GAM(nn.Module):\\n \\'\\'\\' Generalized Autoregressive Models\\n Input: X: (batch, seqlen, embed_dim)\\n Output: Y: (batch, seqlen, embed_dim)\\n \\'\\'\\'\\n def __init__(\\n self,\\n d_model: int,\\n n_block: int,\\n vocab_size: int = 50277,\\n norm_epsilon: float = 1e-5,\\n device = None,\\n dtype = None,\\n ) -> None:\\n self.factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n super().__init__()\\n self.d_model = d_model\\n self.embedding = nn.Embedding(vocab_size, d_model, **self.factory_kwargs)\\n\\n block_config = gab_config()\\n self.blocks = nn.ModuleList(\\n [\\n GAB(\\n embed_dim=d_model, \\n block_loc=(layer_idx,n_block),\\n device=device, \\n dtype=dtype, \\n **block_config\\n )\\n for layer_idx in range(n_block)\\n ]\\n )\\n self.norm_out = nn.LayerNorm(\\n d_model, eps=norm_epsilon, **self.factory_kwargs\\n )\\n\\n def forward(self, input_ids):\\n hidden_states = self.embedding(input_ids)\\n intermediate_vars = {}\\n for block in self.blocks:\\n hidden_states, intermediate_vars = block(\\n hidden_states,\\n **intermediate_vars\\n )\\n hidden_states = self.norm_out(hidden_states)\\n return hidden_states\\n\\n\\nclass GLMHeadModel(PreTrainedModel):\\n \\'\\'\\' Generalized Autoregressive Models with LM Head \\'\\'\\'\\n config_class = GAMConfig\\n\\n def __init__(\\n self,\\n config: GAMConfig,\\n device=None,\\n dtype=None,\\n ) -> None:\\n super().__init__(config)\\n factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n self.backbone = GAM(\\n d_model=config.d_model,\\n n_block=config.n_block,\\n vocab_size=config.vocab_size,\\n **factory_kwargs,\\n )\\n self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False, **factory_kwargs)\\n\\n def forward(self, input_ids, **gab_kwargs):\\n hidden_states = self.backbone(input_ids, **gab_kwargs)\\n lm_logits = self.lm_head(hidden_states)\\n return CausalLMOutput(logits=lm_logits)\\n ```\\n\\nThe produced language model will be pretrained with the corpus and then be\\napplied for downstream tasks. The new model is expected to have a low\\nperplexity, high accuracy, robustness, efficiency, and most importantly, good\\nscalability. \\n\\nSince the autoregressive model design is complicated, so we will break it down\\ninto smaller parts. We represent a block as multiple nested units, the\\nGeneralized Autoregressive Unit (GAU). Each GAU accepts a sequence of embeddings\\nX and a dictionary of intermediate variables Z as input, and outputs a sequence\\nof embeddings Y and a dictionary of new or updated intermediate variables Z_. Z_\\nis optional, when it is provided, it will be used to update Z for the next unit\\nby Z.update(Z_). A GAU is defined in the following base class:\\n\\n```python class GAUBase(nn.Module): \\n \"\"\" \\n Instead of directly giving the full implementation of a GAB block, the agent need to \\n design a series of nested GAB units and construct the full GAB block as a pipeline of these units.\\n\\n GAB is fractal, like GAB itself, each GAB unit accepts X and Z as input and returns Y and Z as output.\\n \"\"\" \\n def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict):\\n super().__init__()\\n self.embed_dim = embed_dim\\n self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\\n self.kwarg_all = kwarg_all # dict of all kwargs, use it to initialize the children units\\n\\n def _forward(self, X, **Z): # you can define the arguments instead of using **Z\\n raise NotImplementedError\\n \\n def forward(self, X, **Z):\\n assert len(X.shape) == 3 and X.shape[-1] == self.embed_dim, f\"Input X must be a sequence of shape (batch, seqlen, embed_dim), all other kinds of variables should be passed by Z. Got {X.shape} instead.\"\\n _params = inspect.signature(self._forward).parameters\\n X=X.to(**self.factory_kwargs)\\n _Z = {k: v for k, v in Z.items() if k in _params}\\n Y = self._forward(X, **_Z)\\n if isinstance(Y, tuple):\\n Y, Z_ = Y\\n else:\\n Z_ = {}\\n assert Y.shape == X.shape, f\"GAU output Y must be a sequence with the same shape as input of {X.shape}, got {Y.shape} instead, all other kinds of variables should be passed by Z.\"\\n assert isinstance(Z_, dict), \"Intermediate variables must be stored in a dict\"\\n Z.update(Z_) # the new intermediate variables are updated to the current Z\\n return Y, Z\\n ```\\n\\nYou will design a GAU by completing the blanks marked in this template, which\\nincludes the initialization where you can define your custom arguments with\\noptional default values, the forward function where you can define convenient\\nfunctions or classes in the GAB class such as caches, notice that you are only\\nallowed to have only one GAU which inherited from the GAUBase class in the file:\\n \\n```python # gau.py # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\\n\\n\\n# YOU CAN IMPORT MORE MODULES HERE #\\n\\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\\n\\n\\nclass UnitName(GAUBase):\\n \"\"\"Generalized Autoregressive Block Unit\\n Input: X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \\n Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\\n Constraints: Causal, differentiable, parameter number, complexity, parallelizable...\\n\\n embed_dim: The dimension of the input embeddings\\n block_loc: The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\\n kwarg_all: A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\\n \"\"\"\\n def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT KEEP THE ORIGINAL ONES #\\n self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\\n super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\\n \\n # COMPLETING THE CODE HERE #\\n\\n raise NotImplementedError\\n\\n\\n # YOU CAN ADD MORE FUNCTIONS HERE #\\n\\n\\n def _forward(self, X, **Z): \\n \\n # THE CODE HERE MUST BE COMPLETED #\\n\\n raise NotImplementedError\\n\\n\\n# WRITE YOUR UNIT TEST FUNCTIONS HERE #\\n\\n@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON\\'T BE RECOGNIZED AS A UNIT TEST #\\ndef unit_test_name(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\\n # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\\n\\n # WRITE ASSERTIONS TO PERFORM THE TEST, USE PRINT TO DEBUG #\\n \\n raise NotImplementedError # YOU MUST IMPLEMENT THIS FUNCTION #\\n ```\\n\\nIn a GAU, you can call other GAUs, as such, you can create a complicated GAB\\nblock by nesting multiple GAUs. However, each GAU should be not too complex, if\\nyou want to create complex block, you should break it down into smaller GAUs and\\nnest them. As such, you should design a GAB block in a top-down manner. \\n\\nInstead of starting from scratch, you will start from an existing design and\\nimprove it. You will be provided with the full information of the design,\\nincluding the proposal, the tree structure, the implementations of the GAUs. You\\nare only allowed to modify *one GAU* from the existing design. \\n\\nYou need to select one GAU to modify, you can define new children GAUs, however\\nyou need to guarantee that your modification wont affect the correctness of the\\noverall design. \\n\\nYou will start by writing down an overal proposal for the design you want to\\nhave, the proposal decides a direction, phylosophy and the plan of the design,\\nand the analysis of the problem and how you gonna solve it by modifying one GAU\\nfrom the existing design. You will be provided with one or multiple references\\nto consider that may inspire you if there are references provided.\\n\\nYour response should include: \\n\\n1. The proposal, it should include but not restrict to the following parts: a. A\\n title with the name of the design in the level 1 header format. You shuld\\n have only one level 1 header in your response which is the name of the\\n design.\\n\\n b. Your motivation of the design. What problem you want to solve based on the\\n insights or observations you have about the autoregressive models today,\\n and any inspirations you may have from the references. \\n\\n c. The analysis of the problem.\\n\\n d. The core idea and phylosophy behind of your design that may solve the\\n problem you proposed. \\n\\n e. The plan of the design. You should include subsections of that describe\\n the details of each part of the design with the justifications. The\\n selection of the GAU to modify and the reasoning of the selection. \\n\\n f. A conclution of the proposal. \\n\\n g. Optional, the references you used in your proposal, should be in the right\\n format.\\n2. The name of the variant of the model you are going to design.\\n3. The selection of the GAU to modify.\\n\\nThe proposal will be reviewed and you will be asked to modify it if it is not\\npassed. You can start to implement the design after the proposal is passed. \\n\\nThe proposal should be as detailed as possible, DO NOT WORRY IF THE PROPOSAL IS\\nTOO LONG, BUT ALSO DO NOT FILL IN BY REDUNDANT WORDS, USE PRECISE AND CONCRETE\\nLANGUAGE, the proposal will be the guideline for the entire design process so it\\nshould be clear and detailed. \\n'}]\n",
    "print(x[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code=test_tree.compose_unit('XAU')\n",
    "code=U.add_line_num(code)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class DilatedReformerRootGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.dilated_attention = DilatedAttentionGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.reversible_residual = ReversibleResidualGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **\n",
    "            self.factory_kwargs, **kwarg_all)\n",
    "        self.lsh_attention = LSHAttentionGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.dilated_attention(X, **Z)\n",
    "        X, Z = self.reversible_residual(X, **Z)\n",
    "        X, Z = self.lsh_attention(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class LSHAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class DilatedAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class ReversibleResidualGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "@gau_test\n",
    "def test_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau(device=\n",
    "    None, dtype=None) ->None:\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    embed_dim = 16\n",
    "    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "    gau = DilatedReformerRootGAU(embed_dim=embed_dim, block_loc=(0, 6), kwarg_all={}, device=\n",
    "        device, dtype=dtype)\n",
    "    Y, Z_ = gau(X, **Z)\n",
    "    assert Y.shape == X.shape, 'Output shape should match input shape'\n",
    "    assert isinstance(Z_, dict), 'Z_ should be a dictionary'\n",
    "    print('Input X:', X)\n",
    "    print('Output Y:', Y)\n",
    "    print('Intermediate variables Z_:', Z_)\n",
    "\n",
    "\n",
    "def run_DilatedReformerRootGAU_tests():\n",
    "\ttry:\n",
    "\t\ttest_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"Error in running unit_test_dilated_reformer_root_gau:\", e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\trun_DilatedReformerRootGAU_tests()\n",
    "'''\n",
    "\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "output=io.StringIO()\n",
    "with redirect_stdout(output):\n",
    "    exec(code)\n",
    "captured = str(output.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ResonanceLOCOSTGAMRoot(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class ResonanceLOCOSTGAMRoot(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.resonance_rope = ResonanceRoPEUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.state_space = StateSpaceUnit(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all)\n",
    "        self.inter_block_comm = InterBlockCommUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y, Z_ = self.resonance_rope(X, **Z)\n",
    "        Y, Z_ = self.state_space(Y, **Z_)\n",
    "        Y, Z_ = self.inter_block_comm(Y, **Z_)\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class ResonanceRoPEUnit(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.freqs = self._init_frequencies(embed_dim)\n",
    "\n",
    "    def _init_frequencies(self, embed_dim):\n",
    "        half_dim = embed_dim // 2\n",
    "        freqs = torch.exp(-torch.arange(half_dim, dtype=torch.float32,\n",
    "            device=self.factory_kwargs['device']) * (math.log(10000.0) / (\n",
    "            half_dim - 1)))\n",
    "        return freqs\n",
    "\n",
    "    def _apply_rope(self, X):\n",
    "        batch_size, seqlen, _ = X.size()\n",
    "        position_ids = torch.arange(seqlen, dtype=X.dtype, device=X.device)\n",
    "        sinusoid_inp = torch.einsum('i,j->ij', position_ids, self.freqs)\n",
    "        sin, cos = sinusoid_inp.sin(), sinusoid_inp.cos()\n",
    "        X1, X2 = X[..., ::2], X[..., 1::2]\n",
    "        Y1 = X1 * cos - X2 * sin\n",
    "        Y2 = X1 * sin + X2 * cos\n",
    "        return torch.stack([Y1, Y2], dim=-1).flatten(-2)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = self._apply_rope(X)\n",
    "        Z_ = Z\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "class StateSpaceUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class InterBlockCommUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "    \n",
    "\n",
    "\n",
    "gab_config = {}\n",
    "\n",
    "\n",
    "# Check if to device still work!\n",
    "\n",
    "gab = GAB(128, (0, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_device(model):\n",
    "    \"\"\"\n",
    "    Returns the device of the first parameter found in the given nn.Module.\n",
    "    If the module has no parameters, returns None.\n",
    "    \"\"\"\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "get_module_device(gab.root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gab.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children={\n",
    "    'unit1': 'MockUnit1',\n",
    "    'unit2': 'MockUnit2',\n",
    "}\n",
    "called=['unit1', 'unit2', 'unit1']\n",
    "test_tree.units['DualPathGAU'] = GAUNode('DualPathGAU', None, None, None, called, None, None, children, None)\n",
    "children={\n",
    "    'unit1': 'MockUnit3',\n",
    "    'unit2': 'MockUnit4',\n",
    "}\n",
    "called=['unit2', 'unit2']\n",
    "test_tree.units['MockUnit1'] = GAUNode('MockUnit1', None, None, None, called, None, None, children, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self,r,d,e,**Z):\n",
    "        print('Args in A:',d,e)\n",
    "        print('Kwargs in A:',Z)\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self,_Z,c=1,d=2,e=3,**Z):\n",
    "        args2={'d':d,'e':e}\n",
    "        super().__init__(c,**_Z,**args2)\n",
    "        print('Kwargs in B:',Z)\n",
    "\n",
    "cfg={\n",
    "    'a':1,\n",
    "    'b':2,\n",
    "    'c':3,\n",
    "}\n",
    "\n",
    "B(cfg,**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(X,Z,**kwargs):\n",
    "    print(X)\n",
    "    print(Z)\n",
    "    print(kwargs)\n",
    "\n",
    "def B(X,Z,a,**kwargs):\n",
    "    A(X,Z,**Z)\n",
    "\n",
    "Z={'a':3}\n",
    "B(1,Z,**Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with GPT tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_discovery.model.composer\n",
    "importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUTree,UnitSpec\n",
    "import model_discovery.evolution\n",
    "importlib.reload(model_discovery.evolution)\n",
    "from model_discovery.evolution import Implementation\n",
    "import copy\n",
    "\n",
    "\n",
    "tree_dir='../model_discovery/model/library/core/gpt2/units'\n",
    "gpt2_tree=GAUTree.load_from_base(tree_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='../ckpt/test_evo_000/db/designs/hybridflow'\n",
    "impl=Implementation.load(dir)\n",
    "itree=impl.implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itree._replace_unit('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itree.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,implemented,unimplemented=itree.view()\n",
    "print(implemented)\n",
    "print(unimplemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([1,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node=GAUNode(**dict['units']['GatedMLP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnitSpec.model_validate_json(node.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_copy=copy.deepcopy(gpt2_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_copy._replace_unit('GatedMLP','MLP2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tree.declares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "    '''\n",
    "    GAU Tree Map of Butterfly-AFT Generalized Autoregressive Block (BAGAB):\n",
    "    ButterflyAFTGAU (Rating: 3.5/5)\n",
    "        |- AFTMechanismGAU (Rating: 3.5/5)\n",
    "            |- PositionBiasGAU (Rating: 3.5/5)\n",
    "            |- ElementWiseOperationGAU (Rating: 3.5/5)\n",
    "        |- ButterflyTransformGAU (Rating: 3.5/5)\n",
    "            |- ButterflyLayerGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "            |- ButterflyInitializationGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "                |- ButterflyParameterGAU (Unimplemented)\n",
    "\n",
    "    Implemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ElementWiseOperationGAU, ButterflyAFTGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    Unimplemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ButterflyParameterGAU, ElementWiseOperationGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    '''\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ButterflyAFTGAU(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase\n",
    "\n",
    "\n",
    "class ButterflyAFTGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.aft_mechanism = AFTMechanismGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.butterfly_transform = ButterflyTransformGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        aft_output, Z = self.aft_mechanism(X, **Z)\n",
    "        butterfly_output, Z = self.butterfly_transform(aft_output, **Z)\n",
    "        return butterfly_output, Z\n",
    "\n",
    "\n",
    "class ButterflyTransformGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_layer = ButterflyLayerGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_init = ButterflyInitializationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.butterfly_init(X, **Z)\n",
    "        Y, Z = self.butterfly_layer(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ButterflyLayerGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.stages = nn.ModuleList([ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all) for _ in range(kwarg_all.get('num_stages', 4))])\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for stage in self.stages:\n",
    "            X, Z = stage(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "class ButterflyMatrixGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ButterflyInitializationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_size = kwarg_all.get('butterfly_size', embed_dim)\n",
    "        self.butterfly_matrices = nn.ParameterList([nn.Parameter(torch.\n",
    "            randn(self.butterfly_size, self.butterfly_size, **self.\n",
    "            factory_kwargs)) for _ in range(int(torch.log2(torch.tensor(\n",
    "            self.butterfly_size))))])\n",
    "        self.butterfly_stage = ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_parameter = ButterflyParameterGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for matrix in self.butterfly_matrices:\n",
    "            X = F.linear(X, matrix)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class ButterflyParameterGAU(GAUBase):\n",
    "    \"\"\"\n",
    "    Generalized Autoregressive Block Unit for learning parameters of Butterfly Factorization.\n",
    "    Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "    Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "    Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "    embed_dim:    The dimension of the input embeddings\n",
    "    block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "    kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.param_size = kwarg_all.get('param_size', 128)\n",
    "        self.butterfly_weights = nn.Parameter(torch.randn(self.param_size,\n",
    "            embed_dim, **self.factory_kwargs))\n",
    "        nn.init.xavier_uniform_(self.butterfly_weights)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X = torch.matmul(X, self.butterfly_weights)\n",
    "        Z_ = {'butterfly_weights': self.butterfly_weights}\n",
    "        return transformed_X, Z_\n",
    "        # transformed_X = F.linear(X, self.butterfly_weights.T)\n",
    "        # return transformed_X, {}\n",
    "\n",
    "\n",
    "class ButterflyComputationGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class ButterflyStageGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_params = ButterflyParameterGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X, Z = self.butterfly_params(X, **Z)\n",
    "        assert transformed_X.shape == X.shape, f'Output shape {transformed_X.shape} does not match input shape {X.shape}'\n",
    "        return transformed_X, Z\n",
    "\n",
    "class AFTMechanismGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = PositionBiasGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.element_wise_operation = ElementWiseOperationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.position_bias(X, **Z)\n",
    "        Y, Z = self.element_wise_operation(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ElementWiseOperationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, embed_dim, **self.\n",
    "            factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        queries = Z.get('queries', X)\n",
    "        keys = Z.get('keys', X)\n",
    "        values = Z.get('values', X)\n",
    "        elementwise_product = queries * keys\n",
    "        elementwise_sum = elementwise_product + values + self.position_bias\n",
    "        return elementwise_sum, {}\n",
    "\n",
    "\n",
    "class PositionBiasGAU(GAUBase):\n",
    "    \"\"\"Position Bias Generalized Autoregressive Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, 1, embed_dim, **\n",
    "            self.factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = X + self.position_bias\n",
    "        return Y, {}\n",
    "    \n",
    "\n",
    "class GABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100, **kwargs):\n",
    "        super().__init__()\n",
    "        self.gab = GAB(embed_dim, (0,1), **kwargs)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, Z = self.gab(X, **Z)\n",
    "        return self.lm_head(X),X\n",
    "\n",
    "\n",
    "\n",
    "gab_config = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100):\n",
    "        super().__init__()\n",
    "        self.butterfly_param = ButterflyParameterGAU(embed_dim, (0,1), {})\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, _ = self.butterfly_param(X, **Z)\n",
    "        return self.lm_head(X), X\n",
    "\n",
    "# Test the simplified model\n",
    "simple_model = GABLM(128)\n",
    "mock_input = torch.randn(1, 10, 128)\n",
    "mock_y = torch.randint(0, 100, (1, 10))\n",
    "\n",
    "optimizer = optim.Adam(simple_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for _ in range(5):  # Run for a few iterations\n",
    "    optimizer.zero_grad()\n",
    "    y, X = simple_model(mock_input)\n",
    "    loss = criterion(y.view(-1, 100), mock_y.view(-1))\n",
    "    loss.backward()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    for name, param in simple_model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"Gradient for {name}: {param.grad.abs().mean()}\")\n",
    "        else:\n",
    "            print(f\"No gradient for {name}\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim \n",
    "\n",
    "model=GABLM(128, **gab_config)\n",
    "vocab_size=100\n",
    "DEFAULT_CONTEXT_LENGTH=10\n",
    "\n",
    "mock_input=torch.randn(1, DEFAULT_CONTEXT_LENGTH, 128)\n",
    "mock_y=torch.randint(0, 100, (1, 10))\n",
    "mock_Z={}\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "y,X=model(mock_input, **mock_Z)\n",
    "loss=nn.CrossEntropyLoss()(y.view(-1, 100), mock_y.view(-1))\n",
    "loss.backward()\n",
    "\n",
    "used_params = set()\n",
    "\n",
    "# Hook to register which parameters are used during the forward pass\n",
    "def hook_fn(module, input, output):\n",
    "    for param in module.parameters():\n",
    "        used_params.add(param)\n",
    "\n",
    "# Register hooks for all submodules\n",
    "hooks = []\n",
    "for submodule in model.modules():\n",
    "    hook = submodule.register_forward_hook(hook_fn)\n",
    "    hooks.append(hook)\n",
    "\n",
    "# Forward pass again to collect used parameters\n",
    "model(mock_input)\n",
    "\n",
    "# Remove all hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "all_gradients_present = True\n",
    "\n",
    "# Check gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if param in used_params and param.grad is None:\n",
    "            # Used parameter without gradient -> Error\n",
    "            print(f\"Error: Used parameter {name} requires gradients but has none.\")\n",
    "            all_gradients_present = False\n",
    "        elif param not in used_params:\n",
    "            # Unused parameter -> Warning\n",
    "            print(f\"Warning: Parameter {name} was not used in the forward pass.\")\n",
    "\n",
    "if all_gradients_present:\n",
    "    print('Differentiability test passed')\n",
    "else:\n",
    "    print('Differentiability test failed due to missing gradients.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INIT_DESIGN_INPUT='''\n",
    "Below is the specification for the GAU you need to refine:\n",
    "Specification: Unit Name: GatedMLP The DynamicGatedMLP is an enhanced version of the GatedMLP that introduces a dynamic gating mechanism capable of selecting between multiple activation functions. This design aims to improve the expressiveness and adaptability of the model by allowing it to dynamically choose the most suitable activation function for different inputs. The unit maintains the same input-output interface as the original GatedMLP, ensuring seamless integration with the existing GPT-2 architecture.\n",
    "\n",
    "\n",
    "Inputs: X\n",
    "Outputs: Y\n",
    "Children list: []\n",
    "\n",
    "Current Implementation: import torch import torch.nn as nn from model_discovery.model.utils.modules import GAUBase, gau_test import torch.nn.functional as F\n",
    "\n",
    "class GatedMLP(GAUBase): \"\"\"Generalized Autoregressive Block Unit Input: X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of new intermediate variables to update the current Z} Constraints: Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "embed_dim:    The dimension of the input embeddings\n",
    "    block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "    kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "\"\"\"\n",
    "\n",
    "def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "    device=None, dtype=None, hidden_features=None, out_features=None,\n",
    "    bias=False, multiple_of=128, **kwargs):\n",
    "    self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "    out_features = out_features if out_features is not None else embed_dim\n",
    "    hidden_features = (hidden_features if hidden_features is not None else\n",
    "        int(8 * embed_dim / 3))\n",
    "    hidden_features = (hidden_features + multiple_of - 1\n",
    "        ) // multiple_of * multiple_of\n",
    "    self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n",
    "        self.factory_kwargs)\n",
    "    self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n",
    "        self.factory_kwargs)\n",
    "    self.activations = [F.relu, F.tanh, F.gelu]\n",
    "    self.gate_weights = nn.Parameter(torch.randn(len(self.activations),\n",
    "        hidden_features, **self.factory_kwargs))\n",
    "\n",
    "def _forward(self, X, **Z):\n",
    "    y = self.fc1(X)\n",
    "    y, gate = y.chunk(2, dim=-1)\n",
    "    gate_scores = F.softmax(self.gate_weights, dim=0)\n",
    "    y = sum(gate_scores[i] * self.activations[i](gate) for i in range(\n",
    "        len(self.activations)))\n",
    "    y = self.fc2(y)\n",
    "    return y, {}\n",
    "\n",
    "Review: ### Highlights:\n",
    "\n",
    "Successful Implementation Fix: The designer has successfully addressed the previous implementation error by initializing the activations attribute before its use, resolving the AttributeError.\n",
    "Dynamic Gating Mechanism: The design continues to leverage a dynamic gating mechanism, which is a novel approach that enhances the model's adaptability and expressiveness by selecting between multiple activation functions.\n",
    "Checker Validation: The updated implementation has passed all checker tests, including forward pass, causality, and differentiability, indicating that the GAU is functioning correctly within the model.\n",
    "Concerns:\n",
    "Efficiency Considerations: While the implementation is now functional, the potential computational overhead of evaluating multiple activation functions simultaneously remains a concern. This could impact efficiency, especially in larger models or datasets.\n",
    "Justification of Activation Functions: The choice of activation functions (ReLU, Tanh, GELU) is reasonable, but further theoretical justification or empirical validation would strengthen the design.\n",
    "Accuracy, Robustness, Efficiency, and Scalability:\n",
    "Accuracy and Robustness: The dynamic selection of activation functions should enhance the model's ability to adapt to various input patterns, potentially improving accuracy and robustness.\n",
    "Efficiency: The design could still benefit from optimizations to reduce computational overhead, ensuring it remains efficient in practice.\n",
    "Scalability: The design appears scalable, as indicated by the successful checker tests, but empirical validation on larger datasets would be beneficial.\n",
    "Novelty:\n",
    "The design introduces a novel dynamic gating mechanism, which is a significant improvement over static activation functions.\n",
    "Clarity and Completeness:\n",
    "The design is now clear and complete, with the previous implementation error resolved.\n",
    "Theoretical Soundness:\n",
    "The theoretical foundation of using dynamic gating is sound, and the implementation aligns well with the proposal's objectives.\n",
    "Implementation Feasibility:\n",
    "The current implementation is feasible and has been validated by the checker, indicating it can be integrated into the broader model.\n",
    "Suggestions:\n",
    "Efficiency Optimization: Consider exploring methods to optimize the dynamic gating mechanism to reduce computational overhead, such as selectively applying it or using more efficient computation techniques.\n",
    "Empirical Validation: Plan for empirical validation to confirm the theoretical benefits of the dynamic gating mechanism and the choice of activation functions in practice.\n",
    "Documentation: Ensure thorough documentation of the design choices and their theoretical justifications to aid future refinements and empirical testing.\n",
    "Rating: 4 out of 5 (Passing score >3)\n",
    "\n",
    "Reviewer Suggestions: 1. Efficiency Optimization: Consider exploring methods to optimize the dynamic gating mechanism to reduce computational overhead, such as selectively applying it or using more efficient computation techniques. 2. Empirical Validation: Plan for empirical validation to confirm the theoretical benefits of the dynamic gating mechanism and the choice of activation functions in practice.\n",
    "\n",
    "Documentation: Ensure thorough documentation of the design choices and their theoretical justifications to aid future refinements and empirical testing.\n",
    "Refinement Process\n",
    "If there is a review provided, you should start by reflecting on the feedback. Otherwise, leave reflection empty. The, proceed with the following:\n",
    "\n",
    "New Analysis and Design: - Provide an updated detailed analysis based on the feedback, including your new design direction and justifications. - Include a high-level pseudocode that captures the core of the new design. You should also provide the updated document of the GAU that allows other people to understand the design and implementation without the need to read the code.\n",
    "\n",
    "Implementation: - Provide the full updated implementation of the GAU, following the specified format and templates.\n",
    "\n",
    "Children list: - Provide the list of the children GAUs that are declared in the current GAU. You can declare new children GAUs or preserve the existing ones. If you do not declare any new children GAUs, you should provide the original children GAUs.\n",
    "\n",
    "Log of Changes: - Summarize the key changes you made during the refinement process. Including all code snippets where you made a change wrapped in python .\n",
    "\n",
    "Key Points to Remember:\n",
    "The bug or issue must always be resolved within the current GAU, as other units are either fully implemented and tested or placeholders that do not perform any computation.\n",
    "Ensure the GAU is self-contained, so you won't need to adjust it later when working on other units.\n",
    "The design must align with the original proposal and follow all instructions, templates, and format requirements.\n",
    "Use a top-down approach: break down complex operations into smaller tasks where necessary and declare each of them as a child GAU. Do not make a single unit overly complex.\n",
    "Remember your final goal is to refine the GAU in a way that enhances the overall design, ensuring both correctness and innovation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.prompts.prompts as P\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DESIGNER_SYSTEM=P.GUE_DESIGNER_SYSTEM(GAB_BASE=GAB_BASE,GAU_BASE=GAU_BASE,GAU_TEMPLATE=GAU_TEMPLATE)\n",
    "system=[{\"type\": \"text\", \"text\": DESIGNER_SYSTEM, \"cache_control\": {\"type\": \"ephemeral\"}}]\n",
    "\n",
    "# messages = [\n",
    "#     SystemMessage(system),\n",
    "#     HumanMessage(TEST_INIT_DESIGN_INPUT)\n",
    "# ]\n",
    "\n",
    "message=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": TEST_INIT_DESIGN_INPUT,\n",
    "            \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "\n",
    "SELECTIONS=['GPT2','RMSNorm','GatedMLP','MHA','RotaryPositionalEmbeddings']\n",
    "lc_model = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20240620\", temperature=0,\n",
    "    extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    ")\n",
    "structured_llm = lc_model.with_structured_output(P.gen_GUE_DESIGN_PROPOSAL(SELECTIONS).format,include_raw=True)\n",
    "tools_args=structured_llm.dict()['first']['steps__']['raw']['kwargs']\n",
    "        # tools[0]['cache_control']={\"type\": \"ephemeral\"}\n",
    "\n",
    "RET=anthropic.Anthropic().messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\", # model in config is ignored\n",
    "    max_tokens=8192,\n",
    "    messages=message, \n",
    "    temperature=0,\n",
    "    system=system, # claude does not has system role, system prompt must be passed separately\n",
    "    extra_headers=extra_headers,\n",
    "    **tools_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RET.dict()['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.gen_GUE_DESIGN_PROPOSAL(SELECTIONS).format.model_validate(RET.content[0].input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RET.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm.dict()['first']['steps__']['raw']['kwargs']['tools'][0]['input_schema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Message(id='msg_01Nvch8XBwzwrrMBETHpch49', content=[TextBlock(text=\"Thank you for providing the seed design and references. I'll now propose a design improvement for the GPT2 model, focusing on modifying one of its GAUs.\", type='text'), ToolUseBlock(id='toolu_01PpbFTQ6NDwJJCfj1cRDvFS', input={'selection': 'RotaryPositionalEmbeddings', 'modelname': 'GPT2-RoPE', 'proposal': \"\\n# GPT2 with Enhanced Rotary Position Embeddings (GPT2-RoPE)\\n\\n## Motivation\\n\\nThe current GPT2 model uses a basic implementation of Rotary Positional Embeddings (RoPE). While this is already an improvement over traditional positional encodings, there's room for enhancement, particularly in handling longer sequences and improving the model's ability to capture relative positions more effectively.\\n\\n## Problem Analysis\\n\\n1. **Limited sequence length handling**: The current implementation may not scale well to very long sequences, which is increasingly important in modern language models.\\n2. **Lack of flexibility**: The existing RoPE implementation doesn't allow for easy adjustment of its properties, such as the rate of position information decay.\\n3. **Potential for improved performance**: While RoPE is effective, recent research suggests that more sophisticated implementations can lead to better model performance, especially in tasks requiring understanding of long-range dependencies.\\n\\n## Core Idea and Philosophy\\n\\nWe propose to enhance the RotaryPositionalEmbeddings GAU by incorporating ideas from the RoFormer paper and the more recent developments in rotary embeddings. The core philosophy is to create a more flexible and powerful positional embedding system that can:\\n\\n1. Handle longer sequences more effectively\\n2. Allow for fine-tuning of the position encoding properties\\n3. Potentially improve the model's performance on tasks requiring long-range understanding\\n\\n## Design Plan\\n\\nWe will modify the RotaryPositionalEmbeddings GAU as follows:\\n\\n1. **Implement a more sophisticated RoPE class**: Based on the RoFormer paper and recent developments, we'll create a new RotaryEmbedding class that offers more flexibility and features.\\n\\n2. **Add support for learned frequencies**: This will allow the model to potentially learn better position representations for specific tasks or domains.\\n\\n3. **Implement interpolation factor**: This will enable the model to handle sequences longer than those seen during training more effectively.\\n\\n4. **Add support for XPos (Extrapolatable Position Embeddings)**: This will further enhance the model's ability to handle longer sequences and improve its understanding of relative positions.\\n\\n5. **Implement caching mechanism**: To improve efficiency, especially for autoregressive generation tasks.\\n\\n6. **Add support for different frequency types**: This will allow the embeddings to be optimized for different types of data (e.g., language, images).\\n\\nHere's a high-level overview of the proposed changes:\\n\\n```python\\nclass RotaryEmbedding(nn.Module):\\n    def __init__(\\n        self,\\n        dim,\\n        custom_freqs = None,\\n        freqs_for = 'lang',\\n        theta = 10000,\\n        max_freq = 10,\\n        num_freqs = 1,\\n        learned_freq = False,\\n        use_xpos = False,\\n        xpos_scale_base = 512,\\n        interpolate_factor = 1.,\\n        theta_rescale_factor = 1.,\\n        seq_before_head_dim = False,\\n        cache_if_possible = True\\n    ):\\n        super().__init__()\\n        # Implementation details...\\n\\n    def rotate_queries_or_keys(self, t, seq_dim = None, offset = 0, scale = None):\\n        # Implementation details...\\n\\n    def rotate_queries_and_keys(self, q, k, seq_dim = None):\\n        # Implementation details...\\n\\n    def get_scale(self, t, seq_len = None, offset = 0):\\n        # Implementation details...\\n\\n    def forward(self, t, seq_len = None, offset = 0):\\n        # Implementation details...\\n```\\n\\nThis new implementation will replace the current RotaryPositionalEmbeddings GAU in the GPT2 model.\\n\\n## Conclusion\\n\\nBy enhancing the RotaryPositionalEmbeddings GAU with these improvements, we expect to see several benefits:\\n\\n1. **Improved handling of long sequences**: The model should be able to maintain performance even on sequences longer than those seen during training.\\n2. **Better capture of relative positions**: The enhanced RoPE implementation should allow the model to better understand and utilize relative position information.\\n3. **Increased flexibility**: The new implementation allows for fine-tuning of various aspects of the positional embeddings, which can be optimized for specific tasks or domains.\\n4. **Potential performance improvements**: These enhancements may lead to improved performance on tasks requiring long-range understanding and more nuanced use of positional information.\\n\\nThis modification maintains the overall structure of the GPT2 model while significantly enhancing its ability to handle and utilize positional information, potentially leading to improved performance across a wide range of natural language processing tasks.\\n\\n## References\\n\\n1. Su, J., Lu, Y., Pan, S., Wen, B., & Liu, Y. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv preprint arXiv:2104.09864.\\n2. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint arXiv:2401.04088.\\n\"}, name='GUE_DESIGN_PROPOSAL_format', type='tool_use')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='tool_use', stop_sequence=None, type='message', usage=Usage(input_tokens=16630, output_tokens=1325))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content='Reflection on Feedback:\\nThe reviewer\\'s suggestions highlight the need for efficiency optimization, empirical validation, and thorough documentation. I\\'ll address these points in the refined design.\\n\\nNew Analysis and Design:\\n\\nThe current GatedMLP design introduces a dynamic gating mechanism that allows the model to select between multiple activation functions. While this approach enhances adaptability, it may introduce computational overhead. To address this, we\\'ll implement a more efficient gating mechanism that maintains the benefits of dynamic activation selection while reducing computational costs.\\n\\nUpdated Design Direction:\\n1. Efficient Gating: Instead of applying all activation functions and then combining them, we\\'ll use a lightweight gating network to predict weights for each activation function. This approach reduces the number of computations while still allowing dynamic selection.\\n\\n2. Sparse Activation: We\\'ll introduce a sparsity factor in the gating mechanism, allowing the model to focus on the most relevant activation functions for each input.\\n\\n3. Adaptive Computation: We\\'ll implement a mechanism to dynamically adjust the number of active gates based on the input complexity, further optimizing computational resources.\\n\\nHigh-level Pseudocode:\\n```\\nfunction GatedMLP(X):\\n    hidden = linear_projection(X)\\n    gate_weights = lightweight_gate_network(hidden)\\n    sparse_gate_weights = apply_sparsity(gate_weights)\\n    activated = sum(sparse_gate_weights[i] * activation[i](hidden) for i in range(num_activations))\\n    output = linear_projection(activated)\\n    return output\\n```\\n\\nUpdated GAU Document:\\nThe GatedMLP unit now incorporates an efficient dynamic gating mechanism that selects between multiple activation functions. Key features include:\\n- Lightweight gating network for predicting activation weights\\n- Sparse activation selection to focus on the most relevant functions\\n- Adaptive computation based on input complexity\\n- Maintains the same input-output interface as the original GatedMLP\\n\\nThis design aims to balance expressiveness and computational efficiency, allowing the model to adapt to various input patterns while managing resource usage effectively.\\n\\nImplementation:\\n\\n```python\\n# gau.py\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test\\n\\nclass GatedMLP(GAUBase):\\n    \"\"\"\\n    Generalized Autoregressive Block Unit with efficient dynamic gating\\n    Input:  X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \\n    Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of new intermediate variables to update the current Z}\\n    \"\"\"\\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\\n                 device=None, dtype=None, hidden_features=None, out_features=None,\\n                 bias=False, multiple_of=128, num_activations=3, sparsity_factor=0.5, **kwargs):\\n        self.factory_kwargs = {\\'device\\': device, \\'dtype\\': dtype}\\n        super().__init__(embed_dim, block_loc, kwarg_all)\\n        \\n        out_features = out_features if out_features is not None else embed_dim\\n        hidden_features = hidden_features if hidden_features is not None else int(8 * embed_dim / 3)\\n        hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of\\n        \\n        self.fc1 = nn.Linear(embed_dim, hidden_features, bias=bias, **self.factory_kwargs)\\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **self.factory_kwargs)\\n        \\n        self.gate_network = nn.Sequential(\\n            nn.Linear(hidden_features, 64, bias=False, **self.factory_kwargs),\\n            nn.ReLU(),\\n            nn.Linear(64, num_activations, bias=False, **self.factory_kwargs)\\n        )\\n        \\n        self.activations = [F.relu, F.tanh, F.gelu]\\n        self.sparsity_factor = sparsity_factor\\n\\n    def _forward(self, X, **Z):\\n        hidden = self.fc1(X)\\n        gate_weights = self.gate_network(hidden)\\n        \\n        # Apply softmax and sp'\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from core library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import importlib\n",
    "import model_discovery.model.composer \n",
    "# importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUBase, GAUTree, check_tree_name, GABComposer\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "import model_discovery.utils as U\n",
    "\n",
    "# dir='/home/junyanc/model_discovery/model_discovery/model/library/core/gpt2/units'\n",
    "dir='C://ChengJunyan1//Research//model_discovery//model_discovery//model//library//core//gpt2//units'\n",
    "tree=GAUTree.load_from_base(dir)\n",
    "\n",
    "print(tree.view()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "gam_prompt_path = 'C://ChengJunyan1//Research//model_discovery//model_discovery//agents//prompts//gam_prompt.py'\n",
    "gau_template_path = 'C://ChengJunyan1//Research//model_discovery//model_discovery//agents//prompts//gau_template.py'\n",
    "GAM_TEMPLATE=open(gam_prompt_path).read()\n",
    "GAU_TEMPLATE=open(gau_template_path).read()\n",
    "\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n",
    "GAB_BASE=inspect.getsource(GABBase)\n",
    "GAB_COMPOSER=inspect.getsource(GABComposer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GAM_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "c:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\library\\core\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18:20:38:08,138 INFO     [model_agent.py:184] Agent name=`designer`, model_details={\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"temperature\": 0.3\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized dataset wikitext-2 from C:\\ChengJunyan1\\Research\\model_discovery\\data\\wikitext-2\\tokenized\\meta-llama/Llama-2-7b-hf\\2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18:20:38:08,941 INFO     [model_agent.py:184] Agent name=`debugger`, model_details={\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-18:20:38:09,337 INFO     [model_agent.py:184] Agent name=`claude`, model_details={\n",
      "    \"model_name\": \"gpt-4o\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evoname': 'test_evo_000', 'scales': '14M,31M,70M', 'selection_ratio': '0.25', 'select_method': 'random', 'design_budget': '0'}\n",
      "Evolution system initialized with scales: ['14M', '31M', '70M']\n",
      "Current scale: 0\n",
      "Budgets remaining: {'70M': 1, '31M': 4, '14M': 16}\n",
      "Checkpoint directory: C:\\ChengJunyan1\\Research\\model_discovery\\ckpt\\test_evo_000\n",
      "gpt2 tree loaded\n",
      "Phylogenetic tree loaded with 300 nodes and 6 design sessions from C:\\ChengJunyan1\\Research\\model_discovery\\ckpt\\test_evo_000\\db.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18:20:38:10,613 INFO     [model_agent.py:184] Agent name=`designer`, model_details={\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"temperature\": 0.3\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized dataset wikitext-2 from C:\\ChengJunyan1\\Research\\model_discovery\\data\\wikitext-2\\tokenized\\meta-llama/Llama-2-7b-hf\\2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18:20:38:11,676 INFO     [model_agent.py:184] Agent name=`debugger`, model_details={\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-18:20:38:12,037 INFO     [model_agent.py:184] Agent name=`claude`, model_details={\n",
      "    \"model_name\": \"gpt-4o\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-18:20:38:12,356 INFO     [discover_namespace_packages.py:12] Discovering subpackages in _NamespacePath(['c:\\\\Users\\\\ChengJunyan1\\\\anaconda3\\\\envs\\\\modis\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "2024-09-18:20:38:12,358 INFO     [discover_plugins.py:9] Looking for plugins in pinecone_plugins.inference\n",
      "2024-09-18:20:38:12,408 INFO     [installation.py:10] Installing plugin inference into PineconeGRPC\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "import importlib\n",
    "import arxiv\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.utils as U\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "headers={\"X-API-KEY\": os.environ['S2_API_KEY']}\n",
    "paper_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}'\n",
    "\n",
    "\n",
    "strparams=[\n",
    "    f\"evoname=test_evo_000\",\n",
    "    \"scales=14M,31M,70M\",\n",
    "    \"selection_ratio=0.25\",\n",
    "    \"select_method=random\",\n",
    "    \"design_budget=0\",\n",
    "]\n",
    "evo_system = BuildEvolution(\n",
    "    strparams=';'.join(strparams),\n",
    "    do_cache=False,\n",
    "    # cache_type='diskcache',\n",
    ")\n",
    "\n",
    "ptree = evo_system.ptree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in ptree.filter_by_type(['DesignArtifact','DesignArtifactImplemented']):\n",
    "    node=ptree.get_node(node)\n",
    "    node.to_prompt()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['AdaptiveMHA', 'GatedMLP', 'RMSNorm']\n",
      "[]\n",
      "['GlobalAttention', 'AdaptiveAttentionRouter']\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "node=ptree.get_node('ahan-transformer')\n",
    "tree=node.implementation.implementation\n",
    "# tree._view(tree.root.spec.unitname, node=tree.root)\n",
    "for u in tree.units:\n",
    "    print(tree.units[u].children)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Tree Map of the GAUs\\n\\n<details><summary>Click to expand</summary>\\n\\n\\n#### ahan-ssi Tree Map\\n\\n```bash\\n  GPT2\\n       |- AdaptiveMHA (Rating: 4.97/5)\\n           |- GlobalAttention\\n           |- CausalLinearAttention (Unimplemented)\\n           |- AdaptiveAttentionRouter\\n       |- GatedMLP\\n       |- RMSNorm\\n\\nImplemented Units: AdaptiveAttentionRouter, GatedMLP, RMSNorm, AdaptiveMHA, GPT2, GlobalAttention\\nUnimplemented Units: CausalLinearAttention\\n```\\n\\n\\n#### Specifications for Implemented Units:\\n\\n##### Unit Name: GatedMLP\\n- Document:\\n```python\\n\\nGated MLP\\n  \\n```\\n- Inputs: X\\n- Outputs: Y\\n\\n\\n###### Implementation \\n\\n<details><summary>Click to expand</summary>\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\\n\\n\\nclass GatedMLP(GAUBase):\\n\\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\\n        device=None, dtype=None, hidden_features=None, out_features=None,\\n        activation=None, bias=False, multiple_of=128, **kwargs):\\n        self.factory_kwargs = {\\'device\\': device, \\'dtype\\': dtype}\\n        super().__init__(embed_dim, block_loc, kwarg_all)\\n        out_features = out_features if out_features is not None else embed_dim\\n        hidden_features = (hidden_features if hidden_features is not None else\\n            int(8 * embed_dim / 3))\\n        hidden_features = (hidden_features + multiple_of - 1\\n            ) // multiple_of * multiple_of\\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\\n            self.factory_kwargs)\\n        self.activation = activation if activation is not None else F.silu\\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\\n            self.factory_kwargs)\\n\\n    def _forward(self, X, **Z):\\n        y = self.fc1(X)\\n        y, gate = y.chunk(2, dim=-1)\\n        y = y * self.activation(gate)\\n        y = self.fc2(y)\\n        return y\\n\\n\\nCHILDREN_DECLARATIONS = []\\n\\n```\\n</details>\\n\\n---\\n\\n##### Unit Name: GPT2\\n- Document:\\n```python\\n\\nGPT2\\n  \\n```\\n- Inputs: X\\n- Outputs: Y\\n\\n\\n###### Implementation \\n\\n<details><summary>Click to expand</summary>\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\\n\\n\\nclass GPT2(GAUBase):\\n\\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\\n        device=None, dtype=None, **kwargs):\\n        self.factory_kwargs = {\\'device\\': device, \\'dtype\\': dtype}\\n        super().__init__(embed_dim, block_loc, kwarg_all)\\n        self.mha = AdaptiveMHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\\n            self.kwarg_all)\\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\\n            self.kwarg_all)\\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\\n            self.kwarg_all)\\n\\n    def _forward(self, X, **Z):\\n        X1, Z = self.norm1(X, **Z)\\n        X2, Z = self.mha(X1, **Z)\\n        X = X + X2\\n        X3, Z = self.norm2(X, **Z)\\n        X4, Z = self.mlp(X3, **Z)\\n        X = X + X4\\n        return X, Z\\n\\n\\nCHILDREN_DECLARATIONS = [UnitDecl(unitname=\\'MHA\\', requirements=\\'\\', inputs=[\\n    \\'X\\'], outputs=[\\'Y\\']), UnitDecl(unitname=\\'GatedMLP\\', requirements=\\'\\',\\n    inputs=[\\'X\\'], outputs=[\\'Y\\']), UnitDecl(unitname=\\'RMSNorm\\', requirements\\n    =\\'\\', inputs=[\\'X\\'], outputs=[\\'Y\\'])]\\n\\n```\\n</details>\\n\\n---\\n\\n##### Unit Name: RMSNorm\\n- Document:\\n```python\\n\\nRMSNorm\\n  \\n```\\n- Inputs: X\\n- Outputs: Y\\n\\n\\n###### Implementation \\n\\n<details><summary>Click to expand</summary>\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch import Tensor\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\\n\\n\\nclass RMSNorm(GAUBase):\\n\\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\\n        device=None, dtype=None, eps=1e-05, **kwargs):\\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\\n        group_size=None is equivalent to group_size=hidden_size (i.e. there\\'s only 1 group).\\n        \"\"\"\\n        self.factory_kwargs = {\\'device\\': device, \\'dtype\\': dtype}\\n        super().__init__(embed_dim, block_loc, kwarg_all)\\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\\n            )\\n        self.variance_epsilon = eps\\n\\n    def _forward(self, X, **Z):\\n        input_dtype = X.dtype\\n        X = X.to(torch.float32)\\n        variance = X.pow(2).mean(-1, keepdim=True)\\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\\n        return self.weight * X.to(input_dtype)\\n\\n\\nCHILDREN_DECLARATIONS = []\\n\\n```\\n</details>\\n\\n---\\n\\n##### Unit Name: GlobalAttention\\n- Document:\\n```python\\nGlobal Attention\\n\\nThis GAU implements standard multi-head self-attention to capture long-range dependencies.\\n\\nInputs:\\n    - X: Input sequence embeddings of shape (B, L, D).\\n\\nOutputs:\\n    - Y: Output sequence embeddings of the same shape as X.  \\n```\\n- Inputs: X\\n- Outputs: Y\\n\\n\\n###### Implementation \\n\\n<details><summary>Click to expand</summary>\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\\nimport torch.nn.functional as F\\nfrom einops import rearrange\\n\\n\\nclass GlobalAttention(GAUBase):\\n    \"\"\"\\n    Global Attention\\n\\n    This GAU implements standard multi-head self-attention to capture long-range dependencies.\\n\\n    Inputs:\\n        - X: Input sequence embeddings of shape (B, L, D).\\n\\n    Outputs:\\n        - Y: Output sequence embeddings of the same shape as X.\\n    \"\"\"\\n\\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\\n        n_heads: int=8, causal: bool=True, device=None, dtype=None, **kwargs):\\n        self.factory_kwargs = {\\'device\\': device, \\'dtype\\': dtype}\\n        super().__init__(embed_dim, block_loc, kwarg_all)\\n        self.n_heads = n_heads\\n        self.head_dim = embed_dim // n_heads\\n        self.causal = causal\\n        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, **self.\\n            factory_kwargs)\\n        self.out_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\\n\\n    def _forward(self, X, **Z):\\n        \"\"\"\\n        Forward pass of GlobalAttention.\\n\\n        Args:\\n            X (Tensor): Input sequence of shape (B, L, D).\\n\\n        Returns:\\n            Y (Tensor): Output sequence of shape (B, L, D).\\n        \"\"\"\\n        B, L, D = X.shape\\n        qkv = self.qkv_proj(X)\\n        q, k, v = qkv.chunk(3, dim=-1)\\n        q = rearrange(q, \\'b l (h d) -> b h l d\\', h=self.n_heads)\\n        k = rearrange(k, \\'b l (h d) -> b h l d\\', h=self.n_heads)\\n        v = rearrange(v, \\'b l (h d) -> b h l d\\', h=self.n_heads)\\n        attn_scores = torch.einsum(\\'b h i d, b h j d -> b h i j\\', q, k\\n            ) / self.head_dim ** 0.5\\n        if self.causal:\\n            mask = torch.triu(torch.ones(L, L, device=X.device), diagonal=1\\n                ).bool()\\n            attn_scores = attn_scores.masked_fill(mask[None, None, :, :],\\n                float(\\'-inf\\'))\\n        attn_probs = F.softmax(attn_scores, dim=-1)\\n        attn_output = torch.matmul(attn_probs, v)\\n        attn_output = rearrange(attn_output, \\'b h l d -> b l (h d)\\')\\n        Y = self.out_proj(attn_output)\\n        return Y, Z\\n\\n```\\n</details>\\n\\n---\\n\\n##### Unit Name: AdaptiveAttentionRouter\\n- Document:\\n```python\\nAdaptiveAttentionRouter computes routing weights for different attention types for each token.\\n\\n**Inputs**:\\n- **X** (*Tensor*): Input embeddings of shape `(batch_size, seq_length, embed_dim)`.\\n- **Z** (*dict*, optional): Contains optional intermediate variables:\\n    - **\\'positional_ids\\'** (*Tensor*, optional): Positional indices of shape `(batch_size, seq_length)`.\\n      If not provided, defaults to `[0, 1, ..., seq_length - 1]` for each sequence in the batch.\\n    - **\\'sequence_lengths\\'** (*Tensor*, optional): Actual lengths of sequences in the batch of shape `(batch_size,)`.\\n      If not provided, defaults to `seq_length` for all sequences.\\n\\n**Outputs**:\\n- **Y** (*Tensor*): Output embeddings (same as input `X`), shape `(batch_size, seq_length, embed_dim)`.\\n- **Z\\\\_** (*dict*): Updated intermediate variables containing:\\n    - **\\'routing_weights\\'** (*Tensor*): Routing weights for attention types,\\n      shape `(batch_size, seq_length, num_attention_types)`.\\n\\n**Purpose**:\\nThe AdaptiveAttentionRouter dynamically computes routing weights for multiple attention mechanisms\\n(e.g., global, linear) for each token in the input sequence. It considers the token embeddings,\\npositional information, and sequence lengths to produce a softmax distribution over attention types,\\nenabling the model to adaptively select the most appropriate attention mechanism for each token.\\n\\n**Details**:\\n- Uses a small neural network (MLP) to compute routing weights.\\n- Concatenates input features: token embeddings, positional embeddings, and sequence length encoding.\\n- Normalizes routing weights using softmax to ensure they sum to 1 across attention types.\\n- Handles missing positional encodings and sequence lengths by providing default values.\\n\\n**Example**:\\n\\n    >>> router = AdaptiveAttentionRouter(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    >>> X = torch.randn(2, 1024, 512)\\n    >>> Y, Z = router(X)  \\n```\\n- Inputs: X\\n- Outputs: routing_weights\\n\\n\\n###### Implementation \\n\\n<details><summary>Click to expand</summary>\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\\nimport torch.nn.functional as F\\nimport math\\n\\n\\nclass AdaptiveAttentionRouter(GAUBase):\\n    \"\"\"\\n    AdaptiveAttentionRouter computes routing weights for different attention types for each token.\\n\\n    **Inputs**:\\n    - **X** (*Tensor*): Input embeddings of shape `(batch_size, seq_length, embed_dim)`.\\n    - **Z** (*dict*, optional): Contains optional intermediate variables:\\n        - **\\'positional_ids\\'** (*Tensor*, optional): Positional indices of shape `(batch_size, seq_length)`.\\n          If not provided, defaults to `[0, 1, ..., seq_length - 1]` for each sequence in the batch.\\n        - **\\'sequence_lengths\\'** (*Tensor*, optional): Actual lengths of sequences in the batch of shape `(batch_size,)`.\\n          If not provided, defaults to `seq_length` for all sequences.\\n\\n    **Outputs**:\\n    - **Y** (*Tensor*): Output embeddings (same as input `X`), shape `(batch_size, seq_length, embed_dim)`.\\n    - **Z\\\\\\\\_** (*dict*): Updated intermediate variables containing:\\n        - **\\'routing_weights\\'** (*Tensor*): Routing weights for attention types,\\n          shape `(batch_size, seq_length, num_attention_types)`.\\n\\n    **Purpose**:\\n    The AdaptiveAttentionRouter dynamically computes routing weights for multiple attention mechanisms\\n    (e.g., global, linear) for each token in the input sequence. It considers the token embeddings,\\n    positional information, and sequence lengths to produce a softmax distribution over attention types,\\n    enabling the model to adaptively select the most appropriate attention mechanism for each token.\\n\\n    **Details**:\\n    - Uses a small neural network (MLP) to compute routing weights.\\n    - Concatenates input features: token embeddings, positional embeddings, and sequence length encoding.\\n    - Normalizes routing weights using softmax to ensure they sum to 1 across attention types.\\n    - Handles missing positional encodings and sequence lengths by providing default values.\\n\\n    **Example**:\\n\\n        >>> router = AdaptiveAttentionRouter(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n        >>> X = torch.randn(2, 1024, 512)\\n        >>> Y, Z = router(X)\\n\\n    \"\"\"\\n\\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\\n        device=None, dtype=None, num_attention_types=2, router_hidden_dim=\\n        None, positional_dim=None, length_dim=1, **kwargs):\\n        self.factory_kwargs = {\\'device\\': device, \\'dtype\\': dtype}\\n        super().__init__(embed_dim, block_loc, kwarg_all)\\n        if router_hidden_dim is None:\\n            router_hidden_dim = embed_dim // 2\\n        if positional_dim is None:\\n            positional_dim = embed_dim // 4\\n        self.num_attention_types = num_attention_types\\n        self.positional_dim = positional_dim\\n        self.length_dim = length_dim\\n        hidden_dim = router_hidden_dim\\n        input_dim = embed_dim + positional_dim + length_dim\\n        self.routing_network = nn.Sequential(nn.Linear(input_dim,\\n            hidden_dim, **self.factory_kwargs), nn.ReLU(), nn.Linear(\\n            hidden_dim, num_attention_types, **self.factory_kwargs))\\n\\n    def _forward(self, X, **Z):\\n        batch_size, seq_length, _ = X.shape\\n        positional_ids = Z.get(\\'positional_ids\\', None)\\n        if positional_ids is None:\\n            positional_ids = torch.arange(seq_length, device=X.device\\n                ).unsqueeze(0).expand(batch_size, seq_length)\\n        positional_embeddings = self.get_positional_embeddings(positional_ids,\\n            X.dtype)\\n        sequence_lengths = Z.get(\\'sequence_lengths\\', None)\\n        if sequence_lengths is None:\\n            sequence_lengths = torch.full((batch_size,), seq_length, device\\n                =X.device, dtype=torch.long)\\n        sequence_length_encoding = sequence_lengths.unsqueeze(1).repeat(1,\\n            seq_length).unsqueeze(-1).to(dtype=X.dtype)\\n        input_features = torch.cat([X, positional_embeddings,\\n            sequence_length_encoding], dim=-1)\\n        routing_logits = self.routing_network(input_features)\\n        routing_weights = torch.softmax(routing_logits, dim=-1)\\n        Y = X\\n        Z_ = {\\'routing_weights\\': routing_weights}\\n        return Y, Z_\\n\\n    def get_positional_embeddings(self, positional_ids, dtype):\\n        positions = positional_ids\\n        position_encodings = self.sinusoidal_embedding(positions, dtype)\\n        return position_encodings\\n\\n    def sinusoidal_embedding(self, positions, dtype):\\n        batch_size, seq_length = positions.shape\\n        dim = self.positional_dim\\n        positions = positions.to(dtype=dtype).unsqueeze(-1)\\n        div_term = torch.exp(torch.arange(0, dim, 2, device=positions.\\n            device, dtype=dtype) * -(math.log(10000.0) / dim))\\n        pe = torch.zeros(batch_size, seq_length, dim, device=positions.\\n            device, dtype=dtype)\\n        pe[..., 0::2] = torch.sin(positions * div_term)\\n        pe[..., 1::2] = torch.cos(positions * div_term)\\n        return pe\\n\\n```\\n</details>\\n\\n---\\n\\n##### Unit Name: AdaptiveMHA\\n- Document:\\n```python\\nAdaptive Multi-Head Attention (AdaptiveMHA)\\n\\nThis GAU implements the Adaptive Hybrid Attention Network (AHAN) design by incorporating multiple attention mechanisms:\\n\\n- **Global Attention**: Captures long-range dependencies using standard full-range self-attention.\\n- **Causal Linear Attention**: Efficiently handles very long sequences using a causal linearized attention mechanism.\\n- **Adaptive Attention Router**: Dynamically computes routing weights for attention types based on input characteristics, allowing the model to adaptively combine attention outputs.\\n\\n**Inputs**:\\n    - **X** (*Tensor*): Input sequence embeddings of shape (B, L, D), where B is batch size, L is sequence length, D is embedding dimension.\\n    - **Z** (*dict*): Intermediate variables.\\n\\n**Outputs**:\\n    - **Y** (*Tensor*): Output sequence embeddings of the same shape as X.\\n    - **Z\\\\_** (*dict*): Updated intermediate variables.\\n\\n**Example**:\\n\\n    >>> attention = AdaptiveMHA(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    >>> X = torch.randn(2, 1024, 512)\\n    >>> Y, Z = attention(X)  \\n```\\n- Inputs: X\\n- Outputs: Y\\n\\n\\n###### Implementation \\n\\n<details><summary>Click to expand</summary>\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\\nimport torch.nn.functional as F\\nfrom einops import rearrange\\n\\n\\nclass AdaptiveMHA(GAUBase):\\n    \"\"\"\\n    Adaptive Multi-Head Attention (AdaptiveMHA)\\n\\n    This GAU implements the Adaptive Hybrid Attention Network (AHAN) design by incorporating multiple attention mechanisms:\\n\\n    - **Global Attention**: Captures long-range dependencies using standard full-range self-attention.\\n    - **Causal Linear Attention**: Efficiently handles very long sequences using a causal linearized attention mechanism.\\n    - **Adaptive Attention Router**: Dynamically computes routing weights for attention types based on input characteristics, allowing the model to adaptively combine attention outputs.\\n\\n    **Inputs**:\\n        - **X** (*Tensor*): Input sequence embeddings of shape (B, L, D), where B is batch size, L is sequence length, D is embedding dimension.\\n        - **Z** (*dict*): Intermediate variables.\\n\\n    **Outputs**:\\n        - **Y** (*Tensor*): Output sequence embeddings of the same shape as X.\\n        - **Z\\\\\\\\_** (*dict*): Updated intermediate variables.\\n\\n    **Example**:\\n\\n        >>> attention = AdaptiveMHA(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n        >>> X = torch.randn(2, 1024, 512)\\n        >>> Y, Z = attention(X)\\n\\n    \"\"\"\\n\\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\\n        n_heads: int=8, device=None, dtype=None, num_attention_types=2, **\\n        kwargs):\\n        self.factory_kwargs = {\\'device\\': device, \\'dtype\\': dtype}\\n        super().__init__(embed_dim, block_loc, kwarg_all)\\n        self.n_heads = n_heads\\n        self.head_dim = embed_dim // n_heads\\n        self.global_attention = GlobalAttention(embed_dim=self.embed_dim,\\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\\n            self.factory_kwargs, **self.kwarg_all)\\n        self.linear_attention = CausalLinearAttention(embed_dim=\\n            self.embed_dim, block_loc=self.block_loc, kwarg_all=\\n            self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\\n        self.attention_router = AdaptiveAttentionRouter(embed_dim=\\n            self.embed_dim, block_loc=self.block_loc, kwarg_all=\\n            self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\\n        self.out_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\\n        num_attention_types = num_attention_types\\n\\n    def _forward(self, X, **Z):\\n        \"\"\"\\n        Forward pass of AdaptiveMHA.\\n\\n        Args:\\n            X (Tensor): Input sequence of shape (B, L, D).\\n\\n        Returns:\\n            Y (Tensor): Output sequence of shape (B, L, D).\\n            Z (dict): Updated intermediate variables.\\n        \"\"\"\\n        _, Z = self.attention_router(X, **Z)\\n        routing_weights = Z.get(\\'routing_weights\\', None)\\n        if routing_weights is None:\\n            routing_weights = torch.full((X.shape[0], X.shape[1], 2), 0.5,\\n                device=X.device, dtype=X.dtype)\\n        global_context, Z = self.global_attention(X, **Z)\\n        linear_context, Z = self.linear_attention(X, **Z)\\n        contexts = torch.stack([global_context, linear_context], dim=2)\\n        routing_weights = routing_weights.unsqueeze(-1)\\n        weighted_contexts = contexts * routing_weights\\n        combined_context = weighted_contexts.sum(dim=2)\\n        Y = self.out_proj(combined_context)\\n        return Y, Z\\n\\n```\\n</details>\\n\\n---\\n\\n\\n##### Declarations for Unimplemented Units:\\n\\n##### Unit Name: CausalLinearAttention\\n- Requirements: \\n- Inputs: X\\n- Outputs: Y\\n\\n---\\n\\n</details>\\n\\n## Composed LM Block Code\\n<details><summary>Click to expand</summary>\\n\\n```python\\n\\n# gab.py    # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom model_discovery.model.utils.modules import GABBase # DO NOT CHANGE THIS IMPORT STATEMENT #\\n\\n\\nclass GAB(GABBase):\\n    def __init__(self,embed_dim: int, block_loc: tuple, device=None,dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\\n        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to nn layers\\n        super().__init__(embed_dim, block_loc) # DO NOT CHANGE THIS LINE #\\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\\n\\n    def _forward(self, X, **Z): \\n        X, Z = self.root(X, **Z)\\n        return X, Z\\n\\n... # Unit implementations\\n```\\n</details>\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Proposal: ahan-ssi\n",
      "\n",
      "\n",
      "# Title: Adaptive Hybrid Attention Network with Selective State Space Integration (AHAN-SSI)\n",
      "\n",
      "## 1. Motivation\n",
      "Current large language models face challenges in efficiently processing long sequences while maintaining high performance across various tasks. While the original AHAN design introduced adaptive routing of attention mechanisms, recent advancements in selective state space models (SSMs) and hybrid architectures offer opportunities for further improvement in efficiency and long-range dependency modeling.\n",
      "\n",
      "## 2. Problem Analysis\n",
      "The main challenges in LM block design include:\n",
      "1. Balancing computational efficiency with model expressiveness\n",
      "2. Effectively capturing both local and global dependencies\n",
      "3. Adapting to varying input characteristics and task requirements\n",
      "4. Maintaining performance on long sequences without excessive computational costs\n",
      "5. Ensuring scalability to very large models\n",
      "\n",
      "## 3. Core Idea and Philosophy\n",
      "We propose AHAN-SSI, an evolution of the original AHAN design that integrates selective SSMs with adaptive hybrid attention. The core philosophy is to leverage the strengths of both SSMs (efficient long-range dependency modeling) and attention mechanisms (flexible, content-based processing) while dynamically adapting the model's behavior based on input characteristics.\n",
      "\n",
      "## 4. Design Plan\n",
      "\n",
      "### 4.1 Overview\n",
      "AHAN-SSI consists of the following main components:\n",
      "1. Selective State Space Module (SSSM)\n",
      "2. Multi-type Attention Module (MAM)\n",
      "3. Adaptive Integration Router (AIR)\n",
      "4. Dynamic Context Processor (DCP)\n",
      "\n",
      "### 4.2 Selective State Space Module (SSSM)\n",
      "This module is inspired by the Mamba architecture and implements a selective SSM:\n",
      "\n",
      "- State equations:\n",
      "  ```\n",
      "  h'(t) = A(x(t)) * h(t) + B(x(t)) * x(t)\n",
      "  y(t) = C * h(t)\n",
      "  ```\n",
      "  where A(x(t)), B(x(t)) are functions of the input, allowing for content-based selection.\n",
      "\n",
      "- Discretization using ZOH (Zero-Order Hold) method:\n",
      "  ```\n",
      "  A_d = exp( * A(x(t)))\n",
      "  B_d = (A(x(t))^-1) * (exp( * A(x(t))) - I) *  * B(x(t))\n",
      "  ```\n",
      "\n",
      "- Hardware-aware parallel algorithm for efficient computation during both training and inference.\n",
      "\n",
      "### 4.3 Multi-type Attention Module (MAM)\n",
      "This module retains the original AHAN's multi-type attention but with improvements:\n",
      "\n",
      "a) Global Attention: Standard full-range attention for capturing long-range dependencies\n",
      "b) Local Attention: Sliding window attention for efficient processing of local contexts\n",
      "c) Linear Attention: Linearized attention for handling very long sequences\n",
      "\n",
      "Attention computation:\n",
      "```\n",
      "Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
      "```\n",
      "\n",
      "### 4.4 Adaptive Integration Router (AIR)\n",
      "This component dynamically determines the optimal combination of SSSM and MAM outputs:\n",
      "\n",
      "```python\n",
      "def adaptive_integration_router(sssm_output, mam_output, input_features):\n",
      "    routing_weights = mlp(concat(input_features, sssm_output, mam_output))\n",
      "    integrated_output = routing_weights[0] * sssm_output + routing_weights[1] * mam_output\n",
      "    return integrated_output\n",
      "```\n",
      "\n",
      "### 4.5 Dynamic Context Processor (DCP)\n",
      "This component enhances the model's ability to handle varying context lengths:\n",
      "\n",
      "```python\n",
      "def dynamic_context_processor(input_sequence, current_context_size):\n",
      "    importance_scores = compute_importance(input_sequence)\n",
      "    pruned_sequence = top_k(input_sequence, importance_scores, k=current_context_size)\n",
      "    return pruned_sequence\n",
      "```\n",
      "\n",
      "### 4.6 Overall Architecture\n",
      "The AHAN-SSI block processes input as follows:\n",
      "\n",
      "1. Input sequence goes through the DCP for adaptive context pruning\n",
      "2. Processed input is fed into both SSSM and MAM in parallel\n",
      "3. AIR combines the outputs of SSSM and MAM\n",
      "4. The integrated output is passed through a feed-forward network and layer normalization\n",
      "\n",
      "Multiple AHAN-SSI blocks are stacked to form the complete model.\n",
      "\n",
      "## 5. Implementation Guidelines\n",
      "\n",
      "1. Implement the SSSM:\n",
      "   - Use efficient CUDA kernels for parallel computation of selective SSM\n",
      "   - Optimize the discretization process for both training and inference\n",
      "\n",
      "2. Enhance the MAM:\n",
      "   - Implement efficient attention variants (e.g., flash attention for global attention)\n",
      "   - Optimize local attention with efficient sliding window operations\n",
      "\n",
      "3. Develop the AIR:\n",
      "   - Design a small, efficient neural network for computing routing weights\n",
      "   - Implement efficient integration of SSSM and MAM outputs\n",
      "\n",
      "4. Implement the DCP:\n",
      "   - Develop an efficient importance scoring mechanism\n",
      "   - Implement dynamic pruning that adapts to varying context sizes\n",
      "\n",
      "5. Combine components:\n",
      "   - Ensure efficient information flow between components\n",
      "   - Implement residual connections and layer normalization\n",
      "\n",
      "6. Optimization:\n",
      "   - Use mixed-precision training for memory efficiency\n",
      "   - Implement gradient checkpointing for very long sequences\n",
      "   - Utilize model parallelism for large-scale versions\n",
      "\n",
      "## 6. Theoretical Analysis\n",
      "\n",
      "1. Complexity Analysis:\n",
      "   - Time complexity: O(N log N) average case, due to the combination of linear-time SSM and efficient attention mechanisms\n",
      "   - Space complexity: O(N), achieved through dynamic pruning and efficient state representation\n",
      "\n",
      "2. Expressiveness:\n",
      "   - The combination of selective SSM and multi-type attention allows AHAN-SSI to capture both long-range dependencies and local patterns effectively\n",
      "   - The adaptive integration mechanism enables the model to flexibly adjust its focus between global and local information\n",
      "\n",
      "3. Adaptability:\n",
      "   - Dynamic parameter adaptation through selective SSM and adaptive routing allows the model to adjust its behavior based on input characteristics\n",
      "   - The dynamic context processor enables efficient handling of varying sequence lengths\n",
      "\n",
      "## 7. Expected Improvements\n",
      "\n",
      "1. Efficiency: Faster inference and training on long sequences compared to full attention models\n",
      "2. Performance: Improved handling of long-range dependencies, potentially surpassing both pure SSM and pure attention models\n",
      "3. Adaptability: Better performance across a wide range of tasks and input types due to the dynamic nature of the architecture\n",
      "4. Scalability: More efficient scaling to larger model sizes and longer sequences\n",
      "\n",
      "## 8. Potential Challenges and Mitigations\n",
      "\n",
      "1. Challenge: Increased model complexity\n",
      "   Mitigation: Careful ablation studies to ensure each component contributes significantly to performance\n",
      "\n",
      "2. Challenge: Training instability due to dynamic routing\n",
      "   Mitigation: Implement gradient stabilization techniques and carefully tuned learning rate schedules\n",
      "\n",
      "3. Challenge: Balancing SSM and attention usage\n",
      "   Mitigation: Extensive experimentation to find optimal integration strategies, potentially using neural architecture search\n",
      "\n",
      "## 9. Conclusion\n",
      "\n",
      "AHAN-SSI represents a novel approach to LM block design, combining the strengths of selective state space models and adaptive attention mechanisms. By dynamically integrating these components and adapting to input characteristics, AHAN-SSI aims to achieve state-of-the-art performance in language modeling while maintaining computational efficiency, especially for long sequences. This design opens new avenues for creating more flexible and powerful language models capable of handling a wide range of tasks and input types.\n",
      "\n",
      "\n",
      "## Review\n",
      "\n",
      "The AHAN-SSI (Adaptive Hybrid Attention Network with Selective State Space Integration) proposal presents an innovative approach to improving autoregressive language model design, particularly for long-range language modeling. The proposal combines elements from recent advancements in State Space Models (SSMs) and adaptive attention mechanisms in a novel way.\n",
      "\n",
      "Clarity: The proposal is generally well-articulated, with clear objectives to improve efficiency and performance in long-range language modeling. The overall architecture, including the Selective State Space Module (SSSM), Multi-type Attention Module (MAM), Adaptive Integration Router (AIR), and Dynamic Context Processor (DCP), is well-defined. However, some aspects of the implementation, particularly the specific algorithms for adaptive routing and dynamic pruning, could benefit from more detailed explanations.\n",
      "\n",
      "Innovation: The AHAN-SSI design introduces several innovative elements that build upon recent research in hybrid SSM-attention models. The combination of multiple attention types (global, local, linear) with a selective SSM, managed by an adaptive router, appears to be a unique contribution. This approach aligns with current trends in hybrid architectures but offers a more comprehensive and adaptive approach than existing models like Mamba-2-Hybrid or Jamba.\n",
      "\n",
      "Feasibility: The proposed design appears feasible within the given framework, as it builds upon established techniques in SSMs and attention mechanisms. However, the integration of multiple adaptive components (AIR, DCP) may present implementation challenges, particularly in terms of training stability and optimization. Recent research on hybrid models suggests that careful balancing of SSM and attention components is crucial for optimal performance.\n",
      "\n",
      "Scalability: The proposal shows promise in terms of scalability. The use of selective SSMs and efficient attention mechanisms should allow the model to handle very long sequences (up to 128K tokens) without significant performance degradation. Recent studies on models like Jamba demonstrate the scalability of hybrid SSM-Transformer architectures to large sizes (e.g., 52B parameters), suggesting that AHAN-SSI could potentially scale to very large model sizes.\n",
      "\n",
      "Accuracy and Robustness: The combination of multiple attention types with selective SSMs has the potential to improve model performance across a wide range of tasks, particularly those involving long-range dependencies. The adaptive nature of the model, with its dynamic routing and context processing, could enhance its ability to handle diverse inputs. However, empirical validation is needed to confirm these potential benefits, especially for challenging tasks like multi-document question answering, which have been identified as areas of difficulty for some hybrid models.\n",
      "\n",
      "Efficiency: The proposed design offers several avenues for improving computational efficiency and memory usage. The claimed O(N log N) time complexity and O(N) space complexity are competitive with other efficient architectures like Mamba. The use of selective SSMs and the dynamic context processor could potentially reduce memory requirements and computational costs, especially for long sequences. However, the actual efficiency gains would need to be empirically verified, as the integration of multiple adaptive components may introduce some overhead.\n",
      "\n",
      "### Rating: 4.2 out of 5\n",
      "\n",
      "### Reviewer Suggestions\n",
      "\n",
      "1. Provide more detailed algorithms and pseudocode for the Adaptive Integration Router (AIR) and Dynamic Context Processor (DCP) to clarify their implementation and potential optimizations.\n",
      "\n",
      "2. Include a more comprehensive analysis of the potential challenges in training and optimizing a model with multiple adaptive components. Address strategies for ensuring training stability and efficient convergence.\n",
      "\n",
      "3. Conduct and present preliminary experiments or simulations to support the claimed benefits of the proposed architecture, particularly in comparison to existing hybrid models like Mamba-2-Hybrid and Jamba.\n",
      "\n",
      "4. Elaborate on the strategy for balancing the ratio of SSM to attention layers and their placement within the network. Recent research suggests this is crucial for optimal performance in hybrid models.\n",
      "\n",
      "5. Provide a more detailed discussion on how the model will handle multi-document and potentially multi-modal inputs, as these have been identified as challenging areas for some hybrid models.\n",
      "\n",
      "6. Include a plan for empirical validation of the adaptive mechanisms (AIR and DCP) and their impact on model performance, efficiency, and generalization across different types of tasks and input lengths.\n",
      "\n",
      "7. Discuss potential approaches for scaling the model to very large sizes, addressing any foreseen challenges in maintaining the efficiency of the adaptive components at scale.\n",
      "\n",
      "8. Consider incorporating recent advancements in efficient attention mechanisms, such as the shared attention module approach used in Zamba, to potentially further reduce parameter count while maintaining performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ptree.get_node('ahan-ssi').to_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.search_utils\n",
    "importlib.reload(model_discovery.agents.search_utils)\n",
    "\n",
    "from model_discovery.agents.search_utils import SuperScholarSearcher, pwc_search_patched\n",
    "\n",
    "sss=SuperScholarSearcher(ptree,evo_system.stream)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss._load_libs()\n",
    "# sss._load_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retry when recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "pdfsp_dir=U.pjoin(sss.files_dir,'pdfsp')\n",
    "\n",
    "for i in tqdm(sss.libp):\n",
    "    if 'externalIds' in sss.libp[i]:\n",
    "        if 'ArXiv' in sss.libp[i]['externalIds']:\n",
    "            arxiv_id=sss.libp[i]['externalIds']['ArXiv']\n",
    "            path=U.pjoin(pdfsp_dir,f'{i}.pdf')\n",
    "            arxiv_pdf=f'https://arxiv.org/pdf/{arxiv_id}'\n",
    "            if U.pexists(path):\n",
    "                continue\n",
    "            try:\n",
    "                pdf=urllib.request.urlretrieve(arxiv_pdf, path)\n",
    "                time.sleep(0.01)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
