{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place for experimenting the progressive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.utils as U\n",
    "from model_discovery.configs.gam_config import GAMConfig, GAMConfig_14M\n",
    "from model_discovery.model.composer import GAUTree,GAUBase,GAUNode\n",
    "# from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "\n",
    "prompts_dir='../model_discovery/agents/prompts/'\n",
    "gab_py = U.read_file(U.pjoin(prompts_dir,'gab_template.py'))\n",
    "gam_py = U.read_file(U.pjoin(prompts_dir,'gam_prompt.py'))\n",
    "GAU_TEMPLATE = U.read_file(U.pjoin(prompts_dir,'gau_template.py'))\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os,sys\n",
    "import importlib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "import model_discovery.agents.prompts.prompts as P\n",
    "import model_discovery.model.composer\n",
    "importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUTree,GAUBase,GAUNode\n",
    "import model_discovery.utils\n",
    "importlib.reload(model_discovery.utils)\n",
    "import model_discovery.utils as U\n",
    "\n",
    "\n",
    "\n",
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "# Placeholder classes for future implementation\n",
    "class MemoryAccessUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, memory_size, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "class DownsamplingUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, downsample_factor, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "\n",
    "class XXX(GAUBase,object):\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "\n",
    "class GAU(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
    "        # argv: list of hyperparameters\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\n",
    "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        self.a1 = kwarg_all.get('a1', 1)\n",
    "        self.a2: int = kwarg_all.get('a2', 2)\n",
    "\n",
    "        # COMPLETING THE CODE HERE #\n",
    "        self.token_scorer = TokenScoringGAU(embed_dim, **factory_kwargs)\n",
    "        self.dual_path: GAUBase = DualPathGAU(embed_dim, **factory_kwargs)\n",
    "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim, **factory_kwargs)\n",
    "\n",
    "        self.stages: nn.ModuleList = nn.ModuleList([CompressionStageGAU(embed_dim, device=\n",
    "            device, dtype=dtype, **kwargs) for _ in range(2)])\n",
    "\n",
    "        self.modelx=nn.ModuleList([\n",
    "            nn.Conv2d(1,20,5),\n",
    "            nn.ReLU(),\n",
    "            LatentAttentionGAU(),\n",
    "            nn.Conv2d(20,64,5),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "            \n",
    "\n",
    "        self.model1 = (\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU(),\n",
    "            'gau3': CompressionStageGAU()\n",
    "        })\n",
    "\n",
    "        self.model2 = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('gau2', DualPathGAU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "    def init_something(self):\n",
    "        self.modelx[3]=LatentAttentionGAU()\n",
    "        self.modelx.append(LatentAttentionGAU())\n",
    "        self.modelx.insert(0,LatentAttentionGAU())\n",
    "        self.activations['gau1']=LatentAttentionGAU()\n",
    "        self.model2.add_module('gau4',LatentAttentionGAU())\n",
    "\n",
    "        X=self.dual_path(X)\n",
    "    \n",
    "    def _forward(self, X, **Z): \n",
    "\n",
    "        # THE CODE HERE MUST BE COMPLETED #\n",
    "        # Step 1: Score tokens\n",
    "        X, Z = self.token_scorer(X, **Z)\n",
    "        # Step 2: Route through dual paths\n",
    "        # Step 3: Apply latent attention\n",
    "        Y, Z = self.latent_attention(X, A=A, B=B, **Z)\n",
    "        for stage in self.stages:\n",
    "            X = stage(X, **Z)\n",
    "        \n",
    "        X, Z = self.dual_path(X, **Z)\n",
    "        X=self.activations['gau3'](X)\n",
    "\n",
    "        return Y, Z\n",
    "\n",
    "@gau_test\n",
    "def test1(a=1,device=None):\n",
    "    print('test1')\n",
    "\n",
    "    \n",
    "@gau_test\n",
    "def test2(device=None):\n",
    "    print('test2')\n",
    "\n",
    "'''\n",
    "children = ['TokenScoringGAU', 'DualPathGAU', 'LatentAttentionGAU','CompressionStageGAU'] # solve redundancy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "class RelevanceScoringGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT KEEP THE ORIGINAL ONES #\n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        # Initialize a linear projection layer\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim // 2, **self.factory_kwargs)\n",
    "        self.projection.weight.requires_grad = True\n",
    "        self.projection.bias.requires_grad = True\n",
    "\n",
    "        # Initialize a learnable query vector\n",
    "        self.query_vector = nn.Parameter(torch.randn(embed_dim // 2, requires_grad=True, **self.factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        # Project the input embeddings\n",
    "        projected_X = self.projection(X)\n",
    "\n",
    "        # Compute similarity scores\n",
    "        similarity_scores = torch.matmul(projected_X, self.query_vector)\n",
    "\n",
    "        # Normalize the similarity scores\n",
    "        relevance_scores = F.softmax(similarity_scores, dim=-1)\n",
    "\n",
    "        # Weight the input embeddings with relevance scores\n",
    "        Y = X * relevance_scores.unsqueeze(-1)\n",
    "\n",
    "        # Return weighted embeddings and updated Z\n",
    "        return Y, {}\n",
    "\n",
    "\n",
    "# WRITE YOUR UNIT TEST FUNCTIONS HERE #\n",
    "\n",
    "@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON'T BE RECOGNIZED AS A UNIT TEST #\n",
    "def test_relevance_scoring_gau(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\n",
    "    # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\n",
    "\n",
    "    # Initialize the GAU\n",
    "    embed_dim = 512\n",
    "    block_loc = (0, 6)\n",
    "    relevance_scoring_gau = RelevanceScoringGAU(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={}, device=device, dtype=dtype)\n",
    "\n",
    "    # Mock input\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "\n",
    "    # Forward pass\n",
    "    Y, Z_ = relevance_scoring_gau(X, **Z)\n",
    "\n",
    "    # Assertions\n",
    "    assert Y.shape == (batch_size, seq_len, embed_dim), \"Output shape mismatch\"\n",
    "    assert isinstance(Z_, dict), \"Z_ should be a dictionary\"\n",
    "\n",
    "    # Print for debugging\n",
    "    print(\"Output embeddings:\", Y)\n",
    "    print(\"Updated Z:\", Z_)\n",
    "\n",
    "    # Additional checks\n",
    "    assert torch.allclose(Y.sum(dim=-1), X.sum(dim=-1), atol=1e-6), \"Weighted embeddings should preserve the sum along the embedding dimension\"\n",
    "\n",
    "    print(\"RelevanceScoringGAU test passed!\")\n",
    "'''\n",
    "children=['ProjectionGAU','SimilarityComputationGAU','NormalizationGAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "class EnhancedGatedMLP(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, hidden_features=None, out_features=None, activation=None, bias=False, multiple_of=128, **kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT KEEP THE ORIGINAL ONES #\n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        out_features = out_features if out_features is not None else embed_dim\n",
    "        hidden_features = (hidden_features if hidden_features is not None else int(8 * embed_dim / 3))\n",
    "        hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of\n",
    "\n",
    "        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **self.factory_kwargs)\n",
    "        self.activation = activation if activation is not None else F.silu\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **self.factory_kwargs)\n",
    "\n",
    "        # Feedback loop components\n",
    "        self.feedback_fc = nn.Linear(out_features, hidden_features, bias=bias, **self.factory_kwargs)\n",
    "        self.memory_buffer = nn.Parameter(torch.zeros(hidden_features, **self.factory_kwargs))\n",
    "        self.layer_norm = nn.LayerNorm(hidden_features, **self.factory_kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        # Forward pass through the first linear layer\n",
    "        y = self.fc1(X)\n",
    "        y, gate = y.chunk(2, dim=-1)\n",
    "        y = y * self.activation(gate)\n",
    "\n",
    "        # Feedback loop\n",
    "        feedback = self.feedback_fc(y)\n",
    "        feedback = self.layer_norm(feedback + self.memory_buffer)\n",
    "\n",
    "        # Update memory buffer\n",
    "        self.memory_buffer.data = feedback.detach()\n",
    "\n",
    "        # Forward pass through the second linear layer\n",
    "        y = self.fc2(feedback)\n",
    "        return y, {}\n",
    "\n",
    "\n",
    "# WRITE YOUR UNIT TEST FUNCTIONS HERE #\n",
    "\n",
    "@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON'T BE RECOGNIZED AS A UNIT TEST #\n",
    "def test_enhanced_gated_mlp(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\n",
    "    # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\n",
    "\n",
    "    embed_dim = 64\n",
    "    block_loc = (0, 1)\n",
    "    kwarg_all = {}\n",
    "    model = EnhancedGatedMLP(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, device=device, dtype=dtype)\n",
    "\n",
    "    X = torch.randn(2, 10, embed_dim, device=device, dtype=dtype)\n",
    "    Y, Z = model(X)\n",
    "\n",
    "    assert Y.shape == X.shape, f\"Expected output shape {X.shape}, but got {Y.shape}\"\n",
    "    assert isinstance(Z, dict), \"Expected Z to be a dictionary\"\n",
    "\n",
    "    print(\"EnhancedGatedMLP test passed!\")\n",
    "\n",
    "    # WRITE ASSERTIONS TO PERFORM THE TEST, USE PRINT TO DEBUG #\n",
    "\n",
    "    # raise NotImplementedError # YOU MUST IMPLEMENT THIS FUNCTION #\n",
    "'''\n",
    "\n",
    "children=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reformatted Code:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
      "import torch.nn.functional as F\n",
      "\n",
      "\n",
      "class MHA(GAUBase):\n",
      "    \"\"\"Generalized Autoregressive Block Unit\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
      "\n",
      "        embed_dim:    The dimension of the input embeddings\n",
      "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
      "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
      "        device=None, dtype=None, hidden_features=None, out_features=None,\n",
      "        activation=None, bias=False, multiple_of=128, **kwargs):\n",
      "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "        out_features = out_features if out_features is not None else embed_dim\n",
      "        hidden_features = (hidden_features if hidden_features is not None else\n",
      "            int(8 * embed_dim / 3))\n",
      "        hidden_features = (hidden_features + multiple_of - 1\n",
      "            ) // multiple_of * multiple_of\n",
      "        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n",
      "            self.factory_kwargs)\n",
      "        self.activation = activation if activation is not None else F.silu\n",
      "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n",
      "            self.factory_kwargs)\n",
      "        self.feedback_fc = nn.Linear(out_features, hidden_features, bias=\n",
      "            bias, **self.factory_kwargs)\n",
      "        self.memory_buffer = nn.Parameter(torch.zeros(hidden_features, **\n",
      "            self.factory_kwargs))\n",
      "        self.layer_norm = nn.LayerNorm(hidden_features, **self.factory_kwargs)\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        y = self.fc1(X)\n",
      "        y, gate = y.chunk(2, dim=-1)\n",
      "        y = y * self.activation(gate)\n",
      "        feedback = self.feedback_fc(y)\n",
      "        feedback = self.layer_norm(feedback + self.memory_buffer)\n",
      "        self.memory_buffer.data = feedback.detach()\n",
      "        y = self.fc2(feedback)\n",
      "        return y, {}\n",
      "\n",
      "Errors:\n",
      " Error: GAUBase class MHA not found, found a GAUBase class EnhancedGatedMLP instead. Please make sure the GAUBase class name is the same as the unit name. If you are refining a unit, you should keep the name of the class unchanged. Do not use the new name to rename the class. The checker will continue the checking process by assuming this class is the unit class.\n",
      "Warnings:\n",
      " \n",
      "New Arguments:\n",
      " {'hidden_features': None, 'out_features': None, 'activation': None, 'bias': False, 'multiple_of': 128}\n"
     ]
    }
   ],
   "source": [
    "unit_name = \"MHA\"  # Provide the unit_name to rename GAU class\n",
    "reformatted_code, new_args, gau_tests, errors, warnings, fetal_errors = check_and_reformat_gau_code(code, unit_name,children)\n",
    "print(\"Reformatted Code:\\n\" + reformatted_code)\n",
    "print(\"Errors:\\n\", '\\n\\t'.join(errors+fetal_errors))\n",
    "print(\"Warnings:\\n\", '\\n\\t'.join(warnings))\n",
    "print(\"New Arguments:\\n\", new_args)\n",
    "\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "spec=P.UnitSpec(\n",
    "   unitname = unit_name,\n",
    "   document = 'Test',\n",
    "   inputs = [],\n",
    "   outputs = [],\n",
    ")\n",
    "test_tree.add_unit(\n",
    "    spec, reformatted_code, new_args, None, None, None, children, gau_tests, None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a professional AI researcher focusing on discovering the best\n",
      "autoregressive language model block. Your goal is to design a novel block\n",
      "following the Generalized Autoregressive Block (GAB) structure defined in the\n",
      "following base class:\n",
      "\n",
      "```python class GABBase(nn.Module):\n",
      " \"\"\" Base class for Generalized Autoregressive Block \"\"\"\n",
      " def __init__(self,embed_dim: int, block_loc: tuple):\n",
      " super().__init__()\n",
      " self.embed_dim = embed_dim\n",
      " self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\n",
      "\n",
      " def _forward(self, X, **Z): \n",
      " raise NotImplementedError\n",
      " \n",
      " # YOU ARE NOT ALLOW TO OVERRIDE THIS METHOD #\n",
      " def forward(self, X, **Z): # kwargs not parsable by torchscript but more flexible\n",
      " \"\"\"Forward pass of the model\"\"\"\n",
      " assert len(X.shape) == 3, f\"Input shape must be (batch, seqlen, embed_dim), got {len(X.shape)}D instead\"\n",
      " assert X.shape[-1] == self.embed_dim, f\"Input shape must be (batch, seqlen, embed_dim), got {X.shape} instead\"\n",
      " Y = self._forward(X, **Z)\n",
      " if isinstance(Y, tuple):\n",
      " Y, Z_ = Y\n",
      " else:\n",
      " Z_ = {}\n",
      " assert Y.shape == X.shape, f\"GAB Output shape must be the same as input shape of {X.shape}, got {Y.shape} instead\"\n",
      " assert isinstance(Z, dict), \"Intermediate variables must be stored in a dict\"\n",
      " Z.update(Z_) # the new intermediate variables are updated to the current Z\n",
      " return Y, Z\n",
      " ```\n",
      "\n",
      "The GAB will be used to construct a Generalized Autoregressive Model (GAM)\n",
      "defined as follows:\n",
      "\n",
      "```python from transformers.modeling_outputs import CausalLMOutput\n",
      "from transformers import PreTrainedModel, PretrainedConfig\n",
      "from dataclasses import dataclass\n",
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "\n",
      "from gab import GAB, gab_config\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class GAMConfig(PretrainedConfig):\n",
      " '''Configurations for Generalized Autoregressive Models.'''\n",
      "\n",
      " d_model: int\n",
      " n_block: int\n",
      " batch_tokens: int \n",
      " vocab_size: int = None\n",
      "\n",
      "\n",
      "class GAM(nn.Module):\n",
      " ''' Generalized Autoregressive Models\n",
      " Input: X: (batch, seqlen, embed_dim)\n",
      " Output: Y: (batch, seqlen, embed_dim)\n",
      " '''\n",
      " def __init__(\n",
      " self,\n",
      " d_model: int,\n",
      " n_block: int,\n",
      " vocab_size: int = 50277,\n",
      " norm_epsilon: float = 1e-5,\n",
      " device = None,\n",
      " dtype = None,\n",
      " ) -> None:\n",
      " self.factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      " super().__init__()\n",
      " self.d_model = d_model\n",
      " self.embedding = nn.Embedding(vocab_size, d_model, **self.factory_kwargs)\n",
      "\n",
      " block_config = gab_config()\n",
      " self.blocks = nn.ModuleList(\n",
      " [\n",
      " GAB(\n",
      " embed_dim=d_model, \n",
      " block_loc=(layer_idx,n_block),\n",
      " device=device, \n",
      " dtype=dtype, \n",
      " **block_config\n",
      " )\n",
      " for layer_idx in range(n_block)\n",
      " ]\n",
      " )\n",
      " self.norm_out = nn.LayerNorm(\n",
      " d_model, eps=norm_epsilon, **self.factory_kwargs\n",
      " )\n",
      "\n",
      " def forward(self, input_ids):\n",
      " hidden_states = self.embedding(input_ids)\n",
      " intermediate_vars = {}\n",
      " for block in self.blocks:\n",
      " hidden_states, intermediate_vars = block(\n",
      " hidden_states,\n",
      " **intermediate_vars\n",
      " )\n",
      " hidden_states = self.norm_out(hidden_states)\n",
      " return hidden_states\n",
      "\n",
      "\n",
      "class GLMHeadModel(PreTrainedModel):\n",
      " ''' Generalized Autoregressive Models with LM Head '''\n",
      " config_class = GAMConfig\n",
      "\n",
      " def __init__(\n",
      " self,\n",
      " config: GAMConfig,\n",
      " device=None,\n",
      " dtype=None,\n",
      " ) -> None:\n",
      " super().__init__(config)\n",
      " factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      " self.backbone = GAM(\n",
      " d_model=config.d_model,\n",
      " n_block=config.n_block,\n",
      " vocab_size=config.vocab_size,\n",
      " **factory_kwargs,\n",
      " )\n",
      " self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False, **factory_kwargs)\n",
      "\n",
      " def forward(self, input_ids, **gab_kwargs):\n",
      " hidden_states = self.backbone(input_ids, **gab_kwargs)\n",
      " lm_logits = self.lm_head(hidden_states)\n",
      " return CausalLMOutput(logits=lm_logits)\n",
      " ```\n",
      "\n",
      "The produced language model will be pretrained with the corpus and then be\n",
      "applied for downstream tasks. The new model is expected to have a low\n",
      "perplexity, high accuracy, robustness, efficiency, and most importantly, good\n",
      "scalability. \n",
      "\n",
      "Since the autoregressive model design is complicated, so we will break it down\n",
      "into smaller parts. We represent a block as multiple nested units, the\n",
      "Generalized Autoregressive Unit (GAU). Each GAU accepts a sequence of embeddings\n",
      "X and a dictionary of intermediate variables Z as input, and outputs a sequence\n",
      "of embeddings Y and a dictionary of new or updated intermediate variables Z_. Z_\n",
      "is optional, when it is provided, it will be used to update Z for the next unit\n",
      "by Z.update(Z_). A GAU is defined in the following base class:\n",
      "\n",
      "```python class GAUBase(nn.Module): \n",
      " \"\"\" \n",
      " Instead of directly giving the full implementation of a GAB block, the agent need to \n",
      " design a series of nested GAB units and construct the full GAB block as a pipeline of these units.\n",
      "\n",
      " GAB is fractal, like GAB itself, each GAB unit accepts X and Z as input and returns Y and Z as output.\n",
      " \"\"\" \n",
      " def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict):\n",
      " super().__init__()\n",
      " self.embed_dim = embed_dim\n",
      " self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\n",
      " self.kwarg_all = kwarg_all # dict of all kwargs, use it to initialize the children units\n",
      "\n",
      " def _forward(self, X, **Z): # you can define the arguments instead of using **Z\n",
      " raise NotImplementedError\n",
      " \n",
      " def forward(self, X, **Z):\n",
      " assert len(X.shape) == 3 and X.shape[-1] == self.embed_dim, f\"Input X must be a sequence of shape (batch, seqlen, embed_dim), all other kinds of variables should be passed by Z. Got {X.shape} instead.\"\n",
      " _params = inspect.signature(self._forward).parameters\n",
      " X=X.to(**self.factory_kwargs)\n",
      " _Z = {k: v for k, v in Z.items() if k in _params}\n",
      " Y = self._forward(X, **_Z)\n",
      " if isinstance(Y, tuple):\n",
      " Y, Z_ = Y\n",
      " else:\n",
      " Z_ = {}\n",
      " assert Y.shape == X.shape, f\"GAU output Y must be a sequence with the same shape as input of {X.shape}, got {Y.shape} instead, all other kinds of variables should be passed by Z.\"\n",
      " assert isinstance(Z_, dict), \"Intermediate variables must be stored in a dict\"\n",
      " Z.update(Z_) # the new intermediate variables are updated to the current Z\n",
      " return Y, Z\n",
      " ```\n",
      "\n",
      "You will design a GAU by completing the blanks marked in this template, which\n",
      "includes the initialization where you can define your custom arguments with\n",
      "optional default values, the forward function where you can define convenient\n",
      "functions or classes in the GAB class such as caches, notice that you are only\n",
      "allowed to have only one GAU which inherited from the GAUBase class in the file:\n",
      " \n",
      "```python # gau.py # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "from model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
      "\n",
      "\n",
      "# YOU CAN IMPORT MORE MODULES HERE #\n",
      "\n",
      "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
      "\n",
      "\n",
      "class UnitName(GAUBase):\n",
      " \"\"\"Generalized Autoregressive Block Unit\n",
      " Input: X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
      " Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      " Constraints: Causal, differentiable, parameter number, complexity, parallelizable...\n",
      "\n",
      " embed_dim: The dimension of the input embeddings\n",
      " block_loc: The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
      " kwarg_all: A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
      " \"\"\"\n",
      " def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT KEEP THE ORIGINAL ONES #\n",
      " self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\n",
      " super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\n",
      " \n",
      " # COMPLETING THE CODE HERE #\n",
      "\n",
      " raise NotImplementedError\n",
      "\n",
      "\n",
      " # YOU CAN ADD MORE FUNCTIONS HERE #\n",
      "\n",
      "\n",
      " def _forward(self, X, **Z): \n",
      " \n",
      " # THE CODE HERE MUST BE COMPLETED #\n",
      "\n",
      " raise NotImplementedError\n",
      "\n",
      "\n",
      "# WRITE YOUR UNIT TEST FUNCTIONS HERE #\n",
      "\n",
      "@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON'T BE RECOGNIZED AS A UNIT TEST #\n",
      "def unit_test_name(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\n",
      " # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\n",
      "\n",
      " # WRITE ASSERTIONS TO PERFORM THE TEST, USE PRINT TO DEBUG #\n",
      " \n",
      " raise NotImplementedError # YOU MUST IMPLEMENT THIS FUNCTION #\n",
      " ```\n",
      "\n",
      "In a GAU, you can call other GAUs, as such, you can create a complicated GAB\n",
      "block by nesting multiple GAUs. However, each GAU should be not too complex, if\n",
      "you want to create complex block, you should break it down into smaller GAUs and\n",
      "nest them. As such, you should design a GAB block in a top-down manner. \n",
      "\n",
      "Instead of starting from scratch, you will start from an existing design and\n",
      "improve it. You will be provided with the full information of the design,\n",
      "including the proposal, the tree structure, the implementations of the GAUs. You\n",
      "are only allowed to modify *one GAU* from the existing design. \n",
      "\n",
      "You need to select one GAU to modify, you can define new children GAUs, however\n",
      "you need to guarantee that your modification wont affect the correctness of the\n",
      "overall design. \n",
      "\n",
      "You will start by writing down an overal proposal for the design you want to\n",
      "have, the proposal decides a direction, phylosophy and the plan of the design,\n",
      "and the analysis of the problem and how you gonna solve it by modifying one GAU\n",
      "from the existing design. You will be provided with one or multiple references\n",
      "to consider that may inspire you if there are references provided.\n",
      "\n",
      "Your response should include: \n",
      "\n",
      "1. The proposal, it should include but not restrict to the following parts: a. A\n",
      " title with the name of the design in the level 1 header format. You shuld\n",
      " have only one level 1 header in your response which is the name of the\n",
      " design.\n",
      "\n",
      " b. Your motivation of the design. What problem you want to solve based on the\n",
      " insights or observations you have about the autoregressive models today,\n",
      " and any inspirations you may have from the references. \n",
      "\n",
      " c. The analysis of the problem.\n",
      "\n",
      " d. The core idea and phylosophy behind of your design that may solve the\n",
      " problem you proposed. \n",
      "\n",
      " e. The plan of the design. You should include subsections of that describe\n",
      " the details of each part of the design with the justifications. The\n",
      " selection of the GAU to modify and the reasoning of the selection. \n",
      "\n",
      " f. A conclution of the proposal. \n",
      "\n",
      " g. Optional, the references you used in your proposal, should be in the right\n",
      " format.\n",
      "2. The name of the variant of the model you are going to design.\n",
      "3. The selection of the GAU to modify.\n",
      "\n",
      "The proposal will be reviewed and you will be asked to modify it if it is not\n",
      "passed. You can start to implement the design after the proposal is passed. \n",
      "\n",
      "The proposal should be as detailed as possible, DO NOT WORRY IF THE PROPOSAL IS\n",
      "TOO LONG, BUT ALSO DO NOT FILL IN BY REDUNDANT WORDS, USE PRECISE AND CONCRETE\n",
      "LANGUAGE, the proposal will be the guideline for the entire design process so it\n",
      "should be clear and detailed. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x=[{'role': 'system', 'content': '\\nYou are a professional AI researcher focusing on discovering the best\\nautoregressive language model block. Your goal is to design a novel block\\nfollowing the Generalized Autoregressive Block (GAB) structure defined in the\\nfollowing base class:\\n\\n```python class GABBase(nn.Module):\\n \"\"\" Base class for Generalized Autoregressive Block \"\"\"\\n def __init__(self,embed_dim: int, block_loc: tuple):\\n super().__init__()\\n self.embed_dim = embed_dim\\n self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\\n\\n def _forward(self, X, **Z): \\n raise NotImplementedError\\n \\n # YOU ARE NOT ALLOW TO OVERRIDE THIS METHOD #\\n def forward(self, X, **Z): # kwargs not parsable by torchscript but more flexible\\n \"\"\"Forward pass of the model\"\"\"\\n assert len(X.shape) == 3, f\"Input shape must be (batch, seqlen, embed_dim), got {len(X.shape)}D instead\"\\n assert X.shape[-1] == self.embed_dim, f\"Input shape must be (batch, seqlen, embed_dim), got {X.shape} instead\"\\n Y = self._forward(X, **Z)\\n if isinstance(Y, tuple):\\n Y, Z_ = Y\\n else:\\n Z_ = {}\\n assert Y.shape == X.shape, f\"GAB Output shape must be the same as input shape of {X.shape}, got {Y.shape} instead\"\\n assert isinstance(Z, dict), \"Intermediate variables must be stored in a dict\"\\n Z.update(Z_) # the new intermediate variables are updated to the current Z\\n return Y, Z\\n ```\\n\\nThe GAB will be used to construct a Generalized Autoregressive Model (GAM)\\ndefined as follows:\\n\\n```python from transformers.modeling_outputs import CausalLMOutput\\nfrom transformers import PreTrainedModel, PretrainedConfig\\nfrom dataclasses import dataclass\\n\\nimport torch\\nfrom torch import nn\\n\\nfrom gab import GAB, gab_config\\n\\n\\n@dataclass\\nclass GAMConfig(PretrainedConfig):\\n \\'\\'\\'Configurations for Generalized Autoregressive Models.\\'\\'\\'\\n\\n d_model: int\\n n_block: int\\n batch_tokens: int \\n vocab_size: int = None\\n\\n\\nclass GAM(nn.Module):\\n \\'\\'\\' Generalized Autoregressive Models\\n Input: X: (batch, seqlen, embed_dim)\\n Output: Y: (batch, seqlen, embed_dim)\\n \\'\\'\\'\\n def __init__(\\n self,\\n d_model: int,\\n n_block: int,\\n vocab_size: int = 50277,\\n norm_epsilon: float = 1e-5,\\n device = None,\\n dtype = None,\\n ) -> None:\\n self.factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n super().__init__()\\n self.d_model = d_model\\n self.embedding = nn.Embedding(vocab_size, d_model, **self.factory_kwargs)\\n\\n block_config = gab_config()\\n self.blocks = nn.ModuleList(\\n [\\n GAB(\\n embed_dim=d_model, \\n block_loc=(layer_idx,n_block),\\n device=device, \\n dtype=dtype, \\n **block_config\\n )\\n for layer_idx in range(n_block)\\n ]\\n )\\n self.norm_out = nn.LayerNorm(\\n d_model, eps=norm_epsilon, **self.factory_kwargs\\n )\\n\\n def forward(self, input_ids):\\n hidden_states = self.embedding(input_ids)\\n intermediate_vars = {}\\n for block in self.blocks:\\n hidden_states, intermediate_vars = block(\\n hidden_states,\\n **intermediate_vars\\n )\\n hidden_states = self.norm_out(hidden_states)\\n return hidden_states\\n\\n\\nclass GLMHeadModel(PreTrainedModel):\\n \\'\\'\\' Generalized Autoregressive Models with LM Head \\'\\'\\'\\n config_class = GAMConfig\\n\\n def __init__(\\n self,\\n config: GAMConfig,\\n device=None,\\n dtype=None,\\n ) -> None:\\n super().__init__(config)\\n factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n self.backbone = GAM(\\n d_model=config.d_model,\\n n_block=config.n_block,\\n vocab_size=config.vocab_size,\\n **factory_kwargs,\\n )\\n self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False, **factory_kwargs)\\n\\n def forward(self, input_ids, **gab_kwargs):\\n hidden_states = self.backbone(input_ids, **gab_kwargs)\\n lm_logits = self.lm_head(hidden_states)\\n return CausalLMOutput(logits=lm_logits)\\n ```\\n\\nThe produced language model will be pretrained with the corpus and then be\\napplied for downstream tasks. The new model is expected to have a low\\nperplexity, high accuracy, robustness, efficiency, and most importantly, good\\nscalability. \\n\\nSince the autoregressive model design is complicated, so we will break it down\\ninto smaller parts. We represent a block as multiple nested units, the\\nGeneralized Autoregressive Unit (GAU). Each GAU accepts a sequence of embeddings\\nX and a dictionary of intermediate variables Z as input, and outputs a sequence\\nof embeddings Y and a dictionary of new or updated intermediate variables Z_. Z_\\nis optional, when it is provided, it will be used to update Z for the next unit\\nby Z.update(Z_). A GAU is defined in the following base class:\\n\\n```python class GAUBase(nn.Module): \\n \"\"\" \\n Instead of directly giving the full implementation of a GAB block, the agent need to \\n design a series of nested GAB units and construct the full GAB block as a pipeline of these units.\\n\\n GAB is fractal, like GAB itself, each GAB unit accepts X and Z as input and returns Y and Z as output.\\n \"\"\" \\n def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict):\\n super().__init__()\\n self.embed_dim = embed_dim\\n self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\\n self.kwarg_all = kwarg_all # dict of all kwargs, use it to initialize the children units\\n\\n def _forward(self, X, **Z): # you can define the arguments instead of using **Z\\n raise NotImplementedError\\n \\n def forward(self, X, **Z):\\n assert len(X.shape) == 3 and X.shape[-1] == self.embed_dim, f\"Input X must be a sequence of shape (batch, seqlen, embed_dim), all other kinds of variables should be passed by Z. Got {X.shape} instead.\"\\n _params = inspect.signature(self._forward).parameters\\n X=X.to(**self.factory_kwargs)\\n _Z = {k: v for k, v in Z.items() if k in _params}\\n Y = self._forward(X, **_Z)\\n if isinstance(Y, tuple):\\n Y, Z_ = Y\\n else:\\n Z_ = {}\\n assert Y.shape == X.shape, f\"GAU output Y must be a sequence with the same shape as input of {X.shape}, got {Y.shape} instead, all other kinds of variables should be passed by Z.\"\\n assert isinstance(Z_, dict), \"Intermediate variables must be stored in a dict\"\\n Z.update(Z_) # the new intermediate variables are updated to the current Z\\n return Y, Z\\n ```\\n\\nYou will design a GAU by completing the blanks marked in this template, which\\nincludes the initialization where you can define your custom arguments with\\noptional default values, the forward function where you can define convenient\\nfunctions or classes in the GAB class such as caches, notice that you are only\\nallowed to have only one GAU which inherited from the GAUBase class in the file:\\n \\n```python # gau.py # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\\n\\n\\n# YOU CAN IMPORT MORE MODULES HERE #\\n\\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\\n\\n\\nclass UnitName(GAUBase):\\n \"\"\"Generalized Autoregressive Block Unit\\n Input: X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \\n Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\\n Constraints: Causal, differentiable, parameter number, complexity, parallelizable...\\n\\n embed_dim: The dimension of the input embeddings\\n block_loc: The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\\n kwarg_all: A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\\n \"\"\"\\n def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT KEEP THE ORIGINAL ONES #\\n self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\\n super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\\n \\n # COMPLETING THE CODE HERE #\\n\\n raise NotImplementedError\\n\\n\\n # YOU CAN ADD MORE FUNCTIONS HERE #\\n\\n\\n def _forward(self, X, **Z): \\n \\n # THE CODE HERE MUST BE COMPLETED #\\n\\n raise NotImplementedError\\n\\n\\n# WRITE YOUR UNIT TEST FUNCTIONS HERE #\\n\\n@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON\\'T BE RECOGNIZED AS A UNIT TEST #\\ndef unit_test_name(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\\n # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\\n\\n # WRITE ASSERTIONS TO PERFORM THE TEST, USE PRINT TO DEBUG #\\n \\n raise NotImplementedError # YOU MUST IMPLEMENT THIS FUNCTION #\\n ```\\n\\nIn a GAU, you can call other GAUs, as such, you can create a complicated GAB\\nblock by nesting multiple GAUs. However, each GAU should be not too complex, if\\nyou want to create complex block, you should break it down into smaller GAUs and\\nnest them. As such, you should design a GAB block in a top-down manner. \\n\\nInstead of starting from scratch, you will start from an existing design and\\nimprove it. You will be provided with the full information of the design,\\nincluding the proposal, the tree structure, the implementations of the GAUs. You\\nare only allowed to modify *one GAU* from the existing design. \\n\\nYou need to select one GAU to modify, you can define new children GAUs, however\\nyou need to guarantee that your modification wont affect the correctness of the\\noverall design. \\n\\nYou will start by writing down an overal proposal for the design you want to\\nhave, the proposal decides a direction, phylosophy and the plan of the design,\\nand the analysis of the problem and how you gonna solve it by modifying one GAU\\nfrom the existing design. You will be provided with one or multiple references\\nto consider that may inspire you if there are references provided.\\n\\nYour response should include: \\n\\n1. The proposal, it should include but not restrict to the following parts: a. A\\n title with the name of the design in the level 1 header format. You shuld\\n have only one level 1 header in your response which is the name of the\\n design.\\n\\n b. Your motivation of the design. What problem you want to solve based on the\\n insights or observations you have about the autoregressive models today,\\n and any inspirations you may have from the references. \\n\\n c. The analysis of the problem.\\n\\n d. The core idea and phylosophy behind of your design that may solve the\\n problem you proposed. \\n\\n e. The plan of the design. You should include subsections of that describe\\n the details of each part of the design with the justifications. The\\n selection of the GAU to modify and the reasoning of the selection. \\n\\n f. A conclution of the proposal. \\n\\n g. Optional, the references you used in your proposal, should be in the right\\n format.\\n2. The name of the variant of the model you are going to design.\\n3. The selection of the GAU to modify.\\n\\nThe proposal will be reviewed and you will be asked to modify it if it is not\\npassed. You can start to implement the design after the proposal is passed. \\n\\nThe proposal should be as detailed as possible, DO NOT WORRY IF THE PROPOSAL IS\\nTOO LONG, BUT ALSO DO NOT FILL IN BY REDUNDANT WORDS, USE PRECISE AND CONCRETE\\nLANGUAGE, the proposal will be the guideline for the entire design process so it\\nshould be clear and detailed. \\n'}]\n",
    "print(x[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 1: import torch\n",
      "line 2: import torch.nn as nn\n",
      "line 3: from model_discovery.model.utils.modules import GAUBase, gau_test\n",
      "line 4: import torch.nn.functional as F\n",
      "line 5: \n",
      "line 6: \n",
      "line 7: class XAU(GAUBase):\n",
      "line 8:     \"\"\"Generalized Autoregressive Block Unit\n",
      "line 9:         Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
      "line 10:         Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "line 11:         Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
      "line 12: \n",
      "line 13:         embed_dim:    The dimension of the input embeddings\n",
      "line 14:         block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
      "line 15:         kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
      "line 16:     \"\"\"\n",
      "line 17: \n",
      "line 18:     def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
      "line 19:         device=None, dtype=None, **kwargs):\n",
      "line 20:         self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "line 21:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 22:         self.projection = nn.Linear(embed_dim, embed_dim // 2, **self.\n",
      "line 23:             factory_kwargs)\n",
      "line 24:         self.projection.weight.requires_grad = True\n",
      "line 25:         self.projection.bias.requires_grad = True\n",
      "line 26:         self.query_vector = nn.Parameter(torch.randn(embed_dim // 2,\n",
      "line 27:             requires_grad=True, **self.factory_kwargs))\n",
      "line 28: \n",
      "line 29:     def _forward(self, X, **Z):\n",
      "line 30:         projected_X = self.projection(X)\n",
      "line 31:         similarity_scores = torch.matmul(projected_X, self.query_vector)\n",
      "line 32:         relevance_scores = F.softmax(similarity_scores, dim=-1)\n",
      "line 33:         Y = X * relevance_scores.unsqueeze(-1)\n",
      "line 34:         return Y, {}\n",
      "line 35: \n",
      "line 36: \n",
      "line 37: class NormalizationGAU(GAUBase): \n",
      "line 38:     def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
      "line 39:         self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "line 40:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 41:         \n",
      "line 42:     def _forward(self, X, **Z): \n",
      "line 43:         return X\n",
      "line 44: \n",
      "line 45: \n",
      "line 46: class ProjectionGAU(GAUBase): \n",
      "line 47:     def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
      "line 48:         self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "line 49:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 50:         \n",
      "line 51:     def _forward(self, X, **Z): \n",
      "line 52:         return X\n",
      "line 53: \n",
      "line 54: \n",
      "line 55: class SimilarityComputationGAU(GAUBase): \n",
      "line 56:     def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
      "line 57:         self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "line 58:         super().__init__(embed_dim, block_loc, kwarg_all)\n",
      "line 59:         \n",
      "line 60:     def _forward(self, X, **Z): \n",
      "line 61:         return X\n",
      "line 62: \n",
      "line 63: \n",
      "line 64: @gau_test\n",
      "line 65: def test_XAU_test_relevance_scoring_gau(device=None, dtype=None) ->None:\n",
      "line 66:     embed_dim = 512\n",
      "line 67:     block_loc = 0, 6\n",
      "line 68:     relevance_scoring_gau = RelevanceScoringGAU(embed_dim=embed_dim,\n",
      "line 69:         block_loc=block_loc, kwarg_all={}, device=device, dtype=dtype)\n",
      "line 70:     batch_size = 2\n",
      "line 71:     seq_len = 10\n",
      "line 72:     X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n",
      "line 73:     Z = {}\n",
      "line 74:     Y, Z_ = relevance_scoring_gau(X, **Z)\n",
      "line 75:     assert Y.shape == (batch_size, seq_len, embed_dim), 'Output shape mismatch'\n",
      "line 76:     assert isinstance(Z_, dict), 'Z_ should be a dictionary'\n",
      "line 77:     print('Output embeddings:', Y)\n",
      "line 78:     print('Updated Z:', Z_)\n",
      "line 79:     assert torch.allclose(Y.sum(dim=-1), X.sum(dim=-1), atol=1e-06\n",
      "line 80:         ), 'Weighted embeddings should preserve the sum along the embedding dimension'\n",
      "line 81:     print('RelevanceScoringGAU test passed!')\n",
      "line 82: \n",
      "line 83: \n",
      "line 84: def run_XAU_tests():\n",
      "line 85: \ttry:\n",
      "line 86: \t\ttest_XAU_test_relevance_scoring_gau()\n",
      "line 87: \texcept Exception as e:\n",
      "line 88: \t\tprint(\"Error in running test_relevance_scoring_gau:\")\n",
      "line 89: \t\tprint(traceback.format_exc())\n",
      "line 90: \n",
      "line 91: \n",
      "line 92: if __name__ == \"__main__\":\n",
      "line 93: \trun_XAU_tests()\n"
     ]
    }
   ],
   "source": [
    "code=test_tree.compose_unit('XAU')\n",
    "code=U.add_line_num(code)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class DilatedReformerRootGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.dilated_attention = DilatedAttentionGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.reversible_residual = ReversibleResidualGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **\n",
    "            self.factory_kwargs, **kwarg_all)\n",
    "        self.lsh_attention = LSHAttentionGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.dilated_attention(X, **Z)\n",
    "        X, Z = self.reversible_residual(X, **Z)\n",
    "        X, Z = self.lsh_attention(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class LSHAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class DilatedAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class ReversibleResidualGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "@gau_test\n",
    "def test_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau(device=\n",
    "    None, dtype=None) ->None:\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    embed_dim = 16\n",
    "    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "    gau = DilatedReformerRootGAU(embed_dim=embed_dim, block_loc=(0, 6), kwarg_all={}, device=\n",
    "        device, dtype=dtype)\n",
    "    Y, Z_ = gau(X, **Z)\n",
    "    assert Y.shape == X.shape, 'Output shape should match input shape'\n",
    "    assert isinstance(Z_, dict), 'Z_ should be a dictionary'\n",
    "    print('Input X:', X)\n",
    "    print('Output Y:', Y)\n",
    "    print('Intermediate variables Z_:', Z_)\n",
    "\n",
    "\n",
    "def run_DilatedReformerRootGAU_tests():\n",
    "\ttry:\n",
    "\t\ttest_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"Error in running unit_test_dilated_reformer_root_gau:\", e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\trun_DilatedReformerRootGAU_tests()\n",
    "'''\n",
    "\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "output=io.StringIO()\n",
    "with redirect_stdout(output):\n",
    "    exec(code)\n",
    "captured = str(output.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ResonanceLOCOSTGAMRoot(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class ResonanceLOCOSTGAMRoot(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.resonance_rope = ResonanceRoPEUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.state_space = StateSpaceUnit(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all)\n",
    "        self.inter_block_comm = InterBlockCommUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y, Z_ = self.resonance_rope(X, **Z)\n",
    "        Y, Z_ = self.state_space(Y, **Z_)\n",
    "        Y, Z_ = self.inter_block_comm(Y, **Z_)\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class ResonanceRoPEUnit(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.freqs = self._init_frequencies(embed_dim)\n",
    "\n",
    "    def _init_frequencies(self, embed_dim):\n",
    "        half_dim = embed_dim // 2\n",
    "        freqs = torch.exp(-torch.arange(half_dim, dtype=torch.float32,\n",
    "            device=self.factory_kwargs['device']) * (math.log(10000.0) / (\n",
    "            half_dim - 1)))\n",
    "        return freqs\n",
    "\n",
    "    def _apply_rope(self, X):\n",
    "        batch_size, seqlen, _ = X.size()\n",
    "        position_ids = torch.arange(seqlen, dtype=X.dtype, device=X.device)\n",
    "        sinusoid_inp = torch.einsum('i,j->ij', position_ids, self.freqs)\n",
    "        sin, cos = sinusoid_inp.sin(), sinusoid_inp.cos()\n",
    "        X1, X2 = X[..., ::2], X[..., 1::2]\n",
    "        Y1 = X1 * cos - X2 * sin\n",
    "        Y2 = X1 * sin + X2 * cos\n",
    "        return torch.stack([Y1, Y2], dim=-1).flatten(-2)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = self._apply_rope(X)\n",
    "        Z_ = Z\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "class StateSpaceUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class InterBlockCommUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "    \n",
    "\n",
    "\n",
    "gab_config = {}\n",
    "\n",
    "\n",
    "# Check if to device still work!\n",
    "\n",
    "gab = GAB(128, (0, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Returns the device of the first parameter found in the given nn.Module.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    If the module has no parameters, returns None.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m----> 8\u001b[0m \u001b[43mget_module_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 6\u001b[0m, in \u001b[0;36mget_module_device\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_module_device\u001b[39m(model):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Returns the device of the first parameter found in the given nn.Module.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    If the module has no parameters, returns None.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_module_device(model):\n",
    "    \"\"\"\n",
    "    Returns the device of the first parameter found in the given nn.Module.\n",
    "    If the module has no parameters, returns None.\n",
    "    \"\"\"\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "get_module_device(gab.root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(gab.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children={\n",
    "    'unit1': 'MockUnit1',\n",
    "    'unit2': 'MockUnit2',\n",
    "}\n",
    "called=['unit1', 'unit2', 'unit1']\n",
    "test_tree.units['DualPathGAU'] = GAUNode('DualPathGAU', None, None, None, called, None, None, children, None)\n",
    "children={\n",
    "    'unit1': 'MockUnit3',\n",
    "    'unit2': 'MockUnit4',\n",
    "}\n",
    "called=['unit2', 'unit2']\n",
    "test_tree.units['MockUnit1'] = GAUNode('MockUnit1', None, None, None, called, None, None, children, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DualPathGAU': GAUNode(name='DualPathGAU', code=None, args=None, desc=None, path=['unit1', 'unit2', 'unit1'], review=None, rating=None, children={'unit1': 'MockUnit1', 'unit2': 'MockUnit2'}, suggestions=None),\n",
       " 'MockUnit1': GAUNode(name='MockUnit1', code=None, args=None, desc=None, path=['unit2', 'unit2'], review=None, rating=None, children={'unit1': 'MockUnit3', 'unit2': 'MockUnit4'}, suggestions=None)}"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tree.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in A: 2 3\n",
      "Kwargs in A: {'a': 1, 'b': 2, 'c': 3}\n",
      "Kwargs in B: {'a': 1, 'b': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.B at 0x2dfbf677b30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class A:\n",
    "    def __init__(self,r,d,e,**Z):\n",
    "        print('Args in A:',d,e)\n",
    "        print('Kwargs in A:',Z)\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self,_Z,c=1,d=2,e=3,**Z):\n",
    "        args2={'d':d,'e':e}\n",
    "        super().__init__(c,**_Z,**args2)\n",
    "        print('Kwargs in B:',Z)\n",
    "\n",
    "cfg={\n",
    "    'a':1,\n",
    "    'b':2,\n",
    "    'c':3,\n",
    "}\n",
    "\n",
    "B(cfg,**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'a': 3}\n",
      "{'a': 3}\n"
     ]
    }
   ],
   "source": [
    "def A(X,Z,**kwargs):\n",
    "    print(X)\n",
    "    print(Z)\n",
    "    print(kwargs)\n",
    "\n",
    "def B(X,Z,a,**kwargs):\n",
    "    A(X,Z,**Z)\n",
    "\n",
    "Z={'a':3}\n",
    "B(1,Z,**Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "    '''\n",
    "    GAU Tree Map of Butterfly-AFT Generalized Autoregressive Block (BAGAB):\n",
    "    ButterflyAFTGAU (Rating: 3.5/5)\n",
    "        |- AFTMechanismGAU (Rating: 3.5/5)\n",
    "            |- PositionBiasGAU (Rating: 3.5/5)\n",
    "            |- ElementWiseOperationGAU (Rating: 3.5/5)\n",
    "        |- ButterflyTransformGAU (Rating: 3.5/5)\n",
    "            |- ButterflyLayerGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "            |- ButterflyInitializationGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "                |- ButterflyParameterGAU (Unimplemented)\n",
    "\n",
    "    Implemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ElementWiseOperationGAU, ButterflyAFTGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    Unimplemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ButterflyParameterGAU, ElementWiseOperationGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    '''\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ButterflyAFTGAU(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase\n",
    "\n",
    "\n",
    "class ButterflyAFTGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.aft_mechanism = AFTMechanismGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.butterfly_transform = ButterflyTransformGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        aft_output, Z = self.aft_mechanism(X, **Z)\n",
    "        butterfly_output, Z = self.butterfly_transform(aft_output, **Z)\n",
    "        return butterfly_output, Z\n",
    "\n",
    "\n",
    "class ButterflyTransformGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_layer = ButterflyLayerGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_init = ButterflyInitializationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.butterfly_init(X, **Z)\n",
    "        Y, Z = self.butterfly_layer(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ButterflyLayerGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.stages = nn.ModuleList([ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all) for _ in range(kwarg_all.get('num_stages', 4))])\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for stage in self.stages:\n",
    "            X, Z = stage(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "class ButterflyMatrixGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ButterflyInitializationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_size = kwarg_all.get('butterfly_size', embed_dim)\n",
    "        self.butterfly_matrices = nn.ParameterList([nn.Parameter(torch.\n",
    "            randn(self.butterfly_size, self.butterfly_size, **self.\n",
    "            factory_kwargs)) for _ in range(int(torch.log2(torch.tensor(\n",
    "            self.butterfly_size))))])\n",
    "        self.butterfly_stage = ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_parameter = ButterflyParameterGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for matrix in self.butterfly_matrices:\n",
    "            X = F.linear(X, matrix)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class ButterflyParameterGAU(GAUBase):\n",
    "    \"\"\"\n",
    "    Generalized Autoregressive Block Unit for learning parameters of Butterfly Factorization.\n",
    "    Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "    Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "    Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "    embed_dim:    The dimension of the input embeddings\n",
    "    block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "    kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.param_size = kwarg_all.get('param_size', 128)\n",
    "        self.butterfly_weights = nn.Parameter(torch.randn(self.param_size,\n",
    "            embed_dim, **self.factory_kwargs))\n",
    "        nn.init.xavier_uniform_(self.butterfly_weights)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X = torch.matmul(X, self.butterfly_weights)\n",
    "        Z_ = {'butterfly_weights': self.butterfly_weights}\n",
    "        return transformed_X, Z_\n",
    "        # transformed_X = F.linear(X, self.butterfly_weights.T)\n",
    "        # return transformed_X, {}\n",
    "\n",
    "\n",
    "class ButterflyComputationGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class ButterflyStageGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_params = ButterflyParameterGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X, Z = self.butterfly_params(X, **Z)\n",
    "        assert transformed_X.shape == X.shape, f'Output shape {transformed_X.shape} does not match input shape {X.shape}'\n",
    "        return transformed_X, Z\n",
    "\n",
    "class AFTMechanismGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = PositionBiasGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.element_wise_operation = ElementWiseOperationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.position_bias(X, **Z)\n",
    "        Y, Z = self.element_wise_operation(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ElementWiseOperationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, embed_dim, **self.\n",
    "            factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        queries = Z.get('queries', X)\n",
    "        keys = Z.get('keys', X)\n",
    "        values = Z.get('values', X)\n",
    "        elementwise_product = queries * keys\n",
    "        elementwise_sum = elementwise_product + values + self.position_bias\n",
    "        return elementwise_sum, {}\n",
    "\n",
    "\n",
    "class PositionBiasGAU(GAUBase):\n",
    "    \"\"\"Position Bias Generalized Autoregressive Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, 1, embed_dim, **\n",
    "            self.factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = X + self.position_bias\n",
    "        return Y, {}\n",
    "    \n",
    "\n",
    "class GABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100, **kwargs):\n",
    "        super().__init__()\n",
    "        self.gab = GAB(embed_dim, (0,1), **kwargs)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, Z = self.gab(X, **Z)\n",
    "        return self.lm_head(X),X\n",
    "\n",
    "\n",
    "\n",
    "gab_config = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 54495796.0\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 899675.4375\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 475023.78125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 810216.875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 777344.75\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 783032.0625\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 771978.3125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 62701.796875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 66148.203125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 67731.2734375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 69984.171875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 74028.8046875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 70948.921875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 78776.75\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 608838.3125\n",
      "Loss: 18159550.0\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 680764.9375\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 377442.28125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 609263.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 591532.1875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 592608.125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 558789.9375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 48893.484375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 53887.46875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 54524.15625\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 57871.140625\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 63347.60546875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 56995.46875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 57605.421875\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 362053.15625\n",
      "Loss: 2057852.75\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 380637.28125\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 183177.609375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 376077.53125\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 359930.09375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 367729.90625\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 371797.71875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 24987.49609375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 30496.25\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 29164.060546875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 32006.0234375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 32426.490234375\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 33316.421875\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 33375.78125\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 196014.078125\n",
      "Loss: 0.0\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 0.0\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 0.0\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 0.0\n",
      "Loss: 0.0\n",
      "Gradient for gab.root.aft_mechanism.position_bias.position_bias: 0.0\n",
      "Gradient for gab.root.aft_mechanism.element_wise_operation.position_bias: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.0.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.1.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.2.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_layer.stages.3.butterfly_params.butterfly_weights: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.0: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.1: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.2: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.3: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.4: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.5: 0.0\n",
      "Gradient for gab.root.butterfly_transform.butterfly_init.butterfly_matrices.6: 0.0\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights\n",
      "No gradient for gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights\n",
      "Gradient for lm_head.weight: 0.0\n"
     ]
    }
   ],
   "source": [
    "class SimpleGABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100):\n",
    "        super().__init__()\n",
    "        self.butterfly_param = ButterflyParameterGAU(embed_dim, (0,1), {})\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, _ = self.butterfly_param(X, **Z)\n",
    "        return self.lm_head(X), X\n",
    "\n",
    "# Test the simplified model\n",
    "simple_model = GABLM(128)\n",
    "mock_input = torch.randn(1, 10, 128)\n",
    "mock_y = torch.randint(0, 100, (1, 10))\n",
    "\n",
    "optimizer = optim.Adam(simple_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for _ in range(5):  # Run for a few iterations\n",
    "    optimizer.zero_grad()\n",
    "    y, X = simple_model(mock_input)\n",
    "    loss = criterion(y.view(-1, 100), mock_y.view(-1))\n",
    "    loss.backward()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    for name, param in simple_model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"Gradient for {name}: {param.grad.abs().mean()}\")\n",
    "        else:\n",
    "            print(f\"No gradient for {name}\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Used parameter gab.root.butterfly_transform.butterfly_init.butterfly_stage.butterfly_params.butterfly_weights requires gradients but has none.\n",
      "Error: Used parameter gab.root.butterfly_transform.butterfly_init.butterfly_parameter.butterfly_weights requires gradients but has none.\n",
      "Differentiability test failed due to missing gradients.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim \n",
    "\n",
    "model=GABLM(128, **gab_config)\n",
    "vocab_size=100\n",
    "DEFAULT_CONTEXT_LENGTH=10\n",
    "\n",
    "mock_input=torch.randn(1, DEFAULT_CONTEXT_LENGTH, 128)\n",
    "mock_y=torch.randint(0, 100, (1, 10))\n",
    "mock_Z={}\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "y,X=model(mock_input, **mock_Z)\n",
    "loss=nn.CrossEntropyLoss()(y.view(-1, 100), mock_y.view(-1))\n",
    "loss.backward()\n",
    "\n",
    "used_params = set()\n",
    "\n",
    "# Hook to register which parameters are used during the forward pass\n",
    "def hook_fn(module, input, output):\n",
    "    for param in module.parameters():\n",
    "        used_params.add(param)\n",
    "\n",
    "# Register hooks for all submodules\n",
    "hooks = []\n",
    "for submodule in model.modules():\n",
    "    hook = submodule.register_forward_hook(hook_fn)\n",
    "    hooks.append(hook)\n",
    "\n",
    "# Forward pass again to collect used parameters\n",
    "model(mock_input)\n",
    "\n",
    "# Remove all hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "all_gradients_present = True\n",
    "\n",
    "# Check gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if param in used_params and param.grad is None:\n",
    "            # Used parameter without gradient -> Error\n",
    "            print(f\"Error: Used parameter {name} requires gradients but has none.\")\n",
    "            all_gradients_present = False\n",
    "        elif param not in used_params:\n",
    "            # Unused parameter -> Warning\n",
    "            print(f\"Warning: Parameter {name} was not used in the forward pass.\")\n",
    "\n",
    "if all_gradients_present:\n",
    "    print('Differentiability test passed')\n",
    "else:\n",
    "    print('Differentiability test failed due to missing gradients.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from core library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAU Tree Map of GPT2:\n",
      "  GPT2\n",
      "       |- MHA\n",
      "           |- RotaryPositionalEmbeddings\n",
      "       |- GatedMLP\n",
      "       |- RMSNorm\n",
      "\n",
      "Implemented Units: MHA, GatedMLP, GPT2, RotaryPositionalEmbeddings, RMSNorm\n",
      "All units are implemented.\n",
      "\n",
      "Specifications for Implemented Units:\n",
      "\n",
      "Unit Name: GatedMLP\n",
      "'''\n",
      "\n",
      "Gated MLP\n",
      "  \n",
      "'''\n",
      "- Inputs: X\n",
      "- Outputs: Y\n",
      "\n",
      "\n",
      "Unit Name: GPT2\n",
      "'''\n",
      "\n",
      "GPT2\n",
      "  \n",
      "'''\n",
      "- Inputs: X\n",
      "- Outputs: Y\n",
      "\n",
      "\n",
      "Unit Name: MHA\n",
      "'''\n",
      "\n",
      "MHA\n",
      "  \n",
      "'''\n",
      "- Inputs: X\n",
      "- Outputs: Y\n",
      "\n",
      "\n",
      "Unit Name: RMSNorm\n",
      "'''\n",
      "\n",
      "RMSNorm\n",
      "  \n",
      "'''\n",
      "- Inputs: X\n",
      "- Outputs: Y\n",
      "\n",
      "\n",
      "Unit Name: RotaryPositionalEmbeddings\n",
      "'''\n",
      "\n",
      "This class implements Rotary Positional Embeddings (RoPE)\n",
      "proposed in https://arxiv.org/abs/2104.09864.\n",
      "\n",
      "Reference implementation (used for correctness verfication)\n",
      "can be found here:\n",
      "https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n",
      "\n",
      "In this implementation we cache the embeddings for each position upto\n",
      "``max_seq_len`` by computing this during init.\n",
      "\n",
      "Args:\n",
      "    dim (int): Embedding dimension. This is usually set to the dim of each\n",
      "        head in the attention module computed as ````embed_dim`` // ``num_heads````\n",
      "    max_seq_len (int): Maximum expected sequence length for the\n",
      "        model, if exceeded the cached freqs will be recomputed\n",
      "    base (int): The base for the geometric progression used to compute\n",
      "        the rotation angles\n",
      "  \n",
      "'''\n",
      "- Inputs: input_emb, *input_pos\n",
      "- Outputs: output_emb\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import importlib\n",
    "import model_discovery.model.composer \n",
    "# importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUBase, GAUTree, check_tree_name, GABComposer\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "import model_discovery.utils as U\n",
    "\n",
    "# dir='/home/junyanc/model_discovery/model_discovery/model/library/core/gpt2/units'\n",
    "dir='C://ChengJunyan1//Research//model_discovery//model_discovery//model//library//core//gpt2//units'\n",
    "tree=GAUTree.load_from_base(dir)\n",
    "\n",
    "print(tree.view()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "gam_prompt_path = 'C://ChengJunyan1//Research//model_discovery//model_discovery//agents//prompts//gam_prompt.py'\n",
    "gau_template_path = 'C://ChengJunyan1//Research//model_discovery//model_discovery//agents//prompts//gau_template.py'\n",
    "GAM_TEMPLATE=open(gam_prompt_path).read()\n",
    "GAU_TEMPLATE=open(gau_template_path).read()\n",
    "\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n",
    "GAB_BASE=inspect.getsource(GABBase)\n",
    "GAB_COMPOSER=inspect.getsource(GABComposer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from transformers.modeling_outputs import CausalLMOutput\n",
      "from transformers import PreTrainedModel, PretrainedConfig\n",
      "from dataclasses import dataclass\n",
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "\n",
      "from gab import GAB, gab_config\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class GAMConfig(PretrainedConfig):\n",
      "    '''Configurations for Generalized Autoregressive Models.'''\n",
      "\n",
      "    d_model: int\n",
      "    n_block: int\n",
      "    batch_tokens: int \n",
      "    vocab_size: int = None\n",
      "\n",
      "\n",
      "class GAM(nn.Module):\n",
      "    ''' Generalized Autoregressive Models\n",
      "        Input:        X: (batch, seqlen, embed_dim)\n",
      "        Output:       Y: (batch, seqlen, embed_dim)\n",
      "    '''\n",
      "    def __init__(\n",
      "        self,\n",
      "        d_model: int,\n",
      "        n_block: int,\n",
      "        vocab_size: int = 50277,\n",
      "        norm_epsilon: float = 1e-5,\n",
      "        device = None,\n",
      "        dtype = None,\n",
      "    ) -> None:\n",
      "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      "        super().__init__()\n",
      "        self.d_model = d_model\n",
      "        self.embedding = nn.Embedding(vocab_size, d_model, **self.factory_kwargs)\n",
      "\n",
      "        block_config = gab_config()\n",
      "        self.blocks = nn.ModuleList(\n",
      "            [\n",
      "                GAB(\n",
      "                    embed_dim=d_model, \n",
      "                    block_loc=(layer_idx,n_block),\n",
      "                    device=device, \n",
      "                    dtype=dtype, \n",
      "                    **block_config\n",
      "                )\n",
      "                for layer_idx in range(n_block)\n",
      "            ]\n",
      "        )\n",
      "        self.norm_out = nn.LayerNorm(\n",
      "            d_model, eps=norm_epsilon, **self.factory_kwargs\n",
      "        )\n",
      "\n",
      "    def forward(self, input_ids):\n",
      "        hidden_states = self.embedding(input_ids)\n",
      "        intermediate_vars = {}\n",
      "        for block in self.blocks:\n",
      "            hidden_states, intermediate_vars = block(\n",
      "                hidden_states,\n",
      "                **intermediate_vars\n",
      "            )\n",
      "        hidden_states = self.norm_out(hidden_states)\n",
      "        return hidden_states\n",
      "\n",
      "\n",
      "class ModisLMHeadModel(PreTrainedModel):\n",
      "    ''' Generalized Autoregressive Models with LM Head '''\n",
      "    config_class = GAMConfig\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: GAMConfig,\n",
      "        device=None,\n",
      "        dtype=None,\n",
      "    ) -> None:\n",
      "        super().__init__(config)\n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      "        self.backbone = GAM(\n",
      "            d_model=config.d_model,\n",
      "            n_block=config.n_block,\n",
      "            vocab_size=config.vocab_size,\n",
      "            **factory_kwargs,\n",
      "        )\n",
      "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False, **factory_kwargs)\n",
      "\n",
      "    def forward(self, input_ids, **gab_kwargs):\n",
      "        hidden_states = self.backbone(input_ids, **gab_kwargs)\n",
      "        lm_logits = self.lm_head(hidden_states)\n",
      "        return CausalLMOutput(logits=lm_logits)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(GAM_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
