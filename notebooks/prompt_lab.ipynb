{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place for experimenting the progressive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\prompts\\prompts.py:1471: SyntaxWarning: invalid escape sequence '\\('\n",
      "  GUE_DESIGN_PROPOSER_SYSTEM_prompt = \"\"\"\n",
      "c:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\prompts\\prompts.py:1696: SyntaxWarning: invalid escape sequence '\\('\n",
      "  GUE_PROPOSAL_REVIEWER_SYSTEM_prompt = \"\"\"\n",
      "c:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\prompts\\prompts.py:2325: SyntaxWarning: invalid escape sequence '\\('\n",
      "  GUE_DESIGNER_SYSTEM_prompt_part4 = \"\"\"\n",
      "c:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\agents\\prompts\\prompts.py:2631: SyntaxWarning: invalid escape sequence '\\('\n",
      "  GUE_IMPLEMENTATION_REVIEWER_SYSTEM_prompt = \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.utils as U\n",
    "from model_discovery.configs.gam_config import GAMConfig, GAMConfig_14M\n",
    "# from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "from model_discovery.model.composer import GAUBase, GAUTree, check_tree_name, GABComposer\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "from model_discovery.evolution import BuildEvolution\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "\n",
    "prompts_dir='../model_discovery/agents/prompts/'\n",
    "gab_py = U.read_file(U.pjoin(prompts_dir,'gab_template.py'))\n",
    "gam_prompt_path = os.path.join(prompts_dir,'gam_prompt.py')\n",
    "gau_template_path = os.path.join(prompts_dir,'gau_template.py')\n",
    "GAM_TEMPLATE=open(gam_prompt_path).read()\n",
    "GAU_TEMPLATE=open(gau_template_path).read()\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n",
    "\n",
    "GAB_BASE=inspect.getsource(GABBase)\n",
    "GAB_COMPOSER=inspect.getsource(GABComposer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os,sys\n",
    "import importlib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "import model_discovery.agents.prompts.prompts as P\n",
    "import model_discovery.model.composer\n",
    "importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUTree,GAUBase,GAUNode\n",
    "import model_discovery.utils\n",
    "importlib.reload(model_discovery.utils)\n",
    "import model_discovery.utils as U\n",
    "\n",
    "\n",
    "\n",
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "# Placeholder classes for future implementation\n",
    "class MemoryAccessUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, memory_size, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "class DownsamplingUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, downsample_factor, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "\n",
    "class XXX(GAUBase,object):\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "\n",
    "class GAU(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
    "        # argv: list of hyperparameters\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\n",
    "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        self.a1 = kwarg_all.get('a1', 1)\n",
    "        self.a2: int = kwarg_all.get('a2', 2)\n",
    "\n",
    "        # COMPLETING THE CODE HERE #\n",
    "        self.token_scorer = TokenScoringGAU(embed_dim, **factory_kwargs)\n",
    "        self.dual_path: GAUBase = DualPathGAU(embed_dim, **factory_kwargs)\n",
    "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim, **factory_kwargs)\n",
    "\n",
    "        self.stages: nn.ModuleList = nn.ModuleList([CompressionStageGAU(embed_dim, device=\n",
    "            device, dtype=dtype, **kwargs) for _ in range(2)])\n",
    "\n",
    "        self.modelx=nn.ModuleList([\n",
    "            nn.Conv2d(1,20,5),\n",
    "            nn.ReLU(),\n",
    "            LatentAttentionGAU(),\n",
    "            nn.Conv2d(20,64,5),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "            \n",
    "\n",
    "        self.model1 = (\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU(),\n",
    "            'gau3': CompressionStageGAU()\n",
    "        })\n",
    "\n",
    "        self.model2 = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('gau2', DualPathGAU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "    def init_something(self):\n",
    "        self.modelx[3]=LatentAttentionGAU()\n",
    "        self.modelx.append(LatentAttentionGAU())\n",
    "        self.modelx.insert(0,LatentAttentionGAU())\n",
    "        self.activations['gau1']=LatentAttentionGAU()\n",
    "        self.model2.add_module('gau4',LatentAttentionGAU())\n",
    "\n",
    "        X=self.dual_path(X)\n",
    "    \n",
    "    def _forward(self, X, **Z): \n",
    "\n",
    "        # THE CODE HERE MUST BE COMPLETED #\n",
    "        # Step 1: Score tokens\n",
    "        X, Z = self.token_scorer(X, **Z)\n",
    "        # Step 2: Route through dual paths\n",
    "        # Step 3: Apply latent attention\n",
    "        Y, Z = self.latent_attention(X, A=A, B=B, **Z)\n",
    "        for stage in self.stages:\n",
    "            X = stage(X, **Z)\n",
    "        \n",
    "        X, Z = self.dual_path(X, **Z)\n",
    "        X=self.activations['gau3'](X)\n",
    "\n",
    "        return Y, Z\n",
    "\n",
    "@gau_test\n",
    "def test1(a=1,device=None):\n",
    "    print('test1')\n",
    "\n",
    "    \n",
    "@gau_test\n",
    "def test2(device=None):\n",
    "    print('test2')\n",
    "\n",
    "\n",
    "CHILDREN_DECLARATIONS = [\n",
    "    UnitDecl(\n",
    "        unitname='TokenScoringGAU', \n",
    "        requirements='Token scoring GAU',\n",
    "        inputs=[],\n",
    "        outputs=[],\n",
    "    ),\n",
    "    UnitDecl(\n",
    "        unitname='DualPathGAU',\n",
    "        requirements='Dual path GAU',\n",
    "        inputs=[],\n",
    "        outputs=[],\n",
    "    ),\n",
    "    UnitDecl(\n",
    "        unitname='LatentAttentionGAU',\n",
    "        requirements='Latent attention GAU',\n",
    "        inputs=[],\n",
    "        outputs=[],\n",
    "    ),\n",
    "    UnitDecl(\n",
    "        unitname='CompressionStageGAU',\n",
    "        requirements='Compression stage GAU',\n",
    "        inputs=[],\n",
    "        outputs=[],\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "class MHA(GAUBase):\n",
    "    \"\"\"\n",
    "    MHA combines hierarchical gating mechanisms from HGRN2 with state space modeling (SSM) to improve long-range dependency capture and computational efficiency.\n",
    "\n",
    "    This class replaces the traditional multi-head attention mechanism with a more efficient approach that maintains linear complexity with respect to sequence length.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): The embedding dimension of the input sequence.\n",
    "        block_loc (tuple): The location of the block within the network.\n",
    "        kwarg_all (dict): Dictionary of additional keyword arguments for initialization.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        expand_ratio (int): The ratio for expansion layers.\n",
    "        d_state (int): The dimension of the state in the state space model.\n",
    "\n",
    "    Returns:\n",
    "        Y (Tensor): The output sequence with the same shape as the input.\n",
    "\n",
    "    Example:\n",
    "        >>> mha = MHA(embed_dim=512, block_loc=(0, 0), kwarg_all={}, num_heads=8, expand_ratio=4, d_state=64)\n",
    "        >>> X = torch.randn(2, 10, 512)\n",
    "        >>> Y, Z = mha(X)\n",
    "\n",
    "    Note:\n",
    "        This implementation maintains interpretability through the `get_attention_map` method, which provides a way to visualize effective attention patterns.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, num_heads: int, expand_ratio: int, d_state: int, device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.d_state = d_state\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # HGRN2-inspired projections\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "        # SSM components\n",
    "        self.ssm_A = nn.Parameter(torch.randn(num_heads, d_state, d_state))\n",
    "        self.ssm_B = nn.Parameter(torch.randn(num_heads, d_state, 1))\n",
    "        self.ssm_C = nn.Parameter(torch.randn(num_heads, 1, d_state))\n",
    "\n",
    "        # Expansion layers\n",
    "        self.expand = nn.Linear(embed_dim, embed_dim * expand_ratio, bias=True)\n",
    "        self.contract = nn.Linear(embed_dim * expand_ratio, embed_dim, bias=True)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "        # Layernorm for stability\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize SSM components\n",
    "        nn.init.orthogonal_(self.ssm_A)\n",
    "        nn.init.normal_(self.ssm_B, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.ssm_C, mean=0.0, std=0.02)\n",
    "\n",
    "        # Initialize other weights\n",
    "        nn.init.xavier_uniform_(self.q_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.k_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.v_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.expand.weight)\n",
    "        nn.init.xavier_uniform_(self.contract.weight)\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        B, L, D = X.shape\n",
    "        H = self.num_heads\n",
    "\n",
    "        # HGRN2-inspired projections\n",
    "        Q = self.q_proj(X).view(B, L, H, -1)\n",
    "        K = self.k_proj(X).view(B, L, H, -1)\n",
    "        V = self.v_proj(X).view(B, L, H, -1)\n",
    "\n",
    "        # Expansion\n",
    "        X_exp = self.expand(X).view(B, L, H, -1)\n",
    "\n",
    "        # SSM state update\n",
    "        state = torch.zeros(B, H, self.d_state, 1, device=X.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(L):\n",
    "            # Update state\n",
    "            state = torch.matmul(self.ssm_A, state) + self.ssm_B * K[:, t, :, :, None]\n",
    "\n",
    "            # Compute output\n",
    "            output = torch.matmul(self.ssm_C, state).squeeze(-1)\n",
    "            output = output * Q[:, t, :, :]\n",
    "            output = torch.sum(output * V[:, t, :, :], dim=-1)\n",
    "\n",
    "            # Apply expansion\n",
    "            output = output * X_exp[:, t, :, :]\n",
    "\n",
    "            outputs.append(output)\n",
    "\n",
    "        output = torch.stack(outputs, dim=1)\n",
    "        output = output.view(B, L, -1)\n",
    "\n",
    "        # Contraction\n",
    "        output = self.contract(output)\n",
    "\n",
    "        # Output projection and residual connection\n",
    "        output = self.out_proj(output)\n",
    "        output = self.norm(X + output)\n",
    "\n",
    "        return output, Z\n",
    "\n",
    "    def get_attention_map(self, X):\n",
    "        B, L, D = X.shape\n",
    "        H = self.num_heads\n",
    "\n",
    "        Q = self.q_proj(X).view(B, L, H, -1)\n",
    "        K = self.k_proj(X).view(B, L, H, -1)\n",
    "\n",
    "        attention_map = torch.zeros(B, H, L, L, device=X.device)\n",
    "        state = torch.zeros(B, H, self.d_state, 1, device=X.device)\n",
    "\n",
    "        for t in range(L):\n",
    "            state = torch.matmul(self.ssm_A, state) + self.ssm_B * K[:, t, :, :, None]\n",
    "            output = torch.matmul(self.ssm_C, state).squeeze(-1)\n",
    "            attention_map[:, :, t, :t+1] = torch.sum(output * Q[:, :t+1, :, :], dim=-1).transpose(1, 2)\n",
    "\n",
    "        return attention_map\n",
    "\n",
    "\n",
    "# WRITE YOUR UNIT TEST FUNCTIONS HERE #\n",
    "\n",
    "@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON'T BE RECOGNIZED AS A UNIT TEST #\n",
    "def test_mha(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\n",
    "    # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\n",
    "    embed_dim = 128\n",
    "    block_loc = (0, 0)\n",
    "    kwarg_all = {}\n",
    "    num_heads = 8\n",
    "    expand_ratio = 4\n",
    "    d_state = 64\n",
    "    mha = MHA(embed_dim, block_loc, kwarg_all, num_heads, expand_ratio, d_state, device=device, dtype=dtype)\n",
    "    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "    y, Z_ = mha(x, **Z)\n",
    "    assert y.shape == (1, 100, 128)\n",
    "\n",
    "    # WRITE ASSERTIONS TO PERFORM THE TEST, USE PRINT TO DEBUG #\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "    # raise NotImplementedError # YOU MUST IMPLEMENT THIS FUNCTION #\n",
    "'''\n",
    "children=['ProjectionGAU','SimilarityComputationGAU','NormalizationGAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_name = \"MHA\"  # Provide the unit_name to rename GAU class\n",
    "reformatted_code, new_args, gau_tests, errors, warnings, fetal_errors,docstring,children_decl = check_and_reformat_gau_code(code, unit_name)\n",
    "print(\"Reformatted Code:\\n\" + reformatted_code)\n",
    "print(\"Errors:\\n\", '\\n\\t'.join(errors+fetal_errors))\n",
    "print(\"Warnings:\\n\", '\\n\\t'.join(warnings))\n",
    "print(\"New Arguments:\\n\", new_args)\n",
    "print(\"Docstring:\\n\", docstring)\n",
    "print(\"Children Declarations:\\n\", children_decl)\n",
    "\n",
    "\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "spec=P.UnitSpec(\n",
    "   unitname = unit_name,\n",
    "   document = 'Test',\n",
    "   inputs = [],\n",
    "   outputs = [],\n",
    ")\n",
    "# test_tree.add_unit(\n",
    "#     spec, reformatted_code, new_args, None, None, None, children, gau_tests, None\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[{'role': 'system', 'content': '\\nYou are a professional AI researcher focusing on discovering the best\\nautoregressive language model block. Your goal is to design a novel block\\nfollowing the Generalized Autoregressive Block (GAB) structure defined in the\\nfollowing base class:\\n\\n```python class GABBase(nn.Module):\\n \"\"\" Base class for Generalized Autoregressive Block \"\"\"\\n def __init__(self,embed_dim: int, block_loc: tuple):\\n super().__init__()\\n self.embed_dim = embed_dim\\n self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\\n\\n def _forward(self, X, **Z): \\n raise NotImplementedError\\n \\n # YOU ARE NOT ALLOW TO OVERRIDE THIS METHOD #\\n def forward(self, X, **Z): # kwargs not parsable by torchscript but more flexible\\n \"\"\"Forward pass of the model\"\"\"\\n assert len(X.shape) == 3, f\"Input shape must be (batch, seqlen, embed_dim), got {len(X.shape)}D instead\"\\n assert X.shape[-1] == self.embed_dim, f\"Input shape must be (batch, seqlen, embed_dim), got {X.shape} instead\"\\n Y = self._forward(X, **Z)\\n if isinstance(Y, tuple):\\n Y, Z_ = Y\\n else:\\n Z_ = {}\\n assert Y.shape == X.shape, f\"GAB Output shape must be the same as input shape of {X.shape}, got {Y.shape} instead\"\\n assert isinstance(Z, dict), \"Intermediate variables must be stored in a dict\"\\n Z.update(Z_) # the new intermediate variables are updated to the current Z\\n return Y, Z\\n ```\\n\\nThe GAB will be used to construct a Generalized Autoregressive Model (GAM)\\ndefined as follows:\\n\\n```python from transformers.modeling_outputs import CausalLMOutput\\nfrom transformers import PreTrainedModel, PretrainedConfig\\nfrom dataclasses import dataclass\\n\\nimport torch\\nfrom torch import nn\\n\\nfrom gab import GAB, gab_config\\n\\n\\n@dataclass\\nclass GAMConfig(PretrainedConfig):\\n \\'\\'\\'Configurations for Generalized Autoregressive Models.\\'\\'\\'\\n\\n d_model: int\\n n_block: int\\n batch_tokens: int \\n vocab_size: int = None\\n\\n\\nclass GAM(nn.Module):\\n \\'\\'\\' Generalized Autoregressive Models\\n Input: X: (batch, seqlen, embed_dim)\\n Output: Y: (batch, seqlen, embed_dim)\\n \\'\\'\\'\\n def __init__(\\n self,\\n d_model: int,\\n n_block: int,\\n vocab_size: int = 50277,\\n norm_epsilon: float = 1e-5,\\n device = None,\\n dtype = None,\\n ) -> None:\\n self.factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n super().__init__()\\n self.d_model = d_model\\n self.embedding = nn.Embedding(vocab_size, d_model, **self.factory_kwargs)\\n\\n block_config = gab_config()\\n self.blocks = nn.ModuleList(\\n [\\n GAB(\\n embed_dim=d_model, \\n block_loc=(layer_idx,n_block),\\n device=device, \\n dtype=dtype, \\n **block_config\\n )\\n for layer_idx in range(n_block)\\n ]\\n )\\n self.norm_out = nn.LayerNorm(\\n d_model, eps=norm_epsilon, **self.factory_kwargs\\n )\\n\\n def forward(self, input_ids):\\n hidden_states = self.embedding(input_ids)\\n intermediate_vars = {}\\n for block in self.blocks:\\n hidden_states, intermediate_vars = block(\\n hidden_states,\\n **intermediate_vars\\n )\\n hidden_states = self.norm_out(hidden_states)\\n return hidden_states\\n\\n\\nclass GLMHeadModel(PreTrainedModel):\\n \\'\\'\\' Generalized Autoregressive Models with LM Head \\'\\'\\'\\n config_class = GAMConfig\\n\\n def __init__(\\n self,\\n config: GAMConfig,\\n device=None,\\n dtype=None,\\n ) -> None:\\n super().__init__(config)\\n factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n self.backbone = GAM(\\n d_model=config.d_model,\\n n_block=config.n_block,\\n vocab_size=config.vocab_size,\\n **factory_kwargs,\\n )\\n self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False, **factory_kwargs)\\n\\n def forward(self, input_ids, **gab_kwargs):\\n hidden_states = self.backbone(input_ids, **gab_kwargs)\\n lm_logits = self.lm_head(hidden_states)\\n return CausalLMOutput(logits=lm_logits)\\n ```\\n\\nThe produced language model will be pretrained with the corpus and then be\\napplied for downstream tasks. The new model is expected to have a low\\nperplexity, high accuracy, robustness, efficiency, and most importantly, good\\nscalability. \\n\\nSince the autoregressive model design is complicated, so we will break it down\\ninto smaller parts. We represent a block as multiple nested units, the\\nGeneralized Autoregressive Unit (GAU). Each GAU accepts a sequence of embeddings\\nX and a dictionary of intermediate variables Z as input, and outputs a sequence\\nof embeddings Y and a dictionary of new or updated intermediate variables Z_. Z_\\nis optional, when it is provided, it will be used to update Z for the next unit\\nby Z.update(Z_). A GAU is defined in the following base class:\\n\\n```python class GAUBase(nn.Module): \\n \"\"\" \\n Instead of directly giving the full implementation of a GAB block, the agent need to \\n design a series of nested GAB units and construct the full GAB block as a pipeline of these units.\\n\\n GAB is fractal, like GAB itself, each GAB unit accepts X and Z as input and returns Y and Z as output.\\n \"\"\" \\n def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict):\\n super().__init__()\\n self.embed_dim = embed_dim\\n self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\\n self.kwarg_all = kwarg_all # dict of all kwargs, use it to initialize the children units\\n\\n def _forward(self, X, **Z): # you can define the arguments instead of using **Z\\n raise NotImplementedError\\n \\n def forward(self, X, **Z):\\n assert len(X.shape) == 3 and X.shape[-1] == self.embed_dim, f\"Input X must be a sequence of shape (batch, seqlen, embed_dim), all other kinds of variables should be passed by Z. Got {X.shape} instead.\"\\n _params = inspect.signature(self._forward).parameters\\n X=X.to(**self.factory_kwargs)\\n _Z = {k: v for k, v in Z.items() if k in _params}\\n Y = self._forward(X, **_Z)\\n if isinstance(Y, tuple):\\n Y, Z_ = Y\\n else:\\n Z_ = {}\\n assert Y.shape == X.shape, f\"GAU output Y must be a sequence with the same shape as input of {X.shape}, got {Y.shape} instead, all other kinds of variables should be passed by Z.\"\\n assert isinstance(Z_, dict), \"Intermediate variables must be stored in a dict\"\\n Z.update(Z_) # the new intermediate variables are updated to the current Z\\n return Y, Z\\n ```\\n\\nYou will design a GAU by completing the blanks marked in this template, which\\nincludes the initialization where you can define your custom arguments with\\noptional default values, the forward function where you can define convenient\\nfunctions or classes in the GAB class such as caches, notice that you are only\\nallowed to have only one GAU which inherited from the GAUBase class in the file:\\n \\n```python # gau.py # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\\n\\n\\n# YOU CAN IMPORT MORE MODULES HERE #\\n\\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\\n\\n\\nclass UnitName(GAUBase):\\n \"\"\"Generalized Autoregressive Block Unit\\n Input: X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \\n Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\\n Constraints: Causal, differentiable, parameter number, complexity, parallelizable...\\n\\n embed_dim: The dimension of the input embeddings\\n block_loc: The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\\n kwarg_all: A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\\n \"\"\"\\n def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT KEEP THE ORIGINAL ONES #\\n self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\\n super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\\n \\n # COMPLETING THE CODE HERE #\\n\\n raise NotImplementedError\\n\\n\\n # YOU CAN ADD MORE FUNCTIONS HERE #\\n\\n\\n def _forward(self, X, **Z): \\n \\n # THE CODE HERE MUST BE COMPLETED #\\n\\n raise NotImplementedError\\n\\n\\n# WRITE YOUR UNIT TEST FUNCTIONS HERE #\\n\\n@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON\\'T BE RECOGNIZED AS A UNIT TEST #\\ndef unit_test_name(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\\n # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\\n\\n # WRITE ASSERTIONS TO PERFORM THE TEST, USE PRINT TO DEBUG #\\n \\n raise NotImplementedError # YOU MUST IMPLEMENT THIS FUNCTION #\\n ```\\n\\nIn a GAU, you can call other GAUs, as such, you can create a complicated GAB\\nblock by nesting multiple GAUs. However, each GAU should be not too complex, if\\nyou want to create complex block, you should break it down into smaller GAUs and\\nnest them. As such, you should design a GAB block in a top-down manner. \\n\\nInstead of starting from scratch, you will start from an existing design and\\nimprove it. You will be provided with the full information of the design,\\nincluding the proposal, the tree structure, the implementations of the GAUs. You\\nare only allowed to modify *one GAU* from the existing design. \\n\\nYou need to select one GAU to modify, you can define new children GAUs, however\\nyou need to guarantee that your modification wont affect the correctness of the\\noverall design. \\n\\nYou will start by writing down an overal proposal for the design you want to\\nhave, the proposal decides a direction, phylosophy and the plan of the design,\\nand the analysis of the problem and how you gonna solve it by modifying one GAU\\nfrom the existing design. You will be provided with one or multiple references\\nto consider that may inspire you if there are references provided.\\n\\nYour response should include: \\n\\n1. The proposal, it should include but not restrict to the following parts: a. A\\n title with the name of the design in the level 1 header format. You shuld\\n have only one level 1 header in your response which is the name of the\\n design.\\n\\n b. Your motivation of the design. What problem you want to solve based on the\\n insights or observations you have about the autoregressive models today,\\n and any inspirations you may have from the references. \\n\\n c. The analysis of the problem.\\n\\n d. The core idea and phylosophy behind of your design that may solve the\\n problem you proposed. \\n\\n e. The plan of the design. You should include subsections of that describe\\n the details of each part of the design with the justifications. The\\n selection of the GAU to modify and the reasoning of the selection. \\n\\n f. A conclution of the proposal. \\n\\n g. Optional, the references you used in your proposal, should be in the right\\n format.\\n2. The name of the variant of the model you are going to design.\\n3. The selection of the GAU to modify.\\n\\nThe proposal will be reviewed and you will be asked to modify it if it is not\\npassed. You can start to implement the design after the proposal is passed. \\n\\nThe proposal should be as detailed as possible, DO NOT WORRY IF THE PROPOSAL IS\\nTOO LONG, BUT ALSO DO NOT FILL IN BY REDUNDANT WORDS, USE PRECISE AND CONCRETE\\nLANGUAGE, the proposal will be the guideline for the entire design process so it\\nshould be clear and detailed. \\n'}]\n",
    "print(x[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code=test_tree.compose_unit('XAU')\n",
    "code=U.add_line_num(code)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class DilatedReformerRootGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.dilated_attention = DilatedAttentionGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.reversible_residual = ReversibleResidualGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **\n",
    "            self.factory_kwargs, **kwarg_all)\n",
    "        self.lsh_attention = LSHAttentionGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.dilated_attention(X, **Z)\n",
    "        X, Z = self.reversible_residual(X, **Z)\n",
    "        X, Z = self.lsh_attention(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class LSHAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class DilatedAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class ReversibleResidualGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "@gau_test\n",
    "def test_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau(device=\n",
    "    None, dtype=None) ->None:\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    embed_dim = 16\n",
    "    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "    gau = DilatedReformerRootGAU(embed_dim=embed_dim, block_loc=(0, 6), kwarg_all={}, device=\n",
    "        device, dtype=dtype)\n",
    "    Y, Z_ = gau(X, **Z)\n",
    "    assert Y.shape == X.shape, 'Output shape should match input shape'\n",
    "    assert isinstance(Z_, dict), 'Z_ should be a dictionary'\n",
    "    print('Input X:', X)\n",
    "    print('Output Y:', Y)\n",
    "    print('Intermediate variables Z_:', Z_)\n",
    "\n",
    "\n",
    "def run_DilatedReformerRootGAU_tests():\n",
    "\ttry:\n",
    "\t\ttest_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"Error in running unit_test_dilated_reformer_root_gau:\", e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\trun_DilatedReformerRootGAU_tests()\n",
    "'''\n",
    "\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "output=io.StringIO()\n",
    "with redirect_stdout(output):\n",
    "    exec(code)\n",
    "captured = str(output.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ResonanceLOCOSTGAMRoot(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class ResonanceLOCOSTGAMRoot(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.resonance_rope = ResonanceRoPEUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.state_space = StateSpaceUnit(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all)\n",
    "        self.inter_block_comm = InterBlockCommUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y, Z_ = self.resonance_rope(X, **Z)\n",
    "        Y, Z_ = self.state_space(Y, **Z_)\n",
    "        Y, Z_ = self.inter_block_comm(Y, **Z_)\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class ResonanceRoPEUnit(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.freqs = self._init_frequencies(embed_dim)\n",
    "\n",
    "    def _init_frequencies(self, embed_dim):\n",
    "        half_dim = embed_dim // 2\n",
    "        freqs = torch.exp(-torch.arange(half_dim, dtype=torch.float32,\n",
    "            device=self.factory_kwargs['device']) * (math.log(10000.0) / (\n",
    "            half_dim - 1)))\n",
    "        return freqs\n",
    "\n",
    "    def _apply_rope(self, X):\n",
    "        batch_size, seqlen, _ = X.size()\n",
    "        position_ids = torch.arange(seqlen, dtype=X.dtype, device=X.device)\n",
    "        sinusoid_inp = torch.einsum('i,j->ij', position_ids, self.freqs)\n",
    "        sin, cos = sinusoid_inp.sin(), sinusoid_inp.cos()\n",
    "        X1, X2 = X[..., ::2], X[..., 1::2]\n",
    "        Y1 = X1 * cos - X2 * sin\n",
    "        Y2 = X1 * sin + X2 * cos\n",
    "        return torch.stack([Y1, Y2], dim=-1).flatten(-2)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = self._apply_rope(X)\n",
    "        Z_ = Z\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "class StateSpaceUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class InterBlockCommUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "    \n",
    "\n",
    "\n",
    "gab_config = {}\n",
    "\n",
    "\n",
    "# Check if to device still work!\n",
    "\n",
    "gab = GAB(128, (0, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_device(model):\n",
    "    \"\"\"\n",
    "    Returns the device of the first parameter found in the given nn.Module.\n",
    "    If the module has no parameters, returns None.\n",
    "    \"\"\"\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "get_module_device(gab.root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gab.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children={\n",
    "    'unit1': 'MockUnit1',\n",
    "    'unit2': 'MockUnit2',\n",
    "}\n",
    "called=['unit1', 'unit2', 'unit1']\n",
    "test_tree.units['DualPathGAU'] = GAUNode('DualPathGAU', None, None, None, called, None, None, children, None)\n",
    "children={\n",
    "    'unit1': 'MockUnit3',\n",
    "    'unit2': 'MockUnit4',\n",
    "}\n",
    "called=['unit2', 'unit2']\n",
    "test_tree.units['MockUnit1'] = GAUNode('MockUnit1', None, None, None, called, None, None, children, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self,r,d,e,**Z):\n",
    "        print('Args in A:',d,e)\n",
    "        print('Kwargs in A:',Z)\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self,_Z,c=1,d=2,e=3,**Z):\n",
    "        args2={'d':d,'e':e}\n",
    "        super().__init__(c,**_Z,**args2)\n",
    "        print('Kwargs in B:',Z)\n",
    "\n",
    "cfg={\n",
    "    'a':1,\n",
    "    'b':2,\n",
    "    'c':3,\n",
    "}\n",
    "\n",
    "B(cfg,**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(X,Z,**kwargs):\n",
    "    print(X)\n",
    "    print(Z)\n",
    "    print(kwargs)\n",
    "\n",
    "def B(X,Z,a,**kwargs):\n",
    "    A(X,Z,**Z)\n",
    "\n",
    "Z={'a':3}\n",
    "B(1,Z,**Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "    '''\n",
    "    GAU Tree Map of Butterfly-AFT Generalized Autoregressive Block (BAGAB):\n",
    "    ButterflyAFTGAU (Rating: 3.5/5)\n",
    "        |- AFTMechanismGAU (Rating: 3.5/5)\n",
    "            |- PositionBiasGAU (Rating: 3.5/5)\n",
    "            |- ElementWiseOperationGAU (Rating: 3.5/5)\n",
    "        |- ButterflyTransformGAU (Rating: 3.5/5)\n",
    "            |- ButterflyLayerGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "            |- ButterflyInitializationGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "                |- ButterflyParameterGAU (Unimplemented)\n",
    "\n",
    "    Implemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ElementWiseOperationGAU, ButterflyAFTGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    Unimplemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ButterflyParameterGAU, ElementWiseOperationGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    '''\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ButterflyAFTGAU(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase\n",
    "\n",
    "\n",
    "class ButterflyAFTGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.aft_mechanism = AFTMechanismGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.butterfly_transform = ButterflyTransformGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        aft_output, Z = self.aft_mechanism(X, **Z)\n",
    "        butterfly_output, Z = self.butterfly_transform(aft_output, **Z)\n",
    "        return butterfly_output, Z\n",
    "\n",
    "\n",
    "class ButterflyTransformGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_layer = ButterflyLayerGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_init = ButterflyInitializationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.butterfly_init(X, **Z)\n",
    "        Y, Z = self.butterfly_layer(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ButterflyLayerGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.stages = nn.ModuleList([ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all) for _ in range(kwarg_all.get('num_stages', 4))])\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for stage in self.stages:\n",
    "            X, Z = stage(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "class ButterflyMatrixGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ButterflyInitializationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_size = kwarg_all.get('butterfly_size', embed_dim)\n",
    "        self.butterfly_matrices = nn.ParameterList([nn.Parameter(torch.\n",
    "            randn(self.butterfly_size, self.butterfly_size, **self.\n",
    "            factory_kwargs)) for _ in range(int(torch.log2(torch.tensor(\n",
    "            self.butterfly_size))))])\n",
    "        self.butterfly_stage = ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_parameter = ButterflyParameterGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for matrix in self.butterfly_matrices:\n",
    "            X = F.linear(X, matrix)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class ButterflyParameterGAU(GAUBase):\n",
    "    \"\"\"\n",
    "    Generalized Autoregressive Block Unit for learning parameters of Butterfly Factorization.\n",
    "    Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "    Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "    Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "    embed_dim:    The dimension of the input embeddings\n",
    "    block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "    kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.param_size = kwarg_all.get('param_size', 128)\n",
    "        self.butterfly_weights = nn.Parameter(torch.randn(self.param_size,\n",
    "            embed_dim, **self.factory_kwargs))\n",
    "        nn.init.xavier_uniform_(self.butterfly_weights)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X = torch.matmul(X, self.butterfly_weights)\n",
    "        Z_ = {'butterfly_weights': self.butterfly_weights}\n",
    "        return transformed_X, Z_\n",
    "        # transformed_X = F.linear(X, self.butterfly_weights.T)\n",
    "        # return transformed_X, {}\n",
    "\n",
    "\n",
    "class ButterflyComputationGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class ButterflyStageGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_params = ButterflyParameterGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X, Z = self.butterfly_params(X, **Z)\n",
    "        assert transformed_X.shape == X.shape, f'Output shape {transformed_X.shape} does not match input shape {X.shape}'\n",
    "        return transformed_X, Z\n",
    "\n",
    "class AFTMechanismGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = PositionBiasGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.element_wise_operation = ElementWiseOperationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.position_bias(X, **Z)\n",
    "        Y, Z = self.element_wise_operation(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ElementWiseOperationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, embed_dim, **self.\n",
    "            factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        queries = Z.get('queries', X)\n",
    "        keys = Z.get('keys', X)\n",
    "        values = Z.get('values', X)\n",
    "        elementwise_product = queries * keys\n",
    "        elementwise_sum = elementwise_product + values + self.position_bias\n",
    "        return elementwise_sum, {}\n",
    "\n",
    "\n",
    "class PositionBiasGAU(GAUBase):\n",
    "    \"\"\"Position Bias Generalized Autoregressive Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, 1, embed_dim, **\n",
    "            self.factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = X + self.position_bias\n",
    "        return Y, {}\n",
    "    \n",
    "\n",
    "class GABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100, **kwargs):\n",
    "        super().__init__()\n",
    "        self.gab = GAB(embed_dim, (0,1), **kwargs)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, Z = self.gab(X, **Z)\n",
    "        return self.lm_head(X),X\n",
    "\n",
    "\n",
    "\n",
    "gab_config = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100):\n",
    "        super().__init__()\n",
    "        self.butterfly_param = ButterflyParameterGAU(embed_dim, (0,1), {})\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, _ = self.butterfly_param(X, **Z)\n",
    "        return self.lm_head(X), X\n",
    "\n",
    "# Test the simplified model\n",
    "simple_model = GABLM(128)\n",
    "mock_input = torch.randn(1, 10, 128)\n",
    "mock_y = torch.randint(0, 100, (1, 10))\n",
    "\n",
    "optimizer = optim.Adam(simple_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for _ in range(5):  # Run for a few iterations\n",
    "    optimizer.zero_grad()\n",
    "    y, X = simple_model(mock_input)\n",
    "    loss = criterion(y.view(-1, 100), mock_y.view(-1))\n",
    "    loss.backward()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    for name, param in simple_model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"Gradient for {name}: {param.grad.abs().mean()}\")\n",
    "        else:\n",
    "            print(f\"No gradient for {name}\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim \n",
    "\n",
    "model=GABLM(128, **gab_config)\n",
    "vocab_size=100\n",
    "DEFAULT_CONTEXT_LENGTH=10\n",
    "\n",
    "mock_input=torch.randn(1, DEFAULT_CONTEXT_LENGTH, 128)\n",
    "mock_y=torch.randint(0, 100, (1, 10))\n",
    "mock_Z={}\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "y,X=model(mock_input, **mock_Z)\n",
    "loss=nn.CrossEntropyLoss()(y.view(-1, 100), mock_y.view(-1))\n",
    "loss.backward()\n",
    "\n",
    "used_params = set()\n",
    "\n",
    "# Hook to register which parameters are used during the forward pass\n",
    "def hook_fn(module, input, output):\n",
    "    for param in module.parameters():\n",
    "        used_params.add(param)\n",
    "\n",
    "# Register hooks for all submodules\n",
    "hooks = []\n",
    "for submodule in model.modules():\n",
    "    hook = submodule.register_forward_hook(hook_fn)\n",
    "    hooks.append(hook)\n",
    "\n",
    "# Forward pass again to collect used parameters\n",
    "model(mock_input)\n",
    "\n",
    "# Remove all hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "all_gradients_present = True\n",
    "\n",
    "# Check gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if param in used_params and param.grad is None:\n",
    "            # Used parameter without gradient -> Error\n",
    "            print(f\"Error: Used parameter {name} requires gradients but has none.\")\n",
    "            all_gradients_present = False\n",
    "        elif param not in used_params:\n",
    "            # Unused parameter -> Warning\n",
    "            print(f\"Warning: Parameter {name} was not used in the forward pass.\")\n",
    "\n",
    "if all_gradients_present:\n",
    "    print('Differentiability test passed')\n",
    "else:\n",
    "    print('Differentiability test failed due to missing gradients.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INIT_DESIGN_INPUT='''\n",
    "Below is the specification for the GAU you need to refine:\n",
    "Specification: Unit Name: GatedMLP The DynamicGatedMLP is an enhanced version of the GatedMLP that introduces a dynamic gating mechanism capable of selecting between multiple activation functions. This design aims to improve the expressiveness and adaptability of the model by allowing it to dynamically choose the most suitable activation function for different inputs. The unit maintains the same input-output interface as the original GatedMLP, ensuring seamless integration with the existing GPT-2 architecture.\n",
    "\n",
    "\n",
    "Inputs: X\n",
    "Outputs: Y\n",
    "Children list: []\n",
    "\n",
    "Current Implementation: import torch import torch.nn as nn from model_discovery.model.utils.modules import GAUBase, gau_test import torch.nn.functional as F\n",
    "\n",
    "class GatedMLP(GAUBase): \"\"\"Generalized Autoregressive Block Unit Input: X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of new intermediate variables to update the current Z} Constraints: Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "embed_dim:    The dimension of the input embeddings\n",
    "    block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "    kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "\"\"\"\n",
    "\n",
    "def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "    device=None, dtype=None, hidden_features=None, out_features=None,\n",
    "    bias=False, multiple_of=128, **kwargs):\n",
    "    self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "    out_features = out_features if out_features is not None else embed_dim\n",
    "    hidden_features = (hidden_features if hidden_features is not None else\n",
    "        int(8 * embed_dim / 3))\n",
    "    hidden_features = (hidden_features + multiple_of - 1\n",
    "        ) // multiple_of * multiple_of\n",
    "    self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n",
    "        self.factory_kwargs)\n",
    "    self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n",
    "        self.factory_kwargs)\n",
    "    self.activations = [F.relu, F.tanh, F.gelu]\n",
    "    self.gate_weights = nn.Parameter(torch.randn(len(self.activations),\n",
    "        hidden_features, **self.factory_kwargs))\n",
    "\n",
    "def _forward(self, X, **Z):\n",
    "    y = self.fc1(X)\n",
    "    y, gate = y.chunk(2, dim=-1)\n",
    "    gate_scores = F.softmax(self.gate_weights, dim=0)\n",
    "    y = sum(gate_scores[i] * self.activations[i](gate) for i in range(\n",
    "        len(self.activations)))\n",
    "    y = self.fc2(y)\n",
    "    return y, {}\n",
    "\n",
    "Review: ### Highlights:\n",
    "\n",
    "Successful Implementation Fix: The designer has successfully addressed the previous implementation error by initializing the activations attribute before its use, resolving the AttributeError.\n",
    "Dynamic Gating Mechanism: The design continues to leverage a dynamic gating mechanism, which is a novel approach that enhances the model's adaptability and expressiveness by selecting between multiple activation functions.\n",
    "Checker Validation: The updated implementation has passed all checker tests, including forward pass, causality, and differentiability, indicating that the GAU is functioning correctly within the model.\n",
    "Concerns:\n",
    "Efficiency Considerations: While the implementation is now functional, the potential computational overhead of evaluating multiple activation functions simultaneously remains a concern. This could impact efficiency, especially in larger models or datasets.\n",
    "Justification of Activation Functions: The choice of activation functions (ReLU, Tanh, GELU) is reasonable, but further theoretical justification or empirical validation would strengthen the design.\n",
    "Accuracy, Robustness, Efficiency, and Scalability:\n",
    "Accuracy and Robustness: The dynamic selection of activation functions should enhance the model's ability to adapt to various input patterns, potentially improving accuracy and robustness.\n",
    "Efficiency: The design could still benefit from optimizations to reduce computational overhead, ensuring it remains efficient in practice.\n",
    "Scalability: The design appears scalable, as indicated by the successful checker tests, but empirical validation on larger datasets would be beneficial.\n",
    "Novelty:\n",
    "The design introduces a novel dynamic gating mechanism, which is a significant improvement over static activation functions.\n",
    "Clarity and Completeness:\n",
    "The design is now clear and complete, with the previous implementation error resolved.\n",
    "Theoretical Soundness:\n",
    "The theoretical foundation of using dynamic gating is sound, and the implementation aligns well with the proposal's objectives.\n",
    "Implementation Feasibility:\n",
    "The current implementation is feasible and has been validated by the checker, indicating it can be integrated into the broader model.\n",
    "Suggestions:\n",
    "Efficiency Optimization: Consider exploring methods to optimize the dynamic gating mechanism to reduce computational overhead, such as selectively applying it or using more efficient computation techniques.\n",
    "Empirical Validation: Plan for empirical validation to confirm the theoretical benefits of the dynamic gating mechanism and the choice of activation functions in practice.\n",
    "Documentation: Ensure thorough documentation of the design choices and their theoretical justifications to aid future refinements and empirical testing.\n",
    "Rating: 4 out of 5 (Passing score >3)\n",
    "\n",
    "Reviewer Suggestions: 1. Efficiency Optimization: Consider exploring methods to optimize the dynamic gating mechanism to reduce computational overhead, such as selectively applying it or using more efficient computation techniques. 2. Empirical Validation: Plan for empirical validation to confirm the theoretical benefits of the dynamic gating mechanism and the choice of activation functions in practice.\n",
    "\n",
    "Documentation: Ensure thorough documentation of the design choices and their theoretical justifications to aid future refinements and empirical testing.\n",
    "Refinement Process\n",
    "If there is a review provided, you should start by reflecting on the feedback. Otherwise, leave reflection empty. The, proceed with the following:\n",
    "\n",
    "New Analysis and Design: - Provide an updated detailed analysis based on the feedback, including your new design direction and justifications. - Include a high-level pseudocode that captures the core of the new design. You should also provide the updated document of the GAU that allows other people to understand the design and implementation without the need to read the code.\n",
    "\n",
    "Implementation: - Provide the full updated implementation of the GAU, following the specified format and templates.\n",
    "\n",
    "Children list: - Provide the list of the children GAUs that are declared in the current GAU. You can declare new children GAUs or preserve the existing ones. If you do not declare any new children GAUs, you should provide the original children GAUs.\n",
    "\n",
    "Log of Changes: - Summarize the key changes you made during the refinement process. Including all code snippets where you made a change wrapped in python .\n",
    "\n",
    "Key Points to Remember:\n",
    "The bug or issue must always be resolved within the current GAU, as other units are either fully implemented and tested or placeholders that do not perform any computation.\n",
    "Ensure the GAU is self-contained, so you won't need to adjust it later when working on other units.\n",
    "The design must align with the original proposal and follow all instructions, templates, and format requirements.\n",
    "Use a top-down approach: break down complex operations into smaller tasks where necessary and declare each of them as a child GAU. Do not make a single unit overly complex.\n",
    "Remember your final goal is to refine the GAU in a way that enhances the overall design, ensuring both correctness and innovation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.prompts.prompts as P\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DESIGNER_SYSTEM=P.GUE_DESIGNER_SYSTEM(GAB_BASE=GAB_BASE,GAU_BASE=GAU_BASE,GAU_TEMPLATE=GAU_TEMPLATE)\n",
    "system=[{\"type\": \"text\", \"text\": DESIGNER_SYSTEM, \"cache_control\": {\"type\": \"ephemeral\"}}]\n",
    "\n",
    "# messages = [\n",
    "#     SystemMessage(system),\n",
    "#     HumanMessage(TEST_INIT_DESIGN_INPUT)\n",
    "# ]\n",
    "\n",
    "message=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": TEST_INIT_DESIGN_INPUT,\n",
    "            \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "\n",
    "SELECTIONS=['GPT2','RMSNorm','GatedMLP','MHA','RotaryPositionalEmbeddings']\n",
    "lc_model = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20240620\", temperature=0,\n",
    "    extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    ")\n",
    "structured_llm = lc_model.with_structured_output(P.gen_GUE_DESIGN_PROPOSAL(SELECTIONS).format,include_raw=True)\n",
    "tools_args=structured_llm.dict()['first']['steps__']['raw']['kwargs']\n",
    "        # tools[0]['cache_control']={\"type\": \"ephemeral\"}\n",
    "\n",
    "RET=anthropic.Anthropic().messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\", # model in config is ignored\n",
    "    max_tokens=8192,\n",
    "    messages=message, \n",
    "    temperature=0,\n",
    "    system=system, # claude does not has system role, system prompt must be passed separately\n",
    "    extra_headers=extra_headers,\n",
    "    **tools_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RET.dict()['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.gen_GUE_DESIGN_PROPOSAL(SELECTIONS).format.model_validate(RET.content[0].input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RET.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm.dict()['first']['steps__']['raw']['kwargs']['tools'][0]['input_schema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Message(id='msg_01Nvch8XBwzwrrMBETHpch49', content=[TextBlock(text=\"Thank you for providing the seed design and references. I'll now propose a design improvement for the GPT2 model, focusing on modifying one of its GAUs.\", type='text'), ToolUseBlock(id='toolu_01PpbFTQ6NDwJJCfj1cRDvFS', input={'selection': 'RotaryPositionalEmbeddings', 'modelname': 'GPT2-RoPE', 'proposal': \"\\n# GPT2 with Enhanced Rotary Position Embeddings (GPT2-RoPE)\\n\\n## Motivation\\n\\nThe current GPT2 model uses a basic implementation of Rotary Positional Embeddings (RoPE). While this is already an improvement over traditional positional encodings, there's room for enhancement, particularly in handling longer sequences and improving the model's ability to capture relative positions more effectively.\\n\\n## Problem Analysis\\n\\n1. **Limited sequence length handling**: The current implementation may not scale well to very long sequences, which is increasingly important in modern language models.\\n2. **Lack of flexibility**: The existing RoPE implementation doesn't allow for easy adjustment of its properties, such as the rate of position information decay.\\n3. **Potential for improved performance**: While RoPE is effective, recent research suggests that more sophisticated implementations can lead to better model performance, especially in tasks requiring understanding of long-range dependencies.\\n\\n## Core Idea and Philosophy\\n\\nWe propose to enhance the RotaryPositionalEmbeddings GAU by incorporating ideas from the RoFormer paper and the more recent developments in rotary embeddings. The core philosophy is to create a more flexible and powerful positional embedding system that can:\\n\\n1. Handle longer sequences more effectively\\n2. Allow for fine-tuning of the position encoding properties\\n3. Potentially improve the model's performance on tasks requiring long-range understanding\\n\\n## Design Plan\\n\\nWe will modify the RotaryPositionalEmbeddings GAU as follows:\\n\\n1. **Implement a more sophisticated RoPE class**: Based on the RoFormer paper and recent developments, we'll create a new RotaryEmbedding class that offers more flexibility and features.\\n\\n2. **Add support for learned frequencies**: This will allow the model to potentially learn better position representations for specific tasks or domains.\\n\\n3. **Implement interpolation factor**: This will enable the model to handle sequences longer than those seen during training more effectively.\\n\\n4. **Add support for XPos (Extrapolatable Position Embeddings)**: This will further enhance the model's ability to handle longer sequences and improve its understanding of relative positions.\\n\\n5. **Implement caching mechanism**: To improve efficiency, especially for autoregressive generation tasks.\\n\\n6. **Add support for different frequency types**: This will allow the embeddings to be optimized for different types of data (e.g., language, images).\\n\\nHere's a high-level overview of the proposed changes:\\n\\n```python\\nclass RotaryEmbedding(nn.Module):\\n    def __init__(\\n        self,\\n        dim,\\n        custom_freqs = None,\\n        freqs_for = 'lang',\\n        theta = 10000,\\n        max_freq = 10,\\n        num_freqs = 1,\\n        learned_freq = False,\\n        use_xpos = False,\\n        xpos_scale_base = 512,\\n        interpolate_factor = 1.,\\n        theta_rescale_factor = 1.,\\n        seq_before_head_dim = False,\\n        cache_if_possible = True\\n    ):\\n        super().__init__()\\n        # Implementation details...\\n\\n    def rotate_queries_or_keys(self, t, seq_dim = None, offset = 0, scale = None):\\n        # Implementation details...\\n\\n    def rotate_queries_and_keys(self, q, k, seq_dim = None):\\n        # Implementation details...\\n\\n    def get_scale(self, t, seq_len = None, offset = 0):\\n        # Implementation details...\\n\\n    def forward(self, t, seq_len = None, offset = 0):\\n        # Implementation details...\\n```\\n\\nThis new implementation will replace the current RotaryPositionalEmbeddings GAU in the GPT2 model.\\n\\n## Conclusion\\n\\nBy enhancing the RotaryPositionalEmbeddings GAU with these improvements, we expect to see several benefits:\\n\\n1. **Improved handling of long sequences**: The model should be able to maintain performance even on sequences longer than those seen during training.\\n2. **Better capture of relative positions**: The enhanced RoPE implementation should allow the model to better understand and utilize relative position information.\\n3. **Increased flexibility**: The new implementation allows for fine-tuning of various aspects of the positional embeddings, which can be optimized for specific tasks or domains.\\n4. **Potential performance improvements**: These enhancements may lead to improved performance on tasks requiring long-range understanding and more nuanced use of positional information.\\n\\nThis modification maintains the overall structure of the GPT2 model while significantly enhancing its ability to handle and utilize positional information, potentially leading to improved performance across a wide range of natural language processing tasks.\\n\\n## References\\n\\n1. Su, J., Lu, Y., Pan, S., Wen, B., & Liu, Y. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv preprint arXiv:2104.09864.\\n2. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint arXiv:2401.04088.\\n\"}, name='GUE_DESIGN_PROPOSAL_format', type='tool_use')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='tool_use', stop_sequence=None, type='message', usage=Usage(input_tokens=16630, output_tokens=1325))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content='Reflection on Feedback:\\nThe reviewer\\'s suggestions highlight the need for efficiency optimization, empirical validation, and thorough documentation. I\\'ll address these points in the refined design.\\n\\nNew Analysis and Design:\\n\\nThe current GatedMLP design introduces a dynamic gating mechanism that allows the model to select between multiple activation functions. While this approach enhances adaptability, it may introduce computational overhead. To address this, we\\'ll implement a more efficient gating mechanism that maintains the benefits of dynamic activation selection while reducing computational costs.\\n\\nUpdated Design Direction:\\n1. Efficient Gating: Instead of applying all activation functions and then combining them, we\\'ll use a lightweight gating network to predict weights for each activation function. This approach reduces the number of computations while still allowing dynamic selection.\\n\\n2. Sparse Activation: We\\'ll introduce a sparsity factor in the gating mechanism, allowing the model to focus on the most relevant activation functions for each input.\\n\\n3. Adaptive Computation: We\\'ll implement a mechanism to dynamically adjust the number of active gates based on the input complexity, further optimizing computational resources.\\n\\nHigh-level Pseudocode:\\n```\\nfunction GatedMLP(X):\\n    hidden = linear_projection(X)\\n    gate_weights = lightweight_gate_network(hidden)\\n    sparse_gate_weights = apply_sparsity(gate_weights)\\n    activated = sum(sparse_gate_weights[i] * activation[i](hidden) for i in range(num_activations))\\n    output = linear_projection(activated)\\n    return output\\n```\\n\\nUpdated GAU Document:\\nThe GatedMLP unit now incorporates an efficient dynamic gating mechanism that selects between multiple activation functions. Key features include:\\n- Lightweight gating network for predicting activation weights\\n- Sparse activation selection to focus on the most relevant functions\\n- Adaptive computation based on input complexity\\n- Maintains the same input-output interface as the original GatedMLP\\n\\nThis design aims to balance expressiveness and computational efficiency, allowing the model to adapt to various input patterns while managing resource usage effectively.\\n\\nImplementation:\\n\\n```python\\n# gau.py\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test\\n\\nclass GatedMLP(GAUBase):\\n    \"\"\"\\n    Generalized Autoregressive Block Unit with efficient dynamic gating\\n    Input:  X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \\n    Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of new intermediate variables to update the current Z}\\n    \"\"\"\\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\\n                 device=None, dtype=None, hidden_features=None, out_features=None,\\n                 bias=False, multiple_of=128, num_activations=3, sparsity_factor=0.5, **kwargs):\\n        self.factory_kwargs = {\\'device\\': device, \\'dtype\\': dtype}\\n        super().__init__(embed_dim, block_loc, kwarg_all)\\n        \\n        out_features = out_features if out_features is not None else embed_dim\\n        hidden_features = hidden_features if hidden_features is not None else int(8 * embed_dim / 3)\\n        hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of\\n        \\n        self.fc1 = nn.Linear(embed_dim, hidden_features, bias=bias, **self.factory_kwargs)\\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **self.factory_kwargs)\\n        \\n        self.gate_network = nn.Sequential(\\n            nn.Linear(hidden_features, 64, bias=False, **self.factory_kwargs),\\n            nn.ReLU(),\\n            nn.Linear(64, num_activations, bias=False, **self.factory_kwargs)\\n        )\\n        \\n        self.activations = [F.relu, F.tanh, F.gelu]\\n        self.sparsity_factor = sparsity_factor\\n\\n    def _forward(self, X, **Z):\\n        hidden = self.fc1(X)\\n        gate_weights = self.gate_network(hidden)\\n        \\n        # Apply softmax and sp'\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from core library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import importlib\n",
    "import model_discovery.model.composer \n",
    "# importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUBase, GAUTree, check_tree_name, GABComposer\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "import model_discovery.utils as U\n",
    "\n",
    "# dir='/home/junyanc/model_discovery/model_discovery/model/library/core/gpt2/units'\n",
    "dir='C://ChengJunyan1//Research//model_discovery//model_discovery//model//library//core//gpt2//units'\n",
    "tree=GAUTree.load_from_base(dir)\n",
    "\n",
    "print(tree.view()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "gam_prompt_path = 'C://ChengJunyan1//Research//model_discovery//model_discovery//agents//prompts//gam_prompt.py'\n",
    "gau_template_path = 'C://ChengJunyan1//Research//model_discovery//model_discovery//agents//prompts//gau_template.py'\n",
    "GAM_TEMPLATE=open(gam_prompt_path).read()\n",
    "GAU_TEMPLATE=open(gau_template_path).read()\n",
    "\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n",
    "GAB_BASE=inspect.getsource(GABBase)\n",
    "GAB_COMPOSER=inspect.getsource(GABComposer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GAM_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Union, List, Dict\n",
    "import time\n",
    "\n",
    "def search_for_papers(query, result_limit=10) -> Union[None, List[Dict]]:\n",
    "    # https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data/operation/post_graph_get_papers\n",
    "    # should also search from the internal base or KGs\n",
    "    if not query:\n",
    "        return None\n",
    "    S2_API_KEY=os.environ['S2_API_KEY']\n",
    "    rsp = requests.get(\n",
    "        \"https://api.semanticscholar.org/graph/v1/paper/search\",\n",
    "        headers={\"X-API-KEY\": S2_API_KEY},\n",
    "        params={\n",
    "            \"query\": query,\n",
    "            \"limit\": result_limit,\n",
    "            \"fields\": \"title,authors,venue,year,abstract,citationStyles,citationCount\",\n",
    "            'fieldsOfStudy': 'Computer Science,Mathematics,Physics'\n",
    "        },\n",
    "    )\n",
    "    print(f\"Response Status Code: {rsp.status_code}\")\n",
    "    print(\n",
    "        f\"Response Content: {rsp.text[:500]}\"\n",
    "    )  # Print the first 500 characters of the response content\n",
    "    rsp.raise_for_status()\n",
    "    results = rsp.json()\n",
    "    total = results[\"total\"]\n",
    "    time.sleep(1.0)\n",
    "    if not total:\n",
    "        return None\n",
    "\n",
    "    papers = results[\"data\"]\n",
    "    return papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Status Code: 200\n",
      "Response Content: {\"total\": 0, \"offset\": 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ret=search_for_papers('\"Retentive Network\" OR \"RetNet\" AND \"attention mechanism\" OR \"traditional attention\" OR \"training parallelism\" OR \"low-cost inference\" OR \"Ring Attention\" OR \"long sequences\" OR \"Dynamical Systems Framework\" OR \"state space models\" OR \"RNNs\" OR \"hybrid approaches\" OR \"robustness\" OR \"generalization\" OR \"Connect Later\" OR \"targeted augmentations\" OR \"efficient attention mechanisms\" OR \"gated linear units\" OR \"mixture of experts\" OR \"positional encoding\" OR \"normalization techniques\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992',\n",
       " 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       " 'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).',\n",
       " 'venue': 'North American Chapter of the Association for Computational Linguistics',\n",
       " 'year': 2019,\n",
       " 'citationCount': 80678,\n",
       " 'openAccessPdf': None,\n",
       " 'citationStyles': {'bibtex': '@Article{Devlin2019BERTPO,\\n author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},\\n booktitle = {North American Chapter of the Association for Computational Linguistics},\\n pages = {4171-4186},\\n title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\\n year = {2019}\\n}\\n'},\n",
       " 'authors': [{'authorId': '39172707', 'name': 'Jacob Devlin'},\n",
       "  {'authorId': '1744179', 'name': 'Ming-Wei Chang'},\n",
       "  {'authorId': '2544107', 'name': 'Kenton Lee'},\n",
       "  {'authorId': '3259253', 'name': 'Kristina Toutanova'}],\n",
       " 'citations': [{'paperId': 'fdf0eff9a41f5ecf08a8f8ef7e8f087cb1f5c2d8',\n",
       "   'title': 'The determinants of parallel invention: Measuring the role of information sharing and personal interaction between inventors'},\n",
       "  {'paperId': '586da3abc8d10eea0fe32991fb714dffa842663a',\n",
       "   'title': 'Exploring motivations for algorithm mention in the domain of natural language processing: A deep learning approach'},\n",
       "  {'paperId': '3bb7dc450854a762e6e74efdba9b5eafe6c494b9',\n",
       "   'title': 'Evolutions of semantic consistency in research topic via contextualized word embedding'},\n",
       "  {'paperId': '51209f3c1f78dc653a02a184fc17ac1929fae74e',\n",
       "   'title': 'Masked autoencoder with dynamic multi-loss adaptation mechanism for few shot wafer map pattern recognition'},\n",
       "  {'paperId': 'a56ac5f5eb688a5c195f70a2cefa9b583263a0b9',\n",
       "   'title': 'Framework for automation of short answer grading based on domain-specific pre-training'},\n",
       "  {'paperId': 'a76a70e37ed378d546ef99c77d231a67f24be57b',\n",
       "   'title': 'Learning from Yesterday: Predicting early-stage startup success for accelerators through content and cohort dynamics'},\n",
       "  {'paperId': '22d4a962c85fdacba765af92ddbc91f5ae984189',\n",
       "   'title': 'A comprehensive survey of text classification techniques and their research applications: Observational and experimental insights'},\n",
       "  {'paperId': '8dee088fe89d32e49b92221a623149dd26ddaf9a',\n",
       "   'title': 'Robust and resource-efficient table-based fact verification through multi-aspect adversarial contrastive learning'},\n",
       "  {'paperId': '5b65e391aece4ef3c5c0eb85050e4ab8f22c3bce',\n",
       "   'title': 'A data fusion method for maritime traffic surveillance: The fusion of AIS data and VHF speech information'},\n",
       "  {'paperId': 'ebe94edd919b14426bf1c01d4c65d0b0d45536fb',\n",
       "   'title': 'Multi-level feature interaction for open knowledge base canonicalization'},\n",
       "  {'paperId': '714525d4a69dc7252b6626cd13ded06e56a9e2b2',\n",
       "   'title': 'Contrastive Learning with Transformer Initialization and Clustering Prior for Text Representation'},\n",
       "  {'paperId': 'ed91538e0f0f0d4a3c0f3ec7a4b650fb67fe4a45',\n",
       "   'title': 'Exploring the potential of DistilBERT architecture for automatic essay scoring task'},\n",
       "  {'paperId': 'dc439837822accb4b5f69c68e15e23ebec5238f6',\n",
       "   'title': 'Self-prompting semantic segmentation of bridge point cloud data using a large computer vision model'},\n",
       "  {'paperId': '6f6e179da67efc74f66753739b791410bc3a63ee',\n",
       "   'title': 'Artificial intelligence in oncology: ensuring safe and effective integration of language models in clinical practice'},\n",
       "  {'paperId': 'f1cd70ca33929a0e28634cd83df473e8ba5beb6a',\n",
       "   'title': 'Preventing future zoonosis: SARS-CoV-2 mutations enhance human–animal cross-transmission'},\n",
       "  {'paperId': 'aadfa321d82136e178440a322087a1c751439007',\n",
       "   'title': 'Aspect Based Sentiment Analysis Using Modified Latent Dirichlet Allocation and Optimized BERT with LSTM Classification'},\n",
       "  {'paperId': '776e62740ec688da9f10e4991af921784296687f',\n",
       "   'title': 'Optimizing Aspect Term Extraction and Sentiment Classification through Attention Mechanism and Sparse Attention Techniques'},\n",
       "  {'paperId': 'ce312750b3960f84eee74041cd243555478cf48d',\n",
       "   'title': 'Using Deep Learning Techniques to Detect Hate and Abusive Language in Arabic Tweets'},\n",
       "  {'paperId': '8709817f5cd722c1ced8e447462ad6f1e621a529',\n",
       "   'title': 'ABSA of Indonesian customer reviews using IndoBERT: single- sentence and sentence-pair classification approaches'},\n",
       "  {'paperId': 'f003174f79ead9bd17f15a0f64f9112e98816ad7',\n",
       "   'title': 'Named entity recognition on Indonesian legal documents: a dataset and study using transformer-based models'},\n",
       "  {'paperId': '43fa5781158676f9fc6e3d6b8919fa9dfa898f24',\n",
       "   'title': 'Full-view salient feature mining and alignment for text-based person search'},\n",
       "  {'paperId': '0382a9d3a138b9f17e9764c6bc8868c4eadcbd0e',\n",
       "   'title': 'A fused convolutional transformer for voltammetric electronic tongue analysis tasks'},\n",
       "  {'paperId': 'f9b10cbdaa4d7c4b1598c14c1687244cecc3bc23',\n",
       "   'title': \"Profiling students' learning engagement in MOOC discussions to identify learning achievement: An automated configurational approach\"},\n",
       "  {'paperId': 'c69cf4a48608305b69a4acdbd1cfaef1fc04582d',\n",
       "   'title': 'A new risk level identification model for aviation safety'},\n",
       "  {'paperId': '3ca1341e271c052a6a03099f7bb7ae1ce0dde3f0',\n",
       "   'title': 'Probing vision and language models for construction waste material recognition'},\n",
       "  {'paperId': '11578049cf5df8d68ea988596ecf6c54efe9e930',\n",
       "   'title': 'Exploring bridge maintenance knowledge graph by leveraging GrapshSAGE and text encoding'},\n",
       "  {'paperId': '51988539458fc4187223160d9af81158050b29a6',\n",
       "   'title': 'End-to-end framework for agricultural entity extraction - A hybrid model with transformer'},\n",
       "  {'paperId': '9cb7b5c7afdb9a2bf84cc235cec7c0093f831433',\n",
       "   'title': 'Spatial intelligence and contextual relevance in AI-driven health information retrieval'},\n",
       "  {'paperId': 'e4f8ff78ae5dce4dfa470ab94c9844f785bf5a8f',\n",
       "   'title': 'AgXQA: A benchmark for advanced Agricultural Extension question answering'},\n",
       "  {'paperId': 'c967a498376e86c8a5aea1b7aab4c3e69a804cd5',\n",
       "   'title': 'Is the U.S. energy independence and Security Act of 2022 associated with stock market volatility?'},\n",
       "  {'paperId': '77c55a49eca650d7f0ed447d9e813eb2f4d30fa8',\n",
       "   'title': 'The influence of two and three-dimensional spatial characteristics of industrial parks on the emotional well-being of employees: A case study of Shenzhen'},\n",
       "  {'paperId': '9ebba638e93957f4426da0d0b7b8cf3aec2997a6',\n",
       "   'title': 'Knowledge structures for knowledge communication: Dominant semantic frames in research articles'},\n",
       "  {'paperId': 'c515b6ee7cfbcb98125bc34db91c9d11abe76799',\n",
       "   'title': 'Sensing noise exposure and its inequality based on noise complaint data through vision-language hybrid method'},\n",
       "  {'paperId': '6ace1991a23327ebc6cc6b9a54723cfcc97d35d7',\n",
       "   'title': 'SARD: Fake news detection based on CLIP contrastive learning and multimodal semantic alignment'},\n",
       "  {'paperId': '8e9bda22c60b1fb5e65e32679d729cc0ac4b69a8',\n",
       "   'title': 'Insights from cross-cultural memes: An empirical study on instagram and Douban'},\n",
       "  {'paperId': '1042cbab483be8d8bf0635141a0dcc5fa8d45761',\n",
       "   'title': 'Natural Language Processing for Arabic Sentiment Analysis: A Systematic Literature Review'},\n",
       "  {'paperId': '8d8e8f5cfdd68691043f2ff97e155715f8b70fbe',\n",
       "   'title': 'SIGMA: Secure GPT Inference with Function Secret Sharing'},\n",
       "  {'paperId': '47643640a7e40d020d7f92cacb727d1a691e5625',\n",
       "   'title': 'Research on Multifeature Fusion False Review Detection Based on DynDistilBERT-BiLSTM-CNN'},\n",
       "  {'paperId': '402c6348cb453255f88fb571c8e52c0a3655bde9',\n",
       "   'title': 'Human Activity Recognition with Unsupervised Learning of Event Logs'},\n",
       "  {'paperId': 'c09e8c469912f69f45997482d9692afd7663c1df',\n",
       "   'title': 'N-gram Prediction and Word Difference Representations for Language Modeling'},\n",
       "  {'paperId': '727846d963c5568623def1fd36de37664e6f84ae',\n",
       "   'title': 'An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification'},\n",
       "  {'paperId': '64b3c3b90296afa8b4f3454ee7e7af3ccd47f5ca',\n",
       "   'title': 'Foundation Model or Finetune? Evaluation of few-shot semantic segmentation for river pollution'},\n",
       "  {'paperId': 'f73ed53fa7ff89114e863624ef2bce336c1f996b',\n",
       "   'title': 'Improving agent performance in fluid environments by perceptual pretraining'},\n",
       "  {'paperId': '11cd38ea7f46136bf9aff6a50abb179c7d01e45e',\n",
       "   'title': 'Efficient Multi-Task Large Model Training via Data Heterogeneity-aware Model Management'},\n",
       "  {'paperId': '99dfb244dc75f0fec7bf44dd62923fb56a51f607',\n",
       "   'title': 'TG-LMM: Enhancing Medical Image Segmentation Accuracy through Text-Guided Large Multi-Modal Model'},\n",
       "  {'paperId': '1e82238a89e91fd4f2cda42307d558ea8d7941ec',\n",
       "   'title': 'End User Authoring of Personalized Content Classifiers: Comparing Example Labeling, Rule Writing, and LLM Prompting'},\n",
       "  {'paperId': '19c447fcac9bc805aff7bbeec6e4f8bb2993e403',\n",
       "   'title': 'State-space models are accurate and efficient neural operators for dynamical systems'},\n",
       "  {'paperId': 'd95dbbdbceaf6e856ddf688924cb9997b0e98ce4',\n",
       "   'title': 'Preserving Empirical Probabilities in BERT for Small-sample Clinical Entity Recognition'},\n",
       "  {'paperId': 'a4b972fa5a6c5c0e0edd48ffe069b6d5e98691d6',\n",
       "   'title': 'LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts'},\n",
       "  {'paperId': 'ab0f338f16ff24cff2d5b43df3f87a12097f2c7a',\n",
       "   'title': 'The AdEMAMix Optimizer: Better, Faster, Older'},\n",
       "  {'paperId': 'd7b8ff63f5e410afccfd40f5cd67b656160ab386',\n",
       "   'title': 'Pre-training data selection for biomedical domain adaptation using journal impact metrics'},\n",
       "  {'paperId': '5692fdc4f3aef9fe6074d1433358f633637177d4',\n",
       "   'title': 'How Privacy-Savvy Are Large Language Models? A Case Study on Compliance and Privacy Technical Review'},\n",
       "  {'paperId': 'c20f2b6c5d405f3828a41546b0b725512dc955f1',\n",
       "   'title': 'Pooling And Attention: What Are Effective Designs For LLM-Based Embedding Models?'},\n",
       "  {'paperId': 'e85213146c7f50474c72116f33d36196c1c2e857',\n",
       "   'title': 'Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling'},\n",
       "  {'paperId': 'ac611baf4a6d1503b98432326a9eaa27e02884ce',\n",
       "   'title': 'iConFormer: Dynamic Parameter-Efficient Tuning with Input-Conditioned Adaptation'},\n",
       "  {'paperId': '53c52d152adeb89652a1b0ca2cb3514f3b8548bb',\n",
       "   'title': 'A Comparative Study of Pre-training and Self-training'},\n",
       "  {'paperId': '6f8c58c34942763976f2878eea3352191070d438',\n",
       "   'title': 'Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for PostNL'},\n",
       "  {'paperId': '55ad5b0fcd0fda50188aaf38b8beefb183d7964f',\n",
       "   'title': 'Understanding anti-immigration sentiment spreading on Twitter'},\n",
       "  {'paperId': '62f91aa013c1cea2b47d46d72dd7512e6a073c43',\n",
       "   'title': 'Accelerating Large Language Model Training with Hybrid GPU-based Compression'},\n",
       "  {'paperId': 'e4fd14d5631250f3b8f7b809e70fd5475d887109',\n",
       "   'title': 'Local map Construction Methods with SD map: A Novel Survey'},\n",
       "  {'paperId': 'ba48cc281c6331ff6959817b9d180be0392489be',\n",
       "   'title': 'Do Large Language Models Possess Sensitive to Sentiment?'},\n",
       "  {'paperId': 'c43b2db0045129724e965a0b253bb13f61e27439',\n",
       "   'title': 'Efficient Extraction of Noise-Robust Discrete Units from Self-Supervised Speech Models'},\n",
       "  {'paperId': '193948beb8c3c7368f0a82267c0b526f2db064fc',\n",
       "   'title': 'Building a Scalable, Effective, and Steerable Search and Ranking Platform'},\n",
       "  {'paperId': 'fbfb80e73bacb0d4d3d108c918dd5b1d5b9d1b4f',\n",
       "   'title': 'SG-MIM: Structured Knowledge Guided Efficient Pre-training for Dense Prediction'},\n",
       "  {'paperId': '347c716a1c3746af4de27ca5d033aba45b26b2ae',\n",
       "   'title': 'MaDis-Stereo: Enhanced Stereo Matching via Distilled Masked Image Modeling'},\n",
       "  {'paperId': '744c3c6338daf0e8a0c69fc627be9f7a082c3a1e',\n",
       "   'title': 'Detecting Calls to Action in Multimodal Content: Analysis of the 2021 German Federal Election Campaign on Instagram'},\n",
       "  {'paperId': '2d963ddec6a1ce911e95cafd672f82278030d148',\n",
       "   'title': 'Bioinformatics Retrieval Augmentation Data (BRAD) Digital Assistant'},\n",
       "  {'paperId': '92db5a46c3f97423686e9a01f2209003ad5543e7',\n",
       "   'title': 'GoT-CQA: Graph-of-Thought Guided Compositional Reasoning for Chart Question Answering'},\n",
       "  {'paperId': 'f2109f7f2412308738e03a60f79946cb1ad2aa7a',\n",
       "   'title': 'Alignment-Aware Model Extraction Attacks on Large Language Models'},\n",
       "  {'paperId': 'f4b49ddbfc15bb47dc9e3d83fa086d0c6b67107b',\n",
       "   'title': 'R2GQA: Retriever-Reader-Generator Question Answering System to Support Students Understanding Legal Regulations in Higher Education'},\n",
       "  {'paperId': 'b14cb2ac1993548385b8e7492c3fadc6edfb5a42',\n",
       "   'title': 'MADiff: Motion-Aware Mamba Diffusion Models for Hand Trajectory Prediction on Egocentric Videos'},\n",
       "  {'paperId': '5b656804ea9b98dc19f08ec94f83335674e10af5',\n",
       "   'title': 'iP3T: an interpretable multimodal time-series model for enhanced gait phase prediction in wearable exoskeletons'},\n",
       "  {'paperId': '788cc951a7e779f7cea0c8cb149e74488b90f7b5',\n",
       "   'title': 'A Physics‐Enhanced Neural Network for Estimating Longitudinal Dispersion Coefficient and Average Solute Transport Velocity in Porous Media'},\n",
       "  {'paperId': '1b229f8cc00496aa6ddb734022d55000043fbc4f',\n",
       "   'title': 'Political DEBATE: Efficient Zero-shot and Few-shot Classifiers for Political Text'},\n",
       "  {'paperId': 'b6224cabb7482249d7cd1acb81fd7c02fef7486c',\n",
       "   'title': 'Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining'},\n",
       "  {'paperId': '48020e5f1a0d5703f6169c20051eeb056194c25b',\n",
       "   'title': 'Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey'},\n",
       "  {'paperId': 'd418baf7f7f273107ae62ee0d3fdc3bcb48f62f1',\n",
       "   'title': 'Differentially Private Kernel Density Estimation'},\n",
       "  {'paperId': '973e40719495b1b3f0f8ab236500c5b51ac712d3',\n",
       "   'title': 'Dreaming is All You Need'},\n",
       "  {'paperId': '207c560f1026d924910a750073bcdbf1ace81118',\n",
       "   'title': 'SPiKE: 3D Human Pose from Point Cloud Sequences'},\n",
       "  {'paperId': '41890be39c363320fb62ec534dd4e9b5f8143219',\n",
       "   'title': 'State-of-the-art Advances of Deep-learning Linguistic Steganalysis Research'},\n",
       "  {'paperId': 'e59f0c0ad2699b94b91968611e00e467b83e0598',\n",
       "   'title': 'Dynamic Motion Synthesis: Masked Audio-Text Conditioned Spatio-Temporal Transformers'},\n",
       "  {'paperId': '9733fef11486689b7716963e0d9ff98b3af75935',\n",
       "   'title': 'Self-Instructed Derived Prompt Generation Meets In-Context Learning: Unlocking New Potential of Black-Box LLMs'},\n",
       "  {'paperId': '7bf9f13817f739a8c1bcc96608085c5840b8f2a1',\n",
       "   'title': 'Dynamic Guidance Adversarial Distillation with Enhanced Teacher Knowledge'},\n",
       "  {'paperId': '7828d7e69a13b27032cd90c3c846fbb5d70771e0',\n",
       "   'title': 'LLM-GAN: Construct Generative Adversarial Network Through Large Language Models For Explainable Fake News Detection'},\n",
       "  {'paperId': 'cb5368a81f15281e54615d863d720e0de2a261f4',\n",
       "   'title': 'Exploiting the Vulnerability of Large Language Models via Defense-Aware Architectural Backdoor'},\n",
       "  {'paperId': '9a79e82d93284bcd1878fcb823541213393261f5',\n",
       "   'title': 'Purification-Agnostic Proxy Learning for Agentic Copyright Watermarking against Adversarial Evidence Forgery'},\n",
       "  {'paperId': 'c14df4e5147e0f36d14de35e12b21f4b74762951',\n",
       "   'title': 'LUK: Empowering Log Understanding with Expert Knowledge from Large Language Models'},\n",
       "  {'paperId': '529ac66e4890b5e404f16b855236e872c6c9252f',\n",
       "   'title': 'AI-accelerated therapeutic antibody development: practical insights'},\n",
       "  {'paperId': '4fe7716110dcb668c5d9e517e7b901f1c238b647',\n",
       "   'title': 'Attention-Based Reading, Highlighting, and Forecasting of the Limit Order Book'},\n",
       "  {'paperId': 'e4acf514302dea4d2ecbe0b98eea2661f021b39f',\n",
       "   'title': 'How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?'},\n",
       "  {'paperId': 'e2e3112babb1fe1a46fbb8242fcbeea42758b10e',\n",
       "   'title': 'Temporal Order Preserved Optimal Transport-based Cross-modal Knowledge Transfer Learning for ASR'},\n",
       "  {'paperId': 'ad1d86d75dbbeef25659414a7ca1de179abb1ae0',\n",
       "   'title': 'An antimicrobial drug recommender system using MALDI-TOF MS and dual-branch neural networks'},\n",
       "  {'paperId': 'b03beb4d4844a9464e11e470801f50ea02c66f4b',\n",
       "   'title': 'LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning'},\n",
       "  {'paperId': '42a9003ca69a32b52edd7af30d7e50788564f463',\n",
       "   'title': 'A dataset of Open Source Intelligence (OSINT) Tweets about the Russo-Ukrainian war'},\n",
       "  {'paperId': '842f8c5907b69d7e52ff12ed6db76c76ed3622fe',\n",
       "   'title': 'Predicting social media users’ indirect aggression through pre-trained models'},\n",
       "  {'paperId': 'bb4f43b4d297f70f2c618e8eb33bf809350ec81a',\n",
       "   'title': 'Vortex: Efficient Sample-Free Dynamic Tensor Program Optimization via Hardware-aware Strategy Space Hierarchization'},\n",
       "  {'paperId': 'a834e404dbdde939a173977aafa67440a7cae0e4',\n",
       "   'title': 'Beyond ChatGPT: Enhancing Software Quality Assurance Tasks with Diverse LLMs and Validation Techniques'},\n",
       "  {'paperId': '7db61e977fc937d0b4f204914bcf003b5a0cacc3',\n",
       "   'title': 'NYK-MS: A Well-annotated Multi-modal Metaphor and Sarcasm Understanding Benchmark on Cartoon-Caption Dataset'},\n",
       "  {'paperId': '24c2e55f51867864598eda08b3a2c7c9975be104',\n",
       "   'title': 'CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models'},\n",
       "  {'paperId': 'd1443673e68371ae3c37420d99e4d34553367cc1',\n",
       "   'title': 'Expanding on EnCLAP with Auxiliary Retrieval Model for Automated Audio Captioning'},\n",
       "  {'paperId': '86b8380a2dd8f0f7bc96985a413e4f4404c0843f',\n",
       "   'title': 'Know When to Fuse: Investigating Non-English Hybrid Retrieval in the Legal Domain'},\n",
       "  {'paperId': '7a3d8f0b9d1d76666162d67408a57aff74b74000',\n",
       "   'title': 'DataSculpt: Crafting Data Landscapes for LLM Post-Training through Multi-objective Partitioning'},\n",
       "  {'paperId': '64c354f76a2dd1825a2cf431cb2a1623021d874e',\n",
       "   'title': 'Unveiling the Vulnerability of Private Fine-Tuning in Split-Based Frameworks for Large Language Models: A Bidirectionally Enhanced Attack'},\n",
       "  {'paperId': 'a67761dd50ba4501c1886946221d0357e094f7cd',\n",
       "   'title': 'Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning'},\n",
       "  {'paperId': 'eb97d75364c909cd090d4ef6e2796b95947140ef',\n",
       "   'title': 'LuWu: An End-to-End In-Network Out-of-Core Optimizer for 100B-Scale Model-in-Network Data-Parallel Training on Distributed GPUs'},\n",
       "  {'paperId': 'c27bad7a10ea192b252fa53410b0a6b377132921',\n",
       "   'title': 'MOOSS: Mask-Enhanced Temporal Contrastive Learning for Smooth State Evolution in Visual Reinforcement Learning'},\n",
       "  {'paperId': '4680de3098845ac8d82181de85fcef2a59a4bb3a',\n",
       "   'title': 'The Role of Transformer Models in Advancing Blockchain Technology: A Systematic Survey'},\n",
       "  {'paperId': '37f31bb5d9a8fd31085f9d69f528b0be4e8d724c',\n",
       "   'title': 'Javanese part-of-speech tagging using cross-lingual transfer learning'},\n",
       "  {'paperId': 'ca1d86ce8cbc26d46a9ec6c698ec3576eac5fcae',\n",
       "   'title': 'A fused large language model for predicting startup success'},\n",
       "  {'paperId': '8c8f104d9ab36b4c762a032330bb81723bc71ac9',\n",
       "   'title': 'A Counterfactual Explanation Framework for Retrieval Models'},\n",
       "  {'paperId': '567196f68377d9a2913a9a3a26cea796a2dabd9a',\n",
       "   'title': 'MuSAM: Mutual-Scenario-Aware Multimodal-Enhanced Representation Learning for Semantic Similarity'},\n",
       "  {'paperId': '40edc8e3cd3c4d410f656fbae079fc4b6b655b0f',\n",
       "   'title': 'A survey on membership inference attacks and defenses in machine learning'},\n",
       "  {'paperId': '5120e8a9b24d79bcf0db80a22dee6b1136a756b1',\n",
       "   'title': 'Deep expertise and interest personalized transformer for expert finding'},\n",
       "  {'paperId': 'dbff50e97bd02d62330b08d9b3b03f05bffdb357',\n",
       "   'title': 'Unsupervised Sensor-Based Continuous Authentication With Low-Rank Transformer Using Learning-to-Rank Algorithms'},\n",
       "  {'paperId': '80c511f7f5682c1f03fecc05151bc96cd951a305',\n",
       "   'title': 'VisualSiteDiary: A detector-free Vision-Language Transformer model for captioning photologs for daily construction reporting and image retrievals'},\n",
       "  {'paperId': '27a3f9f5f2fa85c757518480d08ea2da89d874de',\n",
       "   'title': 'Effective underwater acoustic target passive localization of using a multi-task learning model with attention mechanism: Analysis and comparison under real sea trial datasets'},\n",
       "  {'paperId': '325b64055f197e00e400f338e8172ae37ae44ccf',\n",
       "   'title': 'Integrating discourse features and response assessment for advancing empathetic dialogue'},\n",
       "  {'paperId': 'c189d7aa1f2bc7eac2214bd130e9077dfec407f6',\n",
       "   'title': 'MTFR: An universal multimodal fusion method through Modality Transfer and Fusion Refinement'},\n",
       "  {'paperId': '2f257c43e30dc581c0486d9b75c38265f86a7966',\n",
       "   'title': 'Chinese Spelling Correction Based on Knowledge Enhancement and Contrastive Learning'},\n",
       "  {'paperId': '39415e26e5e972b522411161cc40691c82554dd4',\n",
       "   'title': 'Document-level relation extraction with multi-semantic knowledge interaction'},\n",
       "  {'paperId': 'e094868043e203205f62f962d27b9feb51ce9b9f',\n",
       "   'title': 'Span-level bidirectional retention scheme for aspect sentiment triplet extraction'},\n",
       "  {'paperId': '8ae9d7c31cc5932e49c48a95692217c7d1e0fd22',\n",
       "   'title': 'Cross-transfer Knowledge between Speech and Text Encoders to Evaluate Customer Satisfaction'},\n",
       "  {'paperId': '8a5f473a8f5f6b0add201eca78f5c99c5e642d25',\n",
       "   'title': 'Enhancing emotion detection with synergistic combination of word embeddings and convolutional neural networks'},\n",
       "  {'paperId': '28345520e924c9e17debc1cb3e4e89dee4ae52c5',\n",
       "   'title': 'Enhancing corrosion detection in pulsed eddy current testing systems through autoencoder-based unsupervised learning'},\n",
       "  {'paperId': '41084123bbf1156cf6974f16161714e81156c545',\n",
       "   'title': 'Incorporating target-aware knowledge into prompt-tuning for few-shot stance detection'},\n",
       "  {'paperId': '4f501e145054b2cd2ae56a7142fe32153ab750ae',\n",
       "   'title': 'Virality classification from Twitter data using pre-trained language model and multi-layer perceptron'},\n",
       "  {'paperId': '9511eccab655774e08593aebf223da847b81a931',\n",
       "   'title': 'WhisPAr: Transferring pre-trained audio models to fine-grained classification via Prompt and Adapter'},\n",
       "  {'paperId': '193fa284579c9f475d64265326f65bb4de548d05',\n",
       "   'title': 'Artificial intelligence co-piloted auditing'},\n",
       "  {'paperId': '5fdce3200121bb2dcf502c3404257a1883c15158',\n",
       "   'title': 'BERT-based models for classifying multi-dialect Arabic texts'},\n",
       "  {'paperId': 'd55e6122b3fde4c061f0fedbb2cb4c6317a86c93',\n",
       "   'title': 'Technological trajectory analysis in lithium battery manufacturing: Based on patent claims perspective'},\n",
       "  {'paperId': 'ad0ba6dcd3d8e592bbf37d55e4f1b02219263014',\n",
       "   'title': 'WebUltron: An Ultimate Retriever on Webpages Under the Model-Centric Paradigm'},\n",
       "  {'paperId': 'a5fdb4b30fd2208a2784e1eb8f1a553d28db87f5',\n",
       "   'title': 'ToEx: Accelerating Generation Stage of Transformer-Based Language Models via Token-Adaptive Early Exit'},\n",
       "  {'paperId': '095becc96744fd1ebcb2fe7941b439e681ac4f1f',\n",
       "   'title': 'Hardware–Software Co-Design Enabling Static and Dynamic Sparse Attention Mechanisms'},\n",
       "  {'paperId': '53156538322f3c62b02a54b70158877def58e180',\n",
       "   'title': 'Encoding Group Interests With Persistent Homology for Personalized Search'},\n",
       "  {'paperId': 'd767c5198a9cd6c0a23e743da47dae386cd178a5',\n",
       "   'title': 'XFall: Domain Adaptive Wi-Fi-Based Fall Detection With Cross-Modal Supervision'},\n",
       "  {'paperId': 'd5100485ec47e4c2ed20d35e98353ca3a29a53aa',\n",
       "   'title': 'A future role for health applications of large language models depends on regulators enforcing safety standards.'},\n",
       "  {'paperId': '132fd1916bc1facf952d53fd465339b81dc9c188',\n",
       "   'title': 'A new perspective to understand public response to the Typhoon Doksuri from coastal and inland regions'},\n",
       "  {'paperId': '4988a4be2c6c7e58c7aeb8ff4106d736aefce8d0',\n",
       "   'title': 'Natural disasters detection using explainable deep learning'},\n",
       "  {'paperId': '2acc20f154ff2f9571a86fb75a738ead3bffa0e1',\n",
       "   'title': 'ProSTformer: Progressive Space-Time Self-Attention Model for Short-Term Traffic Flow Forecasting'},\n",
       "  {'paperId': 'cdb664aae3107b9ec8265636737a84fda1e102e4',\n",
       "   'title': 'PIGAT: Physics-Informed Graph Attention Transformer for Air Traffic State Prediction'},\n",
       "  {'paperId': '5242e716ac694072c28f0ff19023eeb93a6f21f5',\n",
       "   'title': 'Multi-Faceted Route Representation Learning for Travel Time Estimation'},\n",
       "  {'paperId': '33afc13a1073fe13e7d3c8caa6676e56675de606',\n",
       "   'title': 'A Joint Spatiotemporal Prediction and Image Confirmation Model for Vehicle Trajectory Concatenation With Low Detection Rates'},\n",
       "  {'paperId': '4c62236e6572666b3f7dc2c3653ebd48d1a01c6b',\n",
       "   'title': 'A Novel Approach for Application Classification with Encrypted Traffic using BERT and Packet Headers'},\n",
       "  {'paperId': '22306a553d5ba12a3de225b3abcb1a0662e64dad',\n",
       "   'title': 'Modeling Text-Label Alignment for Hierarchical Text Classification'},\n",
       "  {'paperId': '75a4294b5d8dc194c613e148251e91b8f4dbdaaa',\n",
       "   'title': 'Attention-Guided Multi-scale Interaction Network for Face Super-Resolution'},\n",
       "  {'paperId': '99c46690d07ddee0bc8b09ecc2746454e455d5e4',\n",
       "   'title': 'LLMs as Evaluators: A Novel Approach to Evaluate Bug Report Summarization'},\n",
       "  {'paperId': '6d0e972b8f9cc9e5b626d0b5663a7ab18552f98c',\n",
       "   'title': \"User-Driven Value Alignment: Understanding Users' Perceptions and Strategies for Addressing Biased and Discriminatory Statements in AI Companions\"},\n",
       "  {'paperId': 'e75fced438c5ab9c67a2972c658f075d0f0ddab6',\n",
       "   'title': 'Feature/vector entity retrieval and disambiguation techniques to create a supervised and unsupervised semantic table interpretation approach'},\n",
       "  {'paperId': 'da7ede42ac91f6478731cf8d94e812f57f664ee7',\n",
       "   'title': 'Towards a configurable and non-hierarchical search space for NAS'},\n",
       "  {'paperId': 'e04c301289314c40f19a5644f3f407525af9eb35',\n",
       "   'title': 'Themes and sentiments in conversations about food waste on Twitter: Proposal of a framework using neural topic modeling'},\n",
       "  {'paperId': '7c180fbdb954d5cd9b3ee4c9f80a2f1bffdd5d8e',\n",
       "   'title': 'Which is better? Taxonomy induction with learning the optimal structure via contrastive learning'},\n",
       "  {'paperId': '1b4754e805340e6e72e3a5712507b81a74becabc',\n",
       "   'title': 'A semi-supervised framework fusing multiple information for knowledge graph entity alignment'},\n",
       "  {'paperId': 'ddbe5a32350a090811ee2cdd9c980c055695d6f2',\n",
       "   'title': 'Software bug prediction using graph neural networks and graph-based text representations'},\n",
       "  {'paperId': 'f1139cd468d83e7a11f6b3872edb0030d03b9ddc',\n",
       "   'title': 'Fuzzy Multimodal Graph Reasoning for Human-Centric Instructional Video Grounding'},\n",
       "  {'paperId': '99f2312049a095b1c6527098b26d67bebd954e54',\n",
       "   'title': 'Fine-grained vulnerability detection for medical sensor systems'},\n",
       "  {'paperId': 'd3835772063b8a59ba03cb4a50c17540a87fb15e',\n",
       "   'title': 'TrajWeaver: Trajectory Recovery with State Propagation Diffusion Model'},\n",
       "  {'paperId': '408e510a97ecd258405cb2229d53ad154742d404',\n",
       "   'title': 'Deep Knowledge-Infusion For Explainable Depression Detection'},\n",
       "  {'paperId': '67a83103aa01a343f040fccfa48fd52e4c36494a',\n",
       "   'title': 'Advancing state of health estimation for electric vehicles: Transformer-based approach leveraging real-world data'},\n",
       "  {'paperId': '0c607d4b88c1fd094a90c7fc0d379087141a8ba5',\n",
       "   'title': 'Graph-based bootstrapped latent recommendation model'},\n",
       "  {'paperId': 'fa0af8d1ab092ccf4c961f4965684cc9d7a379b0',\n",
       "   'title': 'AWDepth: Monocular Depth Estimation for Adverse Weather via Masked Encoding'},\n",
       "  {'paperId': 'e9b9200ef71a1296ed178e0ac607505ec9bc407c',\n",
       "   'title': 'Leverage NLP Models Against Other NLP Models: Two Invisible Feature Space Backdoor Attacks'},\n",
       "  {'paperId': '3d749b9c8e4aee903b389b6f309d78bbb2bd9946',\n",
       "   'title': 'Container Rehandling Probability Prediction Model Based on Seq2Seq Network'},\n",
       "  {'paperId': '944e25cccc347525fb3bfbe55fcf14f4812d89c2',\n",
       "   'title': 'Detecting Android malware: A multimodal fusion method with fine-grained feature'},\n",
       "  {'paperId': 'b48d194d218aeaff474eccd70ca5243d3f54f187',\n",
       "   'title': 'Zero-shot sim-to-real transfer using Siamese-Q-Based reinforcement learning'},\n",
       "  {'paperId': '3b01fb6c23932ecd07ab8ef0f68dbbf13e7af78b',\n",
       "   'title': 'RI-MAE: Rotation-Invariant Masked AutoEncoders for Self-Supervised Point Cloud Representation Learning'},\n",
       "  {'paperId': '68eb348cc0ac8628b987b792d688786257f70978',\n",
       "   'title': 'Multi-label Zero-Shot Audio Classification with Temporal Attention'},\n",
       "  {'paperId': '188bdd2edb3126b357dc646f2ff1f0b2699ad407',\n",
       "   'title': 'LongRecipe: Recipe for Efficient Long Context Generalization in Large Language Models'},\n",
       "  {'paperId': '4807ff55cea1db3bd79556b8870870bb2b3def51',\n",
       "   'title': 'The MERIT Dataset: Modelling and Efficiently Rendering Interpretable Transcripts'},\n",
       "  {'paperId': '39022bf884b473f8d08984921fa96f8f99ee7eca',\n",
       "   'title': 'Data Augmentation for Image Classification using Generative AI'},\n",
       "  {'paperId': '697bc170a569566d7c5669a9d10ad75b7f434741',\n",
       "   'title': 'ActionPose: Pretraining 3D Human Pose Estimation with the Dark Knowledge of Action'},\n",
       "  {'paperId': '5804cec0c25c04460a72332859261f6988dbf709',\n",
       "   'title': 'Post-OCR Text Correction for Bulgarian Historical Documents'},\n",
       "  {'paperId': '05e79b99fe14413be8b04f31e0ac73c73f958d5a',\n",
       "   'title': 'DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer Interaction Module'},\n",
       "  {'paperId': '16623976c1ac0c66f212ccd2055c27614f1da138',\n",
       "   'title': 'COSMo: CLIP Talks on Open-Set Multi-Target Domain Adaptation'},\n",
       "  {'paperId': '8b70bc1e2c75df3dc9f9c07b1fbeb6a9d91291f0',\n",
       "   'title': 'A Hybrid Transformer-Mamba Network for Single Image Deraining'},\n",
       "  {'paperId': 'c6c945624a274263a5bfd6a77f1408313dd939c7',\n",
       "   'title': 'Incremental Open-set Domain Adaptation'},\n",
       "  {'paperId': 'a4fe48146630df3a37c3b9437cc6f551db4aa909',\n",
       "   'title': 'Progressive Residual Extraction based Pre-training for Speech Representation Learning'},\n",
       "  {'paperId': '4cf11ff1ed5898dcc639843227938cbeef4f8547',\n",
       "   'title': 'Simbanex: Similarity-based Exploration of IEEE VIS Publications'},\n",
       "  {'paperId': 'd85d4d270aba6853373d570fdb303f1255e7a675',\n",
       "   'title': 'From Prediction to Application: Language Model-based Code Knowledge Tracing with Domain Adaptive Pre-Training and Automatic Feedback System with Pedagogical Prompting for Comprehensive Programming Education'},\n",
       "  {'paperId': 'd0ae1637238ffe1a48efd39d462fa7a57ebc591d',\n",
       "   'title': 'AdaNAT: Exploring Adaptive Policy for Token-Based Image Generation'},\n",
       "  {'paperId': '92dd620f9cff6ba4b6848632e1a5a3550fe7d156',\n",
       "   'title': 'Emerging Trends in Artificial Intelligence (AI) and Machine Learning (ML)'},\n",
       "  {'paperId': '572f8742e63a4d4d237b97650490d7acb312cf1a',\n",
       "   'title': 'LSD600: the first corpus of biomedical abstracts annotated with lifestyle-disease relations'},\n",
       "  {'paperId': '248794acd8854478ccb619ab0aadaff9f4124e0e',\n",
       "   'title': 'Automatic extraction of transcriptional regulatory interactions of bacteria from biomedical literature using a BERT-based approach'},\n",
       "  {'paperId': 'c120f045da7afa8ecfba59823163f13132c0f79a',\n",
       "   'title': 'Benchmarking the Performance of Large Language Models on the Cerebras Wafer Scale Engine'},\n",
       "  {'paperId': '6a6a8ba206faad8503c32dcf34ca912e2dfd9a11',\n",
       "   'title': 'Sentiment analysis and social media analytics in brand management: Techniques, trends, and implications'},\n",
       "  {'paperId': '2eaede75eb37c1d718bb360914223270b32463aa',\n",
       "   'title': 'Acupuncture indication knowledge bases: meridian entity recognition and classification based on ACUBERT'},\n",
       "  {'paperId': '2f21cfb3fca1378cab64b5ee39a687c99fd317a3',\n",
       "   'title': 'CD-SEM Image Defect Detection and Classification Using Transformers'},\n",
       "  {'paperId': '9ea7f4b7319fbc783e75ac60edfd62989ef88720',\n",
       "   'title': 'Improving Extraction of Clinical Event Contextual Properties from Electronic Health Records: A Comparative Study'},\n",
       "  {'paperId': '3298365dfbc1772f8e51a0420013906cd14a1a5b',\n",
       "   'title': 'Joint Estimation and Prediction of City-wide Delivery Demand: A Large Language Model Empowered Graph-based Learning Approach'},\n",
       "  {'paperId': '1791ac174506801119e54cce779dc5ce33647543',\n",
       "   'title': 'Estimating event-by-event multiplicity by a Machine Learning Method for Hadronization Studies'},\n",
       "  {'paperId': '74a941cc9aa359215047ef698d50b6b6625b3029',\n",
       "   'title': 'InkubaLM: A small language model for low-resource African languages'},\n",
       "  {'paperId': '65d1cc31fccb0b2818c8105ad487d9696fa01129',\n",
       "   'title': 'NanoMVG: USV-Centric Low-Power Multi-Task Visual Grounding based on Prompt-Guided Camera and 4D mmWave Radar'},\n",
       "  {'paperId': '1cf93d90301d02ac5e9e2c4b5e5547b4dbf2bc01',\n",
       "   'title': 'CLOCR-C: Context Leveraging OCR Correction with Pre-trained Language Models'},\n",
       "  {'paperId': '6d9c5cbee7b3388895b47eb0846c29460a9aad7a',\n",
       "   'title': 'Image-Perfect Imperfections: Safety, Bias, and Authenticity in the Shadow of Text-To-Image Model Evolution'},\n",
       "  {'paperId': '51b20b83aa362b78163f19a6d2afb106f7f5194f',\n",
       "   'title': 'Assessing Generative Language Models in Classification Tasks: Performance and Self-Evaluation Capabilities in the Environmental and Climate Change Domain'},\n",
       "  {'paperId': '5101d0316e57b66ffd658eb4e5794fa157c2bc18',\n",
       "   'title': 'LSMS: Language-guided Scale-aware MedSegmentor for Medical Image Referring Segmentation'},\n",
       "  {'paperId': '8e9063c07fa2dfb4ad8ea2ba8095eac06920b666',\n",
       "   'title': 'Understanding the User: An Intent-Based Ranking Dataset'},\n",
       "  {'paperId': 'b39101c85dd748d904f8b79420bdb6c7f5503269',\n",
       "   'title': 'Towards Symbolic XAI -- Explanation Through Human Understandable Logical Relationships Between Features'},\n",
       "  {'paperId': '7686f95ce66334da066f0c94a75f64f0d915dcd4',\n",
       "   'title': 'Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation'},\n",
       "  {'paperId': '92792b43e844320be1a0a2013bad66ffbe21ead0',\n",
       "   'title': 'rerankers: A Lightweight Python Library to Unify Ranking Methods'},\n",
       "  {'paperId': 'a35e97dd7a753e13d317d5d275acc84a1889cbfe',\n",
       "   'title': 'Medical Report Generation Is A Multi-label Classification Problem'},\n",
       "  {'paperId': 'a10cca6800a1a8f280496b8e2b1792e22ecad722',\n",
       "   'title': 'Can Large Language Models Address Open-Target Stance Detection?'},\n",
       "  {'paperId': 'c6b7a0e53ce0b8171172f142947b4bb18123c896',\n",
       "   'title': 'OnlySportsLM: Optimizing Sports-Domain Language Models with SOTA Performance under Billion Parameters'},\n",
       "  {'paperId': '9ae82c6cac594a1c18f5d0837468708d015264fc',\n",
       "   'title': 'Developing an End-to-End Framework for Predicting the Social Communication Severity Scores of Children with Autism Spectrum Disorder'},\n",
       "  {'paperId': '37073bce9ae100f4c56cfca8c5bafd82f313f33f',\n",
       "   'title': 'A Novel Approach to Automatically Digitize Analog Seismograms'},\n",
       "  {'paperId': '43c70b8646088036115027528f075e9384d4789c',\n",
       "   'title': 'MethylBERT: A Transformer-based model for read-level DNA methylation pattern identification and tumour deconvolution'},\n",
       "  {'paperId': 'c43cc65148bf7221ff035c75621b663371fe8250',\n",
       "   'title': 'Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever'},\n",
       "  {'paperId': '45cf90e88cce1daf05313a191a4160ad6a5ee623',\n",
       "   'title': 'Integration of multi-level semantics in PTMs with an attention model for question matching'},\n",
       "  {'paperId': '059375bfb6f6247c2036157dbb83c59f67adee43',\n",
       "   'title': 'HLogformer: A Hierarchical Transformer for Representing Log Data'},\n",
       "  {'paperId': '5324f002a90eeb60af9121fe7aee9357c0920467',\n",
       "   'title': 'ResVG: Enhancing Relation and Semantic Understanding in Multiple Instances for Visual Grounding'},\n",
       "  {'paperId': '13403582a217fc873db5153f455501a5420c2041',\n",
       "   'title': 'Chain-of-Experts (CoE): Reverse Engineering Software Bills of Materials for JavaScript Application Bundles through Code Clone Search'},\n",
       "  {'paperId': 'c1b64296d8b6b9b82a295467e0bebb272e5fd421',\n",
       "   'title': 'Audio xLSTMs: Learning Self-Supervised Audio Representations with xLSTMs'},\n",
       "  {'paperId': '8de5c21995119b0b2e0395c7afd1da619c208b4d',\n",
       "   'title': 'From cart to truck: meaning shift through words in English in the last two centuries'},\n",
       "  {'paperId': '03497079f6ecfe050a2427963b0b78325f09a64f',\n",
       "   'title': 'Space3D-Bench: Spatial 3D Question Answering Benchmark'},\n",
       "  {'paperId': '7d48cdc0bcdb110011676745deb4aea684ac9f00',\n",
       "   'title': 'Maelstrom Networks'},\n",
       "  {'paperId': '7966c7ff8ac2f418bed660acf8851b7e08e49ef2',\n",
       "   'title': 'Benchmarking Japanese Speech Recognition on ASR-LLM Setups with Multi-Pass Augmented Generative Error Correction'},\n",
       "  {'paperId': 'e2b3f4522885dc461efd7aa05c73303d69d80ae6',\n",
       "   'title': 'Adapting Vision-Language Models to Open Classes via Test-Time Prompt Tuning'},\n",
       "  {'paperId': '0873064d96640da4a7353961fe7bc51f1eb52a96',\n",
       "   'title': 'Large-Scale Multi-omic Biosequence Transformers for Modeling Peptide-Nucleotide Interactions'},\n",
       "  {'paperId': '1812fba0f0c1cefbe04668a7b8dc3c41f29692d2',\n",
       "   'title': \"Sparse and Expandable Network for Google's Pathways\"},\n",
       "  {'paperId': 'dd2d40ce5e6fe8489116fc8ae035597b4d4dc8b0',\n",
       "   'title': 'BERT4Cache: a bidirectional encoder representations for data prefetching in cache'},\n",
       "  {'paperId': '873c0bfb67e1291e6820b2257af8413d70531bdf',\n",
       "   'title': 'The co-evolution of AI technology and information environment: Diagnosing social impacts and exploring governance strategies'},\n",
       "  {'paperId': 'd84ced80a5fdda811d867ab9e60dfb65814f515f',\n",
       "   'title': 'A longitudinal sentiment analysis of Sinophobia during COVID-19 using large language models'},\n",
       "  {'paperId': '0aa66b31bffd0eb1f0dfec4d197fc3cfef439d6d',\n",
       "   'title': 'LLaVA-Chef: A Multi-modal Generative Model for Food Recipes'},\n",
       "  {'paperId': '3c735871d0eac21055be53fcf5554c330d015d37',\n",
       "   'title': 'Modeling offensive content detection for TikTok'},\n",
       "  {'paperId': '2328293b9a9da591f820b2f2b4a5551c3d44bbc2',\n",
       "   'title': 'A Survey for Large Language Models in Biomedicine'},\n",
       "  {'paperId': '40aa5bc655f59f34c0b1b4c5ca5c3ddb3d7cab6e',\n",
       "   'title': 'Tiny-Toxic-Detector: A compact transformer-based model for toxic content detection'},\n",
       "  {'paperId': '5a6de79f9e831900b52e8e94310e6bedfac79fdb',\n",
       "   'title': 'Event Extraction for Portuguese: A QA-Driven Approach Using ACE-2005'},\n",
       "  {'paperId': '20038b50f93623bd495c0078a21e21444d0d2d85',\n",
       "   'title': 'TempoFormer: A Transformer for Temporally-aware Representations in Change Detection'},\n",
       "  {'paperId': 'ba8b21f9cf5210a00cb096c8b3bf62da319bcdc5',\n",
       "   'title': 'Pixels to Prose: Understanding the art of Image Captioning'},\n",
       "  {'paperId': 'c96c75c7617b133fb85ec21d460afa6d59b3b590',\n",
       "   'title': 'EMP: Enhance Memory in Data Pruning'},\n",
       "  {'paperId': '20a6a16bb2c65421752c560ae71a830fa15bbce6',\n",
       "   'title': 'Meta-Learn Unimodal Signals with Weak Supervision for Multimodal Sentiment Analysis'},\n",
       "  {'paperId': 'fc8d040f92a2715a0cee106dc1dea8f23069d2fa',\n",
       "   'title': 'Evaluating Named Entity Recognition Using Few-Shot Prompting with Large Language Models'},\n",
       "  {'paperId': '3d39378736a89db8d3de2015b4de53a6cd6c5243',\n",
       "   'title': 'Measuring the Reliability of Causal Probing Methods: Tradeoffs, Limitations, and the Plight of Nullifying Interventions'},\n",
       "  {'paperId': '69b62d57b408044ac5280f30475d481a4fcd9d7a',\n",
       "   'title': 'A Simple Baseline with Single-encoder for Referring Image Segmentation'},\n",
       "  {'paperId': '9f49c12db18274de15058e2b784b8b69105a8b5f',\n",
       "   'title': 'Automatic Differential Diagnosis using Transformer-Based Multi-Label Sequence Classification'},\n",
       "  {'paperId': '54fee367d32ecceae4fb328aaac707761290a915',\n",
       "   'title': \"Predicting Parameter Change's Effect on Cellular Network Time Series\"},\n",
       "  {'paperId': '5eff7c50562a8c58e1a0be1ed2212b798fb4f261',\n",
       "   'title': 'Autoregressive model path dependence near Ising criticality'},\n",
       "  {'paperId': '5a15172e406f7b67a2a17e604ca00176a14a79f4',\n",
       "   'title': 'Certified Causal Defense with Generalizable Robustness'},\n",
       "  {'paperId': '9191ba72195b2074dcf2f900d607f2390ff5bcf6',\n",
       "   'title': 'Conan-embedding: General Text Embedding with More and Better Negative Samples'},\n",
       "  {'paperId': '6a241a6cb8ec05ca89912a99aa3505bdfc846379',\n",
       "   'title': 'Exploring Selective Layer Fine-Tuning in Federated Learning'},\n",
       "  {'paperId': '0f5c3b742084a78523ec5aa74843db224149fa80',\n",
       "   'title': 'Modeling and Analyzing the Influence of Non-Item Pages on Sequential Next-Item Prediction'},\n",
       "  {'paperId': '1bc7aa550043bbbef72101693a4ab576c06689d6',\n",
       "   'title': 'wav2pos: Sound Source Localization using Masked Autoencoders'},\n",
       "  {'paperId': 'fef46de4954888b1057831e12521bcefc4d3a91e',\n",
       "   'title': 'Can SAR improve RSVQA performance?'},\n",
       "  {'paperId': '18c12f5cc1ed5b67877e8b80a8287301f0c931f5',\n",
       "   'title': 'LM-PUB-QUIZ: A Comprehensive Framework for Zero-Shot Evaluation of Relational Knowledge in Language Models'},\n",
       "  {'paperId': 'a981891df555417f206594c67f32e044a0b6c54e',\n",
       "   'title': 'TeFF: Tracking-enhanced Forgetting-free Few-shot 3D LiDAR Semantic Segmentation'},\n",
       "  {'paperId': 'ba5563821f237eefe6c05d6f6ca97b61fab019ae',\n",
       "   'title': 'SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding'},\n",
       "  {'paperId': '849880a4135a5f1137fb6ba08d869fb44a85dce8',\n",
       "   'title': '{\\\\mu}gat: Improving Single-Page Document Parsing by Providing Multi-Page Context'},\n",
       "  {'paperId': '2c6f6a0f3f4aef2db63796bbfc38010eb7aa96b0',\n",
       "   'title': 'Is Personality Prediction Possible Based on Reddit Comments?'},\n",
       "  {'paperId': '633e1f1b6f34b33504e5aad3934116b55136223e',\n",
       "   'title': 'An Extremely Data-efficient and Generative LLM-based Reinforcement Learning Agent for Recommenders'},\n",
       "  {'paperId': 'baf221479b0163a02cfaa503ef3caec277c9df7d',\n",
       "   'title': 'Recognition and normalization of multilingual symptom entities using in-domain-adapted BERT models and classification layers'},\n",
       "  {'paperId': '5c0e06228b288398e33bcc1028d861860be2d2d2',\n",
       "   'title': 'Multilingual question answering systems for knowledge graphs\\xa0– a survey'},\n",
       "  {'paperId': 'ed24c8dbb47fcc5fb72053d81d51778439e7df30',\n",
       "   'title': 'FedMCP: Parameter-Efficient Federated Learning with Model-Contrastive Personalization'},\n",
       "  {'paperId': '24eaabd257006c8bd41bb9f60f535e7546321ca3',\n",
       "   'title': 'ConCSE: Unified Contrastive Learning and Augmentation for Code-Switched Embeddings'},\n",
       "  {'paperId': '3dcd3939fa6374e475213e0480c80bffe24e0d78',\n",
       "   'title': 'GSIFN: A Graph-Structured and Interlaced-Masked Multimodal Transformer Based Fusion Network for Multimodal Sentiment Analysis'},\n",
       "  {'paperId': '570cd544049580503d704baadc097ccd34c1b32f',\n",
       "   'title': 'Cross-Modal Learning for Chemistry Property Prediction: Large Language Models Meet Graph Machine Learning'},\n",
       "  {'paperId': 'ea13c994dd563c88f011b064ddf5e3cac12a2c61',\n",
       "   'title': 'Medical Information Extraction With NLP-Powered QABots: A Real-World Scenario.'},\n",
       "  {'paperId': '6052bc4a08cdd0a2847c8edffa8a1b2c57fe05d8',\n",
       "   'title': 'Classifying populist language in American presidential and governor speeches using automatic text analysis'},\n",
       "  {'paperId': '11fba6ef7b84ab7aed72cbe49a13f870c81a2678',\n",
       "   'title': 'How transformers learn structured data: insights from hierarchical filtering'},\n",
       "  {'paperId': '38118a66d0ddc9954bd07e69277b8e0d2a5fc978',\n",
       "   'title': 'A Preliminary Exploration Towards General Image Restoration'},\n",
       "  {'paperId': 'd111795c38a9ce45fbc767abae141206ac583f23',\n",
       "   'title': 'Brain-inspired Artificial Intelligence: A Comprehensive Review'},\n",
       "  {'paperId': '960ccf57b1e90128103817cff7b3de23c71f4e08',\n",
       "   'title': 'The VoxCeleb Speaker Recognition Challenge: A Retrospective'},\n",
       "  {'paperId': '6f14bceef03678f2667c8fe030ba82c86531e8ae',\n",
       "   'title': 'MRSE: An Efficient Multi-modality Retrieval System for Large Scale E-commerce'},\n",
       "  {'paperId': 'a0c221b9ff61fb238ff4271d3015f090f617c000',\n",
       "   'title': 'Pre-training Everywhere: Parameter-Efficient Fine-Tuning for Medical Image Analysis via Target Parameter Pre-training'},\n",
       "  {'paperId': '4105887daa665790bbb3cfc7f7b32ec0019df250',\n",
       "   'title': 'The Benefits of Balance: From Information Projections to Variance Reduction'},\n",
       "  {'paperId': 'f06fd77e7ba1767af23c579107f364230b0e1704',\n",
       "   'title': 'StyleSpeech: Parameter-efficient Fine Tuning for Pre-trained Controllable Text-to-Speech'},\n",
       "  {'paperId': '34654b72c9c248f23367a80eecd7023dd2fa5e95',\n",
       "   'title': 'CVPT: Cross-Attention help Visual Prompt Tuning adapt visual task'},\n",
       "  {'paperId': '3baa8a4ecbaff7cc0ac2342d4b570692d28a1322',\n",
       "   'title': 'Impact of Noisy Labels on Sound Event Detection: Deletion Errors Are More Detrimental Than Insertion Errors'},\n",
       "  {'paperId': '796453db982b6b47cb0a1823290befae73bf7d37',\n",
       "   'title': 'Graph and Sequential Neural Networks in Session-based Recommendation: A Survey'},\n",
       "  {'paperId': 'da99c68367f3892231e219abd01f46a4210a9a56',\n",
       "   'title': 'Unsupervised-to-Online Reinforcement Learning'},\n",
       "  {'paperId': '735b8df5ee3c2cd1ed19517832d201279aa78744',\n",
       "   'title': 'DocLayLLM: An Efficient and Effective Multi-modal Extension of Large Language Models for Text-rich Document Understanding'},\n",
       "  {'paperId': '64b34d20d217a2db702716d924396fef555abddc',\n",
       "   'title': 'A Survey of Large Language Models for European Languages'},\n",
       "  {'paperId': '08855772b070e140900a9b727d6ca629b2fa1414',\n",
       "   'title': 'SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking State Space Models'},\n",
       "  {'paperId': '4389aa099bf1e3e610ee88973e0ec9a16d2a11ba',\n",
       "   'title': 'Can Transformers Do Enumerative Geometry?'},\n",
       "  {'paperId': 'ee2551d1b929a441388d30b9ed10a1fc8ee938b3',\n",
       "   'title': 'DualKanbaFormer: Kolmogorov-Arnold Networks and State Space Model Transformer for Multimodal Aspect-based Sentiment Analysis'},\n",
       "  {'paperId': 'bc3db30cf1b6dbaffa27bbd1d5136e32c1099707',\n",
       "   'title': 'Fine-grained length controllable video captioning with ordinal embeddings'},\n",
       "  {'paperId': '272c29946cae59281889aa0c1d2e2fd11cfcfe2c',\n",
       "   'title': 'The Uniqueness of LLaMA3-70B with Per-Channel Quantization: An Empirical Study'},\n",
       "  {'paperId': '6621d751ca8af98fb85de639a2b2aac9913be7fc',\n",
       "   'title': 'Hate speech detection in low-resourced Indian languages: An analysis of transformer-based monolingual and multilingual models with cross-lingual experiments'},\n",
       "  {'paperId': 'f7b8985cb76660d7274ca7d1fa3712aee336b92a',\n",
       "   'title': 'Traditional Machine Learning, Deep Learning, and BERT (Large Language Model) Approaches for Predicting Hospitalizations From Nurse Triage Notes: Comparative Evaluation of Resource Management.'},\n",
       "  {'paperId': '1075a9b84e62cb333a18a93fe4750d2a7cdfb190',\n",
       "   'title': 'Leveraging large language models through natural language processing to provide interpretable machine learning predictions of mental deterioration in real time'},\n",
       "  {'paperId': '961f245df3ff7f6b3c84ea053376f0d9a7d0aab0',\n",
       "   'title': 'Probing Causality Manipulation of Large Language Models'},\n",
       "  {'paperId': 'afdc9b9c86f06db2b10816fac916e9f72d7b04ff',\n",
       "   'title': 'What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation'},\n",
       "  {'paperId': '7b7d4c4a4bd260fd57c7164c659b2149be2883db',\n",
       "   'title': 'BiRNA-BERT allows efficient RNA language modeling with adaptive tokenization'},\n",
       "  {'paperId': '1a0da96780c6863b1a5ccbc4abfc74d7295d44e7',\n",
       "   'title': 'Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models'},\n",
       "  {'paperId': '362d457ba2f854b9fdaa8cc50e02e50bdb227a96',\n",
       "   'title': 'Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning'},\n",
       "  {'paperId': 'c15882f26916814b48230eedef314db8ba943819',\n",
       "   'title': 'Are LLM-based Recommenders Already the Best? Simple Scaled Cross-entropy Unleashes the Potential of Traditional Sequential Recommenders'},\n",
       "  {'paperId': '614d89e5c8839291270eb8ff0ed2e9bbadf49957',\n",
       "   'title': 'An Embedding is Worth a Thousand Noisy Labels'},\n",
       "  {'paperId': '5a1659a064484ae2434d9c4e1acd6a5a55931f1e',\n",
       "   'title': 'Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning'},\n",
       "  {'paperId': '49a9aafa91c1497e35996222c47013367bb5e6f0',\n",
       "   'title': 'Enhancing Fairness through Reweighting: A Path to Attain the Sufficiency Rule'},\n",
       "  {'paperId': '7404210a5c226034e7ea052cffd6237023cb76f3',\n",
       "   'title': 'Reprogramming Foundational Large Language Models(LLMs) for Enterprise Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in Copilot-Guided Cross-Modal Time Series Representation Learning'},\n",
       "  {'paperId': 'de630124d259fec0133263bd51124a72a5f3b4ed',\n",
       "   'title': 'Sifting through the Chaff: On Utilizing Execution Feedback for Ranking the Generated Code Candidates'},\n",
       "  {'paperId': '68091aa4eb1424d8479e845de5d1d4ea00050957',\n",
       "   'title': 'Exploring the Potential of Synthetic Data to Replace Real Data'},\n",
       "  {'paperId': 'f5ff48062127ef0c7f9e3c1f5e85ed796386b8a0',\n",
       "   'title': 'Can Optimization Trajectories Explain Multi-Task Transfer?'},\n",
       "  {'paperId': '03c00f41b7d9c05d5171db3bd12e169e5d424cf3',\n",
       "   'title': 'Artificial Intelligence in Landscape Architecture: A Survey'},\n",
       "  {'paperId': '0cfeaa745a07a3494c7e3faa4e24c7071e50c177',\n",
       "   'title': 'Retrieval Augmented Generation for Dynamic Graph Modeling'},\n",
       "  {'paperId': 'c950833b4306f78eddcb651dd17d9b9670b77a2f',\n",
       "   'title': 'Towards Graph Prompt Learning: A Survey and Beyond'},\n",
       "  {'paperId': 'f405e6e65fd9fa9e638bb413bfe82579679a9b08',\n",
       "   'title': 'A Multilateral Attention-enhanced Deep Neural Network for Disease Outbreak Forecasting: A Case Study on COVID-19'},\n",
       "  {'paperId': '8abda77d66650944d428a34a334c4c14f55b2ffa',\n",
       "   'title': 'BIJAKAWEB: Platform Berbasis Web Untuk Deteksi Hate Speech Pada Komentar Berita Bahasa Indonesia'},\n",
       "  {'paperId': 'b233f7d5ebe9c15f39236bcde62faa406c27d506',\n",
       "   'title': 'Watermarking Techniques for Large Language Models: A Survey'},\n",
       "  {'paperId': 'e02b587f9ec9988f720781db9b1f90ab99b8f57d',\n",
       "   'title': 'Examining Independence in Ensemble Sentiment Analysis: A Study on the Limits of Large Language Models Using the Condorcet Jury Theorem'},\n",
       "  {'paperId': '599766bed4de4eba24e6a24008aa43d93d3c6405',\n",
       "   'title': 'Reducing the Cost: Cross-Prompt Pre-finetuning for Short Answer Scoring'},\n",
       "  {'paperId': '1467a5ccfe73b20947a271b9a9be8044755a377d',\n",
       "   'title': 'LowCLIP: Adapting the CLIP Model Architecture for Low-Resource Languages in Multimodal Image Retrieval Task'},\n",
       "  {'paperId': '9f261b42a286313561f12afac0ee683cea9cf0d4',\n",
       "   'title': 'Bidirectional Awareness Induction in Autoregressive Seq2Seq Models'},\n",
       "  {'paperId': '3ba86f9723c7f1185c05a38231b4a2d3765e6a3d',\n",
       "   'title': 'LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings'},\n",
       "  {'paperId': '28719530ac4f9a500117388936eda0e316dad1c9',\n",
       "   'title': 'Knowledge-Aware Conversation Derailment Forecasting Using Graph Convolutional Networks'},\n",
       "  {'paperId': '7706f583e5b713be163cc631ff700e88594fe0ac',\n",
       "   'title': 'DOPPLER: Differentially Private Optimizers with Low-pass Filter for Privacy Noise Reduction'},\n",
       "  {'paperId': '5446d934709b8fb0fa592b2e47af8ea8651e4289',\n",
       "   'title': \"ColBERT's [MASK]-based Query Augmentation: Effects of Quadrupling the Query Input Length\"},\n",
       "  {'paperId': '3e320c32ead44294e6ff96bcf35a75eaa9c5c407',\n",
       "   'title': 'Hierarchical Network Fusion for Multi-Modal Electron Micrograph Representation Learning with Foundational Large Language Models'},\n",
       "  {'paperId': 'c4b10288b38d82dd92367396362eaf44878510e1',\n",
       "   'title': 'Integrating Multi-Head Convolutional Encoders with Cross-Attention for Improved SPARQL Query Translation'},\n",
       "  {'paperId': 'f2c5528b2690dc5d50e5be760a9082cb887d878b',\n",
       "   'title': 'Advancing Enterprise Spatio-Temporal Forecasting Applications: Data Mining Meets Instruction Tuning of Language Models For Multi-modal Time Series Analysis in Low-Resource Settings'},\n",
       "  {'paperId': '125bbc217e3e95ab720b5f81a64761b39979c825',\n",
       "   'title': 'Probing the Robustness of Vision-Language Pretrained Models: A Multimodal Adversarial Attack Approach'},\n",
       "  {'paperId': 'cd92ad06412f1310b6a8bf08c9076509473cb9dd',\n",
       "   'title': 'Preliminary Investigations of a Multi-Faceted Robust and Synergistic Approach in Semiconductor Electron Micrograph Analysis: Integrating Vision Transformers with Large Language and Multimodal Models'},\n",
       "  {'paperId': '8c0d181858a631aa252dee35637304f0b7663d1a',\n",
       "   'title': 'Disentangled Generative Graph Representation Learning'},\n",
       "  {'paperId': 'a80c872d58f9e560bfe09fb8403ead1d6818d5bc',\n",
       "   'title': 'Utilizing Large Language Models for Named Entity Recognition in Traditional Chinese Medicine against COVID-19 Literature: Comparative Study'},\n",
       "  {'paperId': '5ae4111a357d89aa354979652d069ddab0955ad3',\n",
       "   'title': 'GNN: Graph Neural Network and Large Language Model for Data Discovery'},\n",
       "  {'paperId': '1c464e0cb0758a4206915332fd9e3cf35ee770d8',\n",
       "   'title': 'A Law of Next-Token Prediction in Large Language Models'},\n",
       "  {'paperId': '292233bff38fe6e8336ce55619ef5513dc63b359',\n",
       "   'title': 'Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic'},\n",
       "  {'paperId': 'd3a0480e89746ecc470be856e5df58d52888c88d',\n",
       "   'title': 'Cost-Aware Uncertainty Reduction in Schema Matching with GPT-4: The Prompt-Matcher Framework'},\n",
       "  {'paperId': '5f82c765574f40053d2920596690323a2618916c',\n",
       "   'title': 'Symmetric masking strategy enhances the performance of Masked Image Modeling'},\n",
       "  {'paperId': 'fc196e3294bf69ce1d3c1437fc4a53253bb93a34',\n",
       "   'title': 'In-Context Learning with Reinforcement Learning for Incomplete Utterance Rewriting'},\n",
       "  {'paperId': 'c74e1671cee1e19c49a5aafbedfe403af471b8ec',\n",
       "   'title': 'GENA-LM: A Family of Open-Source Foundational DNA Language Models for Long Sequences'},\n",
       "  {'paperId': '270905b8b62e7056ee0f2495bd455ec95879f353',\n",
       "   'title': 'MedDec: A Dataset for Extracting Medical Decisions from Discharge Summaries'},\n",
       "  {'paperId': '64baa1713c63f4c3438f9bbcf220a546cf9f6f18',\n",
       "   'title': 'Development of an Internet-based Product-related Child Injury Textual Data Platform (IPCITDP) in China'},\n",
       "  {'paperId': 'cfb5697b441a3fce3dc3f24290718bad0118a0b3',\n",
       "   'title': 'A multi-view representation technique based on principal component analysis for enhanced short text clustering'},\n",
       "  {'paperId': 'efb9473388b9a816aeb55263e2a7a89f5cb760a6',\n",
       "   'title': 'On Class Separability Pitfalls In Audio-Text Contrastive Zero-Shot Learning'},\n",
       "  {'paperId': 'e0046143bede2d03c360ddf625335180e453d9d1',\n",
       "   'title': 'Domain-specific long text classification from sparse relevant information'},\n",
       "  {'paperId': '9e14045c0229a182fea2dc6e9c5bedc52fdfe41c',\n",
       "   'title': 'NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks'},\n",
       "  {'paperId': '0bf8644294c43b7684122a33a4af4f409c9a7899',\n",
       "   'title': 'Structural Representation Learning and Disentanglement for Evidential Chinese Patent Approval Prediction'},\n",
       "  {'paperId': '395a2cde951a65ce343dca3cd7726fb5ecd433ab',\n",
       "   'title': 'Multimodal Contrastive In-Context Learning'},\n",
       "  {'paperId': '355c398a96715714021e436af01d2fdb2c584e66',\n",
       "   'title': 'Moral Judgments in Online Discourse are not Biased by Gender'},\n",
       "  {'paperId': '19db315279fda840e6e2614f52f3c9b0e7b7b0a6',\n",
       "   'title': 'IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities'},\n",
       "  {'paperId': '04daf8046d036a5d92c52c0fa890f763ee38662e',\n",
       "   'title': 'Image Segmentation in Foundation Model Era: A Survey'},\n",
       "  {'paperId': 'b4bfabb8e1d1a5176369b94d47ba74cc9a1d646c',\n",
       "   'title': 'VFM-Det: Towards High-Performance Vehicle Detection via Large Foundation Models'},\n",
       "  {'paperId': 'a00a142b6393b9f83383fb9831992c88946b9128',\n",
       "   'title': 'CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations of Research Papers'},\n",
       "  {'paperId': '1c56edb4bff9801cd551a36529de9331f867d784',\n",
       "   'title': 'A New Era in Computational Pathology: A Survey on Foundation and Vision-Language Models'},\n",
       "  {'paperId': '5d86ca74e45e62b314e304c181a6a1b774efcc48',\n",
       "   'title': 'A novel method for the classification of 3D point clouds based on the improved PointNet++'},\n",
       "  {'paperId': '522f918bbe7b818596a5f533e1ac74d2f7718d05',\n",
       "   'title': 'An alternative formulation of attention pooling function in translation'},\n",
       "  {'paperId': 'fdaea66532b39f8f68bd87872000c2750e3e8ae4',\n",
       "   'title': 'NLP and Machine Learning for Sentiment Analysis in COVID-19 Tweets: A Comparative Study'},\n",
       "  {'paperId': '58c087bfc3d8b3906b3ea4ae21034f8bf94a9796',\n",
       "   'title': 'PHeP: TrustAlert Open-Source Platform for Enhancing Predictive Healthcare with Deep Learning'},\n",
       "  {'paperId': '5317e9edefa667b9ec702a37846154f2689313b2',\n",
       "   'title': \"Leveraging Large Language Models for Identifying Interpretable Linguistic Markers and Enhancing Alzheimer's Disease Diagnostics\"},\n",
       "  {'paperId': '784fc77ed8d4ef5b502d513646528a56e5253869',\n",
       "   'title': 'Visual Verity in AI-Generated Imagery: Computational Metrics and Human-Centric Analysis'},\n",
       "  {'paperId': '9ae638d85dddc017c292d1c15f1d58c5a50bf420',\n",
       "   'title': 'Indiv-Brain: Individualized Brain Network Partition Learned from Individual fMRI Data using Deep Clustering with Vertex-level Attention'},\n",
       "  {'paperId': 'f2182f156aaaa6986b9ab208d872b934b1f0d8ec',\n",
       "   'title': 'Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on Large Language Models'},\n",
       "  {'paperId': '494c8f4c2bdfc7bea842b82598a45053d19427c9',\n",
       "   'title': 'SAM-SP: Self-Prompting Makes SAM Great Again'},\n",
       "  {'paperId': '1cb60f9050c7553fcff65980cdeb228d7d50f4b8',\n",
       "   'title': 'Cell-ontology guided transcriptome foundation model'},\n",
       "  {'paperId': '9e82167db71af6483e16ec80a8bd216262d7cd29',\n",
       "   'title': 'RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual Preference Data'},\n",
       "  {'paperId': '32ebda42a1ae5556b8d67e60c7bd9e84fde45615',\n",
       "   'title': 'Dynamics of Meta-learning Representation in the Teacher-student Scenario'},\n",
       "  {'paperId': 'f9c5813b55ec15fa0b0995e2f6a666a848ba49d8',\n",
       "   'title': \"The Russian-focused embedders' exploration: ruMTEB benchmark and Russian embedding model design\"},\n",
       "  {'paperId': 'b5d54cb2314cef05d9974083ca2ba81e4d54c9a6',\n",
       "   'title': 'Self-supervised Learning for Geospatial AI: A Survey'},\n",
       "  {'paperId': '64c606726616ae4879864d740b59b1e1a50a525b',\n",
       "   'title': 'Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment'},\n",
       "  {'paperId': '19de22fc50c5f8609b2c31854f5e568fa58c6041',\n",
       "   'title': 'Sapiens: Foundation for Human Vision Models'},\n",
       "  {'paperId': 'b3f1f2c4fbda68dc648799ee98f7bd4c26489a7e',\n",
       "   'title': 'Simplified Mamba with Disentangled Dependency Encoding for Long-Term Time Series Forecasting'},\n",
       "  {'paperId': '331c64bc0c4b4b45966814f58c7b85a008aebb8f',\n",
       "   'title': 'Generalized SAM: Efficient Fine-Tuning of SAM for Variable Input Image Sizes'},\n",
       "  {'paperId': '87ef7150d5a372f4b68c42fbb68d4f66da9d3746',\n",
       "   'title': 'Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning with LLMs'},\n",
       "  {'paperId': 'a7132ccd5f2fd001aa146f49723ffc26ea9e0a11',\n",
       "   'title': 'Exploring the Role of Audio in Multimodal Misinformation Detection'},\n",
       "  {'paperId': '69a767f49c91b190a23b48fdcad9d16d0910a850',\n",
       "   'title': 'CLEANANERCorp: Identifying and Correcting Incorrect Labels in the ANERcorp Dataset'},\n",
       "  {'paperId': '22fcc2c4738b80518ed4a3423d318e1299891471',\n",
       "   'title': 'Cross-Domain Foundation Model Adaptation: Pioneering Computer Vision Models for Geophysical Data Analysis'},\n",
       "  {'paperId': '5368c7ff4c27aead903aa8876745562f9c981497',\n",
       "   'title': 'Integrating Audio, Visual, and Semantic Information for Enhanced Multimodal Speaker Diarization'},\n",
       "  {'paperId': '0431696737bf02bf36a7fea3a906b0d46409b118',\n",
       "   'title': 'Exploiting Student Parallelism for Low-latency GPU Inference of BERT-like Models in Online Services'},\n",
       "  {'paperId': '8f242d5c9a10b98ce17fb78bc44d16bfbbf56177',\n",
       "   'title': 'A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation'},\n",
       "  {'paperId': '5a5c018dc5480f9a0c1fd499124d1f86c9d9c1e2',\n",
       "   'title': 'When In-memory Computing Meets Spiking Neural Networks -- A Perspective on Device-Circuit-System-and-Algorithm Co-design'},\n",
       "  {'paperId': 'ceeac71653b0723ab01b34b14396a33ee0d5e385',\n",
       "   'title': \"TReX- Reusing Vision Transformer's Attention for Efficient Xbar-based Computing\"},\n",
       "  {'paperId': '9d6d06ce836780c7e9821aa5386e39322a12f5d2',\n",
       "   'title': 'Bridging the gap between natural user expression with complex automation programming in smart homes'},\n",
       "  {'paperId': '5285febfdc65a830b3c79ce46a66c5a38214651b',\n",
       "   'title': 'Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review'},\n",
       "  {'paperId': '33b6ed46d6948a55eca4ab39f607eec69caff65b',\n",
       "   'title': 'Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language'},\n",
       "  {'paperId': '90fa775750906062af271d2997d1ba2af73abf47',\n",
       "   'title': 'Limitations in Employing Natural Language Supervision for Sensor-Based Human Activity Recognition -- And Ways to Overcome Them'},\n",
       "  {'paperId': 'cafe2d34fb783cf0600051328a74885f7b8103a1',\n",
       "   'title': 'BURExtract-Llama: An LLM for Clinical Concept Extraction in Breast Ultrasound Reports'},\n",
       "  {'paperId': 'c32061c307f08dd68d74b7a7116fbf37a1be637d',\n",
       "   'title': 'Lookism: The overlooked bias in computer vision'},\n",
       "  {'paperId': 'ed2d7a018f1c45c916322f61b41a9bc192b3e113',\n",
       "   'title': 'Vision HgNN: An Electron-Micrograph is Worth Hypergraph of Hypernodes'},\n",
       "  {'paperId': '0073803f3c0d2dfb4a201f9d4f53ae9d61a20550',\n",
       "   'title': 'Large Language Models are Good Attackers: Efficient and Stealthy Textual Backdoor Attacks'},\n",
       "  {'paperId': 'd936373c2a2057e7f895cc70f86c69aaef81f288',\n",
       "   'title': 'Practical token pruning for foundation models in few-shot conversational virtual assistant systems'},\n",
       "  {'paperId': '78245f38a20b24b035086fe13519fd742829189e',\n",
       "   'title': 'Research on the Application of Large Language Models in Automatic Question Generation: A Case Study of ChatGLM in the Context of High School Information Technology Curriculum'},\n",
       "  {'paperId': '0b5a4b76c7d41a9fd0709fb7ce2858bc1f1af601',\n",
       "   'title': 'Story3D-Agent: Exploring 3D Storytelling Visualization with Large Language Models'},\n",
       "  {'paperId': '178a6ad1f7e74441edc0d828e28b7b59255e7e0a',\n",
       "   'title': 'LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification Testsuites'},\n",
       "  {'paperId': 'cc20c53d98034437ac6c63d8e241c057a7a474d1',\n",
       "   'title': 'Stream-K++: Adaptive GPU GEMM Kernel Scheduling and Selection using Bloom Filters'},\n",
       "  {'paperId': 'a02d446e4c302a084c2da95ef40b6f2a4e098b1b',\n",
       "   'title': 'Great Memory, Shallow Reasoning: Limits of $k$NN-LMs'},\n",
       "  {'paperId': 'df2c607e88134578abfe1e093d6373a2938f3b98',\n",
       "   'title': 'Open-Ended 3D Point Cloud Instance Segmentation'},\n",
       "  {'paperId': '89666e80e99c6247768b52613415dfc1d42a3b11',\n",
       "   'title': 'On the Interchangeability of Positional Embeddings in Multilingual Neural Machine Translation Models'},\n",
       "  {'paperId': '90bac3513f3dc7a8b641454d2eb084f3e1d1988a',\n",
       "   'title': 'Differentiating Choices via Commonality for Multiple-Choice Question Answering'},\n",
       "  {'paperId': '7a29abf0a82f316d0ed9787afc90b608fbb6a4de',\n",
       "   'title': 'Clinical Insights: A Comprehensive Review of Language Models in Medicine'},\n",
       "  {'paperId': '6589620d722dc584b463f014c08711899eb1571a',\n",
       "   'title': 'RAG-Optimized Tibetan Tourism LLMs: Enhancing Accuracy and Personalization'},\n",
       "  {'paperId': 'b3388ff8c4cae95088d1e83392e7b1d0ca880847',\n",
       "   'title': 'Large Language Models for Page Stream Segmentation'},\n",
       "  {'paperId': 'd4ce4a28339c38b1d4cadd4ee61932ca31accd5d',\n",
       "   'title': 'Floating-Point Multiply-Add with Approximate Normalization for Low-Cost Matrix Engines'},\n",
       "  {'paperId': '8fe2a4de93e345b89954083d43c481faf5b653b4',\n",
       "   'title': 'Defining Boundaries: The Impact of Domain Specification on Cross-Language and Cross-Domain Transfer in Machine Translation'},\n",
       "  {'paperId': '46b36601f9d25edbd241efbb3f7b3e9eb212bd46',\n",
       "   'title': 'Time Series Foundation Models and Deep Learning Architectures for Earthquake Temporal and Spatial Nowcasting'},\n",
       "  {'paperId': 'd241497fa7f3d3d4fba8834006970e33d6af7cfd',\n",
       "   'title': 'What are the limits of cross-lingual dense passage retrieval for low-resource languages?'},\n",
       "  {'paperId': '913ea3945ce10d39b01fdd6d688dd82834d76f79',\n",
       "   'title': 'The Whole Is Bigger Than the Sum of Its Parts: Modeling Individual Annotators to Capture Emotional Variability'},\n",
       "  {'paperId': 'de3ae87390d8bc2ccf7f785397595e0c03f9c4e6',\n",
       "   'title': 'Characterizing Online Toxicity During the 2022 Mpox Outbreak: A Computational Analysis of Topical and Network Dynamics'},\n",
       "  {'paperId': 'aa9b17aa10765055bd24863820585ea3088fbafc',\n",
       "   'title': 'Estimating Contribution Quality in Online Deliberations Using a Large Language Model'},\n",
       "  {'paperId': '8d967f0e7125dcdcc68da030ca07ed8efe7f1649',\n",
       "   'title': 'VAIV bio-discovery service using transformer model and retrieval augmented generation'},\n",
       "  {'paperId': '62547ab1a12335e242cda0de8b7733e65f8ec162',\n",
       "   'title': 'Dual‐task enhanced global–local temporal–spatial network for depression recognition from facial videos'},\n",
       "  {'paperId': 'd8f413cb08d0db8679ab9c7dbc88a62bf29209f5',\n",
       "   'title': 'The Self-Contained Negation Test Set'},\n",
       "  {'paperId': '4634367c046442a2484d58dc73ff9b813deb7cb7',\n",
       "   'title': \"Crafting Tomorrow's Headlines: Neural News Generation and Detection in English, Turkish, Hungarian, and Persian\"},\n",
       "  {'paperId': 'c1eb5c89b6f7a3289a2933095e8786ed87a8cf4a',\n",
       "   'title': 'Mistral-SPLADE: LLMs for better Learned Sparse Retrieval'},\n",
       "  {'paperId': 'a533f91f835593263702940183f3195c46a8336e',\n",
       "   'title': 'Efficient and Deployable Knowledge Infusion for Open-World Recommendations via Large Language Models'},\n",
       "  {'paperId': '15b9d049822634496361f774d39545bb346ecc65',\n",
       "   'title': 'Inside the Black Box: Detecting Data Leakage in Pre-trained Language Encoders'},\n",
       "  {'paperId': '5d265ba40000676a147caf25cbf7aac8829b5cd7',\n",
       "   'title': 'NoMatterXAI: Generating\"No Matter What\"Alterfactual Examples for Explaining Black-Box Text Classification Models'},\n",
       "  {'paperId': '1cbf29e761ef9a847ef9c70e6cd3c3fd5378d2c3',\n",
       "   'title': 'Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research'},\n",
       "  {'paperId': 'c5b580b60e24a3738fb5d5570ae19110d6c6e203',\n",
       "   'title': 'Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution'},\n",
       "  {'paperId': 'b1a1b8153a01d2e141d6489c3ba476401bd250c0',\n",
       "   'title': 'Adversarial Attack for Explanation Robustness of Rationalization Models'},\n",
       "  {'paperId': '0d65a73389ddf58233f057594d579fdb91fd1236',\n",
       "   'title': 'Synergistic Approach for Simultaneous Optimization of Monolingual, Cross-lingual, and Multilingual Information Retrieval'},\n",
       "  {'paperId': 'c4144d0687cfed7c41ac9359687bc93feec15228',\n",
       "   'title': 'CTP-LLM: Clinical Trial Phase Transition Prediction Using Large Language Models'},\n",
       "  {'paperId': '2ee757dbbb6c21da5bef2d2d5ca51acb74f23f20',\n",
       "   'title': 'Hologram Reasoning for Solving Algebra Problems with Geometry Diagrams'},\n",
       "  {'paperId': 'cc7512cefb89f42175e36b907eaff31ca71c2ba8',\n",
       "   'title': 'Speech Representation Learning Revisited: The Necessity of Separate Learnable Parameters and Robust Data Augmentation'},\n",
       "  {'paperId': '09565702790148a4abd81145e36598504b1f510e',\n",
       "   'title': 'Rethinking Video Segmentation with Masked Video Consistency: Did the Model Learn as Intended?'},\n",
       "  {'paperId': 'b86c2ae627495ff5d08cb0139de353485f585711',\n",
       "   'title': 'DAAD: Dynamic Analysis and Adaptive Discriminator for Fake News Detection'},\n",
       "  {'paperId': '2ec5164d3752e3a1aaf41ecb6c5e28a5b3dc8369',\n",
       "   'title': 'SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition'},\n",
       "  {'paperId': 'c4fcc5d9d67f6e67efcd40644e273d7cd852f053',\n",
       "   'title': 'MEGen: Generative Backdoor in Large Language Models via Model Editing'},\n",
       "  {'paperId': '0a7bf17cc96d1b8e19db2d3766d29247f22d137b',\n",
       "   'title': 'ShapeSplat: A Large-scale Dataset of Gaussian Splats and Their Self-Supervised Pretraining'},\n",
       "  {'paperId': '0ec3cac854ccdb63e6894666a1aff0d931936ee1',\n",
       "   'title': 'Exploiting Large Language Models Capabilities for Question Answer-Driven Knowledge Graph Completion Across Static and Temporal Domains'},\n",
       "  {'paperId': '4e148a693753ecb1cb55cc6458629c9fba0f93dd',\n",
       "   'title': 'Analysis of Plan-based Retrieval for Grounded Text Generation'},\n",
       "  {'paperId': '155f91579593a8a2cd43c581a70f76c4beeeefa1',\n",
       "   'title': 'Multilingual Non-Factoid Question Answering with Silver Answers'},\n",
       "  {'paperId': 'ef5f651db16112d4e01480d412c2915bf416f1cd',\n",
       "   'title': 'CHECKWHY: Causal Fact Verification via Argument Structure'},\n",
       "  {'paperId': '2d2492dfa37ed5e11ad9ed32e9427d365c85fa9c',\n",
       "   'title': 'Automated Detection of Algorithm Debt in Deep Learning Frameworks: An Empirical Study'},\n",
       "  {'paperId': '3d21263d87d280b6cc63c3e07f6d39279e509d67',\n",
       "   'title': 'Large Language Models for Multimodal Deformable Image Registration'},\n",
       "  {'paperId': '8d7310172f657ebe626465a0b39dc350e4606e8e',\n",
       "   'title': 'MambaDS: Near-Surface Meteorological Field Downscaling with Topography Constrained Selective State Space Modeling'},\n",
       "  {'paperId': '95812f29175eb7de27bba01a8ffa68c19ea55e88',\n",
       "   'title': 'LBC: Language-Based-Classifier for Out-Of-Variable Generalization'},\n",
       "  {'paperId': 'f15be8527ba309cebaf3f686ec006636b40ff71d',\n",
       "   'title': 'Multi-Modal Large Language Model Enables Protein Function Prediction'},\n",
       "  {'paperId': 'ef96bfdfad0fb14074a14bc0f1b81d8cebbab6c8',\n",
       "   'title': 'Combining Objective and Subjective Perspectives for Political News Understanding'},\n",
       "  {'paperId': '95608fa3c34a6c547a28852dd963a9c7e13746f6',\n",
       "   'title': 'On the Potential of Open-Vocabulary Models for Object Detection in Unusual Street Scenes'},\n",
       "  {'paperId': 'a638321240d1f518c3117103d2453f6c7d74cdd9',\n",
       "   'title': 'EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation'},\n",
       "  {'paperId': '524da808a711c0b3c63c5eb400097860d6e0dd3a',\n",
       "   'title': 'Do Neural Scaling Laws Exist on Graph Self-Supervised Learning?'},\n",
       "  {'paperId': 'dfbb88e3b3989f06499dd600c1e5594b754d5cd7',\n",
       "   'title': 'Beyond Labels: Aligning Large Language Models with Human-like Reasoning'},\n",
       "  {'paperId': 'ec423285be9770acc11f95e780e1d640972fc6f5',\n",
       "   'title': 'Tesorai Search: Large pretrained model boosts identifications in mass spectrometry proteomics without the need for Percolator'},\n",
       "  {'paperId': 'a5dddad5000f457af176da586f8a0daa266c46c2',\n",
       "   'title': 'Multi-modal deep learning framework for damage detection in social media posts'},\n",
       "  {'paperId': '4a4f2c0576e9531739ec536069b59dd3cbf45b32',\n",
       "   'title': 'OpBench: an operator-level GPU benchmark for deep learning'},\n",
       "  {'paperId': 'a590a4cd5de3b94470bc2d6b6fa3919fdb5ecd8c',\n",
       "   'title': 'Aspect-based Sentiment Analysis based on Feature Extraction and Attention'},\n",
       "  {'paperId': '99aa71db9708dee6ebe051901211d557a627922e',\n",
       "   'title': 'Acquiring Bidirectionality via Large and Small Language Models'},\n",
       "  {'paperId': '22aba632fd56545ca9dc8ac055277f237e428e88',\n",
       "   'title': 'Baby Bear: Seeking a Just Right Rating Scale for Scalar Annotations'},\n",
       "  {'paperId': '2192d9a8fb209a51c4d143070d1eafe0d60ad522',\n",
       "   'title': 'Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation'},\n",
       "  {'paperId': '54ed2db02ab448e888a037b6ca2e31e1b8a5e832',\n",
       "   'title': 'Customizing Language Models with Instance-wise LoRA for Sequential Recommendation'},\n",
       "  {'paperId': 'd23b8bce8d8abeab50fce08b6b7628ebb7de4d08',\n",
       "   'title': 'Goldfish: Monolingual Language Models for 350 Languages'},\n",
       "  {'paperId': 'bf184e23d67837e00bac4ec043c8afb691dc718c',\n",
       "   'title': 'Are Large Language Models More Honest in Their Probabilistic or Verbalized Confidence?'},\n",
       "  {'paperId': '128a21f4c1d37ba100239895f756e2fbff06d347',\n",
       "   'title': 'Demystifying the Communication Characteristics for Distributed Transformer Models'},\n",
       "  {'paperId': '3d3202c242c6c2c4cc1ca742184697386c32f1b3',\n",
       "   'title': 'UNINEXT-Cutie: The 1st Solution for LSVOS Challenge RVOS Track'},\n",
       "  {'paperId': 'c5568280c68c3f47d06436cf6b6ad6da28fcbdbb',\n",
       "   'title': 'Summarizing long regulatory documents with a multi-step pipeline'},\n",
       "  {'paperId': '0729563fa314f0440ffd313420ab0d27b2a977ef',\n",
       "   'title': 'Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large Language Model Augmented Framework'},\n",
       "  {'paperId': '6164f12be3b6b37ee2a2b8b9675e241a6c2ed20d',\n",
       "   'title': 'Paired Completion: Flexible Quantification of Issue-framing at Scale with LLMs'},\n",
       "  {'paperId': 'd26f3de10f2505bb8a52a629fec7a0ddefb1ef0d',\n",
       "   'title': 'ShortCircuit: AlphaZero-Driven Circuit Design'},\n",
       "  {'paperId': '2c0e773138b9f9cc054a7596a0f960387f265319',\n",
       "   'title': \"Application of Large Language Models in Automated Question Generation: A Case Study on ChatGLM's Structured Questions for National Teacher Certification Exams\"},\n",
       "  {'paperId': 'b83a697b9a1238abe1295c58d769a04ae52be0b4',\n",
       "   'title': 'R2GenCSR: Retrieving Context Samples for Large Language Model based X-ray Medical Report Generation'},\n",
       "  {'paperId': 'db8b335f5286f9173e4d2315044575f6af9ded68',\n",
       "   'title': 'Geometry Informed Tokenization of Molecules for Language Model Generation'},\n",
       "  {'paperId': 'bf729dbbb4ababd328f39b49a3b7f04934262edc',\n",
       "   'title': 'A Strategy to Combine 1stGen Transformers and Open LLMs for Automatic Text Classification'},\n",
       "  {'paperId': 'b9514041b70ace7768d5da7fd14e5697772047cb',\n",
       "   'title': 'SEMDR: A Semantic-Aware Dual Encoder Model for Legal Judgment Prediction with Legal Clue Tracing'},\n",
       "  {'paperId': '86439f9345c8359ea4a943cf40ae2cee36f2b654',\n",
       "   'title': 'Impact of Large Language Models of Code on Fault Localization'},\n",
       "  {'paperId': 'f42f262036cbe5252780686cf7d42b322c8731bc',\n",
       "   'title': 'Active Learning for Identifying Disaster-Related Tweets: A Comparison with Keyword Filtering and Generic Fine-Tuning'},\n",
       "  {'paperId': 'efb36d20e941a92bc1d4a57787256f51ef5d886c',\n",
       "   'title': 'AutoML-guided Fusion of Entity and LLM-based representations'},\n",
       "  {'paperId': '46f55af46a597f4a6d450e09e78b825577bb4bd9',\n",
       "   'title': 'Rhyme-aware Chinese lyric generator based on GPT'},\n",
       "  {'paperId': 'e27dd0dc9d06e857a98233d90a6fb26e004e1cf7',\n",
       "   'title': 'Attention is a smoothed cubic spline'},\n",
       "  {'paperId': '7544fe7231da7cd26c5a1bf88a25f94583df4965',\n",
       "   'title': 'MambaLoc: Efficient Camera Localisation via State Space Model'},\n",
       "  {'paperId': 'ba903f70e9376ce707257a274dfe517b2cb0b1ec',\n",
       "   'title': 'Recent Surge in Public Interest in Transportation: Sentiment Analysis of Baidu Apollo Go Using Weibo Data'},\n",
       "  {'paperId': '0aead2032217c0afd96b2ee989a9847b9b635d62',\n",
       "   'title': 'Uniting contrastive and generative learning for event sequences models'},\n",
       "  {'paperId': 'e2b0de0cff43d4b171ac509b9dec4cb7da147dff',\n",
       "   'title': 'TBA: Faster Large Language Model Training Using SSD-Based Activation Offloading'},\n",
       "  {'paperId': 'ed49051d75879f2604ab4c3cbad4fee303d0e91b',\n",
       "   'title': 'Fragment and Geometry Aware Tokenization of Molecules for Structure-Based Drug Design Using Language Models'},\n",
       "  {'paperId': 'e207c5dd3af449ecc0f43c76e968d3955ed3c497',\n",
       "   'title': 'Multi-Agent Reinforcement Learning for Autonomous Driving: A Survey'},\n",
       "  {'paperId': '519e151b5ccbf2b1bba0b85c27d495ba8c3f42dd',\n",
       "   'title': 'Enhanced document retrieval with topic embeddings'},\n",
       "  {'paperId': '54eff79234e66a17196b34bde8deaef3c5066101',\n",
       "   'title': 'Resolving Lexical Bias in Edit Scoping with Projector Editor Networks'},\n",
       "  {'paperId': 'c3f801687d2c16c004c09758fa3d370a114b0498',\n",
       "   'title': 'Leveraging Superfluous Information in Contrastive Representation Learning'},\n",
       "  {'paperId': '9913f0b488b5e3e74870d991a16be130e495ac98',\n",
       "   'title': 'Quantized multi-task learning for context-specific representations of gene network dynamics'},\n",
       "  {'paperId': 'bd2480049c2719c4d1ccc4df2084290182b60fc1',\n",
       "   'title': 'MegaFake: A Theory-Driven Dataset of Fake News Generated by Large Language Models'},\n",
       "  {'paperId': '48a1b7116a11b3b385826f8ad6cc6325fab8c1c1',\n",
       "   'title': 'Joint coordinate attention mechanism and instance normalization for COVID online comments text classification'},\n",
       "  {'paperId': '3c758c3c513564dfda956f2cd1dbbea723e457b7',\n",
       "   'title': 'PhysBERT: A Text Embedding Model for Physics Scientific Literature'},\n",
       "  {'paperId': 'e974084ef8da1a7898d97013c974a6180f681b87',\n",
       "   'title': 'How Do Social Bots Participate in Misinformation Spread? A Comprehensive Dataset and Analysis'},\n",
       "  {'paperId': '986752e0de257d7798e052285081bf8b124442e0',\n",
       "   'title': 'Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design'},\n",
       "  {'paperId': 'ab9e71cf853cee377c37b5688993ddb69d535b6d',\n",
       "   'title': 'Deep Code Search with Naming-Agnostic Contrastive Multi-View Learning'},\n",
       "  {'paperId': '0213d6f4495a445a7f8bdf7ef7aee88162f293d9',\n",
       "   'title': 'DiffZOO: A Purely Query-Based Black-Box Attack for Red-teaming Text-to-Image Generative Model via Zeroth Order Optimization'},\n",
       "  {'paperId': '83e1d5189bfdf405ad30d479adb4b676a7e10f1a',\n",
       "   'title': 'Protein Design by Directed Evolution Guided by Large Language Models'},\n",
       "  {'paperId': 'd976e08ac91d62484f29230208d22565c279f39d',\n",
       "   'title': 'Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks'},\n",
       "  {'paperId': '8c519e68afcce15838050f5daf1f30532673368d',\n",
       "   'title': 'REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning'},\n",
       "  {'paperId': '6c33e66f58bb9cc88f6179f6f62ac3d4f67b9c70',\n",
       "   'title': 'Meta-Learning Empowered Meta-Face: Personalized Speaking Style Adaptation for Audio-Driven 3D Talking Face Animation'},\n",
       "  {'paperId': '8b499eaad0a7297c7412fb91d4b291815be7081c',\n",
       "   'title': 'Combo: Co-speech holistic 3D human motion generation and efficient customizable adaptation in harmony'},\n",
       "  {'paperId': 'c923bb61ab5ef5e99809571ccc0395b2802f2807',\n",
       "   'title': 'Out-of-distribution generalization via composition: a lens through induction heads in Transformers'},\n",
       "  {'paperId': 'bbe2c7b703e2323dd3dbfda410425e793e6cf37d',\n",
       "   'title': 'Flemme: A Flexible and Modular Learning Platform for Medical Images'},\n",
       "  {'paperId': '2a8471b883bb83ccbe723c3e41d0349be7e95560',\n",
       "   'title': 'NAVERO: Unlocking Fine-Grained Semantics for Video-Language Compositionality'},\n",
       "  {'paperId': 'b7e34c42d56cac83a20fbc08ad05085ea279d0d3',\n",
       "   'title': 'Threshold Filtering Packing for Supervised Fine-Tuning: Training Related Samples within Packs'},\n",
       "  {'paperId': 'f2ae2e34358f073f9d6150ea299ef32dc4040ca5',\n",
       "   'title': 'Activated Parameter Locating via Causal Intervention for Model Merging'},\n",
       "  {'paperId': 'd06850c81dd90135c8e5eacbbe307489b7a57cd2',\n",
       "   'title': 'Advancements in Molecular Property Prediction: A Survey of Single and Multimodal Approaches'},\n",
       "  {'paperId': '374d3f85584ac5a22f27ca15fe3f928ccda8ebc0',\n",
       "   'title': 'From Specifications to Prompts: On the Future of Generative Large Language Models in Requirements Engineering'},\n",
       "  {'paperId': 'fd03b1e5f7800aacaa11195657cf923001a546db',\n",
       "   'title': 'Chinese Metaphor Recognition Using a Multi-stage Prompting Large Language Model'},\n",
       "  {'paperId': 'fbf0e72bd11fc80d1873891850201ebaadd5ca8f',\n",
       "   'title': 'CogLM: Tracking Cognitive Development of Large Language Models'},\n",
       "  {'paperId': '2d6bd5ebdd5183b2c754725e923c94e334f5d26b',\n",
       "   'title': 'Zero-Shot Object-Centric Representation Learning'},\n",
       "  {'paperId': 'a90ebcbbdc3ef240b0a01535e27d1929f9614fdc',\n",
       "   'title': 'FabricQA-Extractor: A Question Answering System to Extract Information from Documents using Natural Language Questions'},\n",
       "  {'paperId': '2aeb165c4306b0f605c0d0ed997475f9d59080e4',\n",
       "   'title': 'EagleEye: Attention to Unveil Malicious Event Sequences from Provenance Graphs'},\n",
       "  {'paperId': '964bd9fcbf8da6add021e45586213398c50c7374',\n",
       "   'title': 'ConVerSum: A Contrastive Learning based Approach for Data-Scarce Solution of Cross-Lingual Summarization Beyond Direct Equivalents'},\n",
       "  {'paperId': '903e291106415b5279e8d651446e1942764fddd5',\n",
       "   'title': 'Locate Anything on Earth: Advancing Open-Vocabulary Object Detection for Remote Sensing Community'},\n",
       "  {'paperId': '8914e60bd4205ce0141b4442b596dd5768b9a1ce',\n",
       "   'title': 'Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson'},\n",
       "  {'paperId': '5eeb3a09c6e5c05f65bfe81ebc2c5305b0c95f43',\n",
       "   'title': 'Identifying Technical Debt and Its Types Across Diverse Software Projects Issues'},\n",
       "  {'paperId': '3f4af416ffce9af29e9d07a1c1eed712a8893207',\n",
       "   'title': 'Increasing transformer token length with a Maximum Entropy Principle Method'},\n",
       "  {'paperId': '6e3ae67632e427efb4e0b7b19a84701c92683c7f',\n",
       "   'title': \"Sentiment analysis of preservice teachers' reflections using a large language model\"},\n",
       "  {'paperId': 'db1ea78fd890adc493c8de4444e93a9a46718138',\n",
       "   'title': 'Multimodal Relational Triple Extraction with Query-based Entity Object Transformer'},\n",
       "  {'paperId': '388abb88c799507252e34227bea7fbaf92359f8d',\n",
       "   'title': 'A Multi-Task and Multi-Label Classification Model for Implicit Discourse Relation Recognition'},\n",
       "  {'paperId': '27d793deee008a148e1b3bb3f371fb2c8a734be1',\n",
       "   'title': \"mRNA2vec: mRNA Embedding with Language Model in the 5'UTR-CDS for mRNA Design\"},\n",
       "  {'paperId': '4e11d368fdc8eb44c889c4bad502e09d5fa220c2',\n",
       "   'title': 'CIKMar: A Dual-Encoder Approach to Prompt-Based Reranking in Educational Dialogue Systems'},\n",
       "  {'paperId': '8bd2ee335d518f52d39d2c83017a6a4746b8d9db',\n",
       "   'title': 'Leveraging FourierKAN Classification Head for Pre-Trained Transformer-based Text Classification'},\n",
       "  {'paperId': 'b19777932ebf20d0d4bf062f3db47a3e2fdd6d98',\n",
       "   'title': 'BnSentMix: A Diverse Bengali-English Code-Mixed Dataset for Sentiment Analysis'},\n",
       "  {'paperId': 'dc0f0f691f96ec6cb56f369e4da91d987826bdac',\n",
       "   'title': 'Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models'},\n",
       "  {'paperId': '5f2080d76c774ff2b2a66675e7a09094ccfc6cef',\n",
       "   'title': 'Detecting Misinformation in Multimedia Content through Cross-Modal Entity Consistency: A Dual Learning Approach'},\n",
       "  {'paperId': '0e81e15dac9f8475e04271103ba35fed1fbf259e',\n",
       "   'title': 'The application of blood flow sound contrastive learning to predict arteriovenous graft stenosis of patients with hemodialysis'},\n",
       "  {'paperId': '1290b492b3f66e22605f93c7f35ef09a90dff100',\n",
       "   'title': 'ChatZero:Zero-shot Cross-Lingual Dialogue Generation via Pseudo-Target Language'},\n",
       "  {'paperId': '9e02fb04d6056fc404cbd831691b4309d650e756',\n",
       "   'title': 'RealMedQA: A pilot biomedical question answering dataset containing realistic clinical questions'},\n",
       "  {'paperId': '57a3d2f141f4ed5348c47f2f98601cf68aedd403',\n",
       "   'title': 'Collaborative Cross-modal Fusion with Large Language Model for Recommendation'},\n",
       "  {'paperId': 'ecd8955f51ad4fecd320acbd6a1ea4a3e7b95bb2',\n",
       "   'title': 'GeoTransformer: Enhancing Urban Forecasting with Geospatial Attention Mechanisms'},\n",
       "  {'paperId': '7701614c92f0da1403eac1fff5bf78d85a529df0',\n",
       "   'title': 'MAT-SED: A Masked Audio Transformer with Masked-Reconstruction Based Pre-training for Sound Event Detection'},\n",
       "  {'paperId': '9d446652496cc9172dd2c0d85aec8401d82c1d89',\n",
       "   'title': 'TEXTOC: Text-driven Object-Centric Style Transfer'},\n",
       "  {'paperId': 'b19bdb682671e24c81191200cd19030115c7cde2',\n",
       "   'title': 'Xpikeformer: Hybrid Analog-Digital Hardware Acceleration for Spiking Transformers'},\n",
       "  {'paperId': '333deea259d3843182e886586d73a099066da2a5',\n",
       "   'title': 'TAMER: Tree-Aware Transformer for Handwritten Mathematical Expression Recognition'},\n",
       "  {'paperId': '1204cd5408f4246f8ed22f4402e3009db01c44ce',\n",
       "   'title': 'CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking'},\n",
       "  {'paperId': '8b76c2cb50dd6860a598f690858af3738ed9341b',\n",
       "   'title': 'Rubick: Exploiting Job Reconfigurability for Deep Learning Cluster Scheduling'},\n",
       "  {'paperId': '8247c7a67e4e3e526cee10ca9499b1936204e79c',\n",
       "   'title': 'Scaling up Multimodal Pre-training for Sign Language Understanding'},\n",
       "  {'paperId': '1f0db7a8be492886568c89d03742e7ed6e7182c2',\n",
       "   'title': 'Instruction-tuned large language models misalign with natural language comprehension in humans'},\n",
       "  {'paperId': 'e1e597dd866474c55177ff8d5187db8e78963f62',\n",
       "   'title': 'Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges'},\n",
       "  {'paperId': '81eeb280e169aa6a5c592ffa4d7a61fd6e86a8dd',\n",
       "   'title': 'Adaptive Uncertainty Quantification for Generative AI'},\n",
       "  {'paperId': '4835861467e81bc265a6f3dc44f7dc4e5abe7b1a',\n",
       "   'title': 'AdaRank: Disagreement Based Module Rank Prediction for Low-rank Adaptation'},\n",
       "  {'paperId': 'eb429372837deca2f08a4a617f5af099750534e7',\n",
       "   'title': 'DAC: Decomposed Automation Correction for Text-to-SQL'},\n",
       "  {'paperId': '104f88b6934a3f2c3a27c14afe639e1f559dbf5b',\n",
       "   'title': 'A theory-informed deep learning approach to extracting and characterizing substance use-related stigma in social media'},\n",
       "  {'paperId': 'b50619714c5dcc5b8b34e761928bea319cbd1b65',\n",
       "   'title': 'Feature Interaction Dual Self-attention network for sequential recommendation'},\n",
       "  {'paperId': '3a94867da0a2fd3c3167a515c0f39c1316737fdf',\n",
       "   'title': 'Distilroberta2gnn: a new hybrid deep learning approach for aspect-based sentiment analysis'},\n",
       "  {'paperId': '3ea2db02e8760a67f692e296ed2d544343c5e814',\n",
       "   'title': 'Joint merging and pruning: adaptive selection of better token compression strategy'},\n",
       "  {'paperId': '7469c4191bef07dda8e222f6cd1e0d3a7ca325d3',\n",
       "   'title': 'AI-enabled FinTech for innovative sustainability: promoting organizational sustainability practices in digital accounting and finance'},\n",
       "  {'paperId': 'ebd6031201aa009036cc8470343f5810342f7bd6',\n",
       "   'title': 'Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval'},\n",
       "  {'paperId': 'fe3a856255ce03c326977454168abc7b76c2e575',\n",
       "   'title': 'Dynamic Adaptive Optimization for Effective Sentiment Analysis Fine-Tuning on Large Language Models'},\n",
       "  {'paperId': '1af306fc0620255d53c20c9a0a07ccc3841c1f4c',\n",
       "   'title': 'OPDR: Order-Preserving Dimension Reduction for Semantic Embedding of Multimodal Scientific Data'},\n",
       "  {'paperId': 'e3bcc9d98aee8f3217aa0302cfb6cf00a7f32ca7',\n",
       "   'title': 'RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train and Deploy Edge Classifiers for Computational Social Science'},\n",
       "  {'paperId': 'b613b54e8283e3a2ec833450bc4099076e248424',\n",
       "   'title': 'mhGPT: A Lightweight Generative Pre-Trained Transformer for Mental Health Text Analysis'},\n",
       "  {'paperId': '7c3a4c1ae54ac8eaebcfab8cda8ed64db9169d8e',\n",
       "   'title': 'GERestaurant: A German Dataset of Annotated Restaurant Reviews for Aspect-Based Sentiment Analysis'},\n",
       "  {'paperId': 'a677bacb923ebf38094ce6f901c8df8b0e62ee2f',\n",
       "   'title': 'P/D-Serve: Serving Disaggregated Large Language Model at Scale'},\n",
       "  {'paperId': '2423aa76d8e69f0523931f776731cf30e60598ad',\n",
       "   'title': 'Enhancing Large Language Model-based Speech Recognition by Contextualization for Rare and Ambiguous Words'},\n",
       "  {'paperId': '3967e050023ea8893ad95afd0dc4f46412971987',\n",
       "   'title': 'Normalized AOPC: Fixing Misleading Faithfulness Metrics for Feature Attribution Explainability'},\n",
       "  {'paperId': 'd72c2429dac75c8563d30332c9680fdcafcd063a',\n",
       "   'title': 'Unsupervised Part Discovery via Dual Representation Alignment.'},\n",
       "  {'paperId': '843f161ea1136e15c36f7d2d36e2dfe5de8d9299',\n",
       "   'title': 'Adaptive Learning of Consistency and Inconsistency Information for Fake News Detection'},\n",
       "  {'paperId': 'd0fd6eacce00579a95ebdbc69f227774cee349ab',\n",
       "   'title': 'The Clever Hans Effect in Unsupervised Learning'},\n",
       "  {'paperId': '2cf2ed8edde18beaa709f4ed890abd76814be03c',\n",
       "   'title': 'Advancing Multi-grained Alignment for Contrastive Language-Audio Pre-training'},\n",
       "  {'paperId': '128e05957311bfeacb687398e84e288eea1a5c27',\n",
       "   'title': 'Supporting the working life exposome: Annotating occupational exposure for enhanced literature search'},\n",
       "  {'paperId': 'beaf72981de56f3636302a7d9d6e2a63bc1418b8',\n",
       "   'title': 'W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering'},\n",
       "  {'paperId': 'c97747830f573f1153f5660bd049d6c995ea7ed8',\n",
       "   'title': 'An Efficient and Explainable Transformer-Based Few-Shot Learning for Modeling Electricity Consumption Profiles Across Thousands of Domains'},\n",
       "  {'paperId': '1f16aa0610714966b4855a9b837487d69a442266',\n",
       "   'title': 'Zero-Shot Learning and Key Points Are All You Need for Automated Fact-Checking'},\n",
       "  {'paperId': '549eac92f87a60f73645e1aba5384decfa4ebfee',\n",
       "   'title': 'A Survey of Trojan Attacks and Defenses to Deep Neural Networks'},\n",
       "  {'paperId': '9ab45aa875b56335303398e84a59a3756cd9d530',\n",
       "   'title': 'Graph Retrieval-Augmented Generation: A Survey'},\n",
       "  {'paperId': '98099536ffef1b0b2fdc46d8115e63a726900f9a',\n",
       "   'title': 'Coarse-to-fine Alignment Makes Better Speech-image Retrieval'},\n",
       "  {'paperId': '9202ce93d9c0b81befc7ce01655c31ad12bd4cd4',\n",
       "   'title': 'Cross-Modal Denoising: A Novel Training Paradigm for Enhancing Speech-Image Retrieval'},\n",
       "  {'paperId': '03be06a96a24f507a9c9447962d811c968ee2814',\n",
       "   'title': 'Automated information extraction model enhancing traditional Chinese medicine RCT evidence extraction (Evi-BERT): algorithm development and validation'},\n",
       "  {'paperId': 'c2273e50aa10f5d5b6c37e20e16857c6ff2568cb',\n",
       "   'title': 'HIST: Hierarchical and sequential transformer for image captioning'},\n",
       "  {'paperId': 'c751895512a857f6979acfc0ac2b16166600db60',\n",
       "   'title': 'Conversational Agents: An Exploration into Chatbot Evolution, Architecture, and Important Techniques'},\n",
       "  {'paperId': '5232bd8114978f18284fe0c9bbb0aa4b247f0a6b',\n",
       "   'title': 'Enhancing Trustworthiness and Minimising Bias Issues in Leveraging Social Media Data for Disaster Management Response'},\n",
       "  {'paperId': 'e9492b079ade57ca4c7ccfeb603db7aa3cc18913',\n",
       "   'title': 'Only One Relation Possible? Modeling the Ambiguity in Event Temporal Relation Extraction'},\n",
       "  {'paperId': '14fdd562c54b7cb6daaebf3623d06023ab195707',\n",
       "   'title': 'Fact or Fiction? Improving Fact Verification with Knowledge Graphs through Simplified Subgraph Retrievals'},\n",
       "  {'paperId': 'fb779b8cad3321a255cbe71fdc16599f5770b987',\n",
       "   'title': 'Cross-Aware Early Fusion With Stage-Divided Vision and Language Transformer Encoders for Referring Image Segmentation'},\n",
       "  {'paperId': '4b6155d1e202f79953218cd0d7182f6dc060b4c1',\n",
       "   'title': 'LLMI3D: Empowering LLM with 3D Perception from a Single 2D Image'},\n",
       "  {'paperId': '20a9c8b89804ae9369c1032c653840a1cfac637c',\n",
       "   'title': 'End-to-end Semantic-centric Video-based Multimodal Affective Computing'},\n",
       "  {'paperId': '280a5913482183556cb9ba93c8bcf8bee1693366',\n",
       "   'title': 'Operator Feature Neural Network for Symbolic Regression'},\n",
       "  {'paperId': '37789bf2f84f1eb8570c9bc1f18c6ad81ac392dd',\n",
       "   'title': 'GRIF-DM: Generation of Rich Impression Fonts using Diffusion Models'},\n",
       "  {'paperId': 'fb71e639c2ecad16bf0a5c76ef442ac5b8eb9dd8',\n",
       "   'title': 'LiPCoT: Linear Predictive Coding based Tokenizer for Self-supervised Learning of Time Series Data via Language Models'},\n",
       "  {'paperId': '6cfed267e1b432e668d984ab894055c0b33e93ab',\n",
       "   'title': 'Exploring Retrieval Augmented Generation in Arabic'},\n",
       "  {'paperId': '116dbfca721897ff45ff5e99838b1f97d7c09d70',\n",
       "   'title': 'NL2OR: Solve Complex Operations Research Problems Using Natural Language Inputs'},\n",
       "  {'paperId': '0d74faa798bce46a6970723c042bdd15d2273079',\n",
       "   'title': 'DataVisT5: A Pre-trained Language Model for Jointly Understanding Text and Data Visualization'},\n",
       "  {'paperId': '345e3093f76d6e0e36b68abb229d85bb926104f4',\n",
       "   'title': 'From Brazilian Portuguese to European Portuguese'},\n",
       "  {'paperId': '41026e7ccb1f1dac273c255b591073116479afb1',\n",
       "   'title': 'Assessing the Role of Lexical Semantics in Cross-lingual Transfer through Controlled Manipulations'},\n",
       "  {'paperId': 'cb43ca140c22fa46f5566226f4d482e7e428aee3',\n",
       "   'title': 'Large Language Models Prompting With Episodic Memory'},\n",
       "  {'paperId': 'fcb7bb76c5645deaedf464c57146240b18cb406a',\n",
       "   'title': 'Image-Based Leopard Seal Recognition: Approaches and Challenges in Current Automated Systems'},\n",
       "  {'paperId': 'f8a065986ce18364c7cbe7a6f6fd44e4c60f57df',\n",
       "   'title': 'Evaluating Large Language Model based Personal Information Extraction and Countermeasures'},\n",
       "  {'paperId': 'c0272bf2485a69bae44096bfcf4ce7db58ff2382',\n",
       "   'title': 'PolyCL: Contrastive Learning for Polymer Representation Learning via Explicit and Implicit Augmentations'},\n",
       "  {'paperId': '74b4872e0ede41a06001eeb0305546633d13cf9a',\n",
       "   'title': 'Assessing the alignment of\\xa0corporate ESG disclosures with the UN sustainable development goals: a BERT-based text analysis'},\n",
       "  {'paperId': '0f5b687fea38c0e77183b0a7f74f3f4acfe6af67',\n",
       "   'title': 'An Effective Information Theoretic Framework for Channel Pruning'},\n",
       "  {'paperId': 'd2ebec4455b35c0806d24db09657f0a7daa43b29',\n",
       "   'title': \"BERT's Conceptual Cartography: Mapping the Landscapes of Meaning\"},\n",
       "  {'paperId': '749af11854db6b02422c6073e34d6bcae5781e0d',\n",
       "   'title': 'Fast-and-Frugal Text-Graph Transformers are Effective Link Predictors'},\n",
       "  {'paperId': 'db0a44b586b7c02c1a6f506e3ced2106c092adf3',\n",
       "   'title': 'Multilingual Models for Check-Worthy Social Media Posts Detection'},\n",
       "  {'paperId': '7b81c34d39995b890d22285d368eb115a68879d3',\n",
       "   'title': 'Masked Image Modeling: A Survey'},\n",
       "  {'paperId': '437f3ddc70113852c17e4d5711b7ff2e545c72c9',\n",
       "   'title': 'ChatGPT and imaginaries of the future of education: insights of Finnish teacher educators'},\n",
       "  {'paperId': '86d2226690fc90837b10215538a9a4178ed9988d',\n",
       "   'title': 'X-News dataset for online news categorization'},\n",
       "  {'paperId': 'cb90b5dcd0454f7317294cbd4127e2cf296cc3c2',\n",
       "   'title': 'CTISum: A New Benchmark Dataset For Cyber Threat Intelligence Summarization'},\n",
       "  {'paperId': '51d862461bccad7fe48c09c8ad85ddfbd092764c',\n",
       "   'title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model'},\n",
       "  {'paperId': '6cc893d8dd4f5385a60e744d963bb3b1c8608958',\n",
       "   'title': 'LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large Language Models'},\n",
       "  {'paperId': 'dbbf7bff411811a9bf715c0ab1526f9757782ebf',\n",
       "   'title': 'Pragmatic inference of scalar implicature by LLMs'},\n",
       "  {'paperId': 'f4a214205aae401e665ea320f82bfc2325b82389',\n",
       "   'title': 'Towards Holistic Disease Risk Prediction using Small Language Models'},\n",
       "  {'paperId': '398c04880f61ec2491911b7c2b25e197c3956ae6',\n",
       "   'title': 'Membership Inference Attack Against Masked Image Modeling'},\n",
       "  {'paperId': '2bc0e1b782ec7ff56bcd0d117e553f411c7940b0',\n",
       "   'title': 'Prompt Tuning as User Inherent Profile Inference Machine'},\n",
       "  {'paperId': '10ede0af669828313fb0d1285ddeefea3a268db7',\n",
       "   'title': 'Characterizing human spontaneous thoughts and its application in major depressive disorder.'},\n",
       "  {'paperId': '683c8c71d08cd57eed635d1771cad42b71dd3b47',\n",
       "   'title': 'What should I wear to a party in a Greek taverna? Evaluation for Conversational Agents in the Fashion Domain'},\n",
       "  {'paperId': '1ab2b1262e84b7071758d7757d37a2761185056b',\n",
       "   'title': 'Detecting the Impact of COVID-19 on Social Media using BERT-Based Model'},\n",
       "  {'paperId': '5f11ff19235d3273010b7e17ccafdd9d5cf793ee',\n",
       "   'title': 'FLAT: Fusing layer representations for more efficient transfer learning in NLP.'},\n",
       "  {'paperId': '771b4d6a94dce59f90ce48a43a3b84a1f7713b1c',\n",
       "   'title': 'Image-text multimodal classification via cross-attention contextual transformer with modality-collaborative learning'},\n",
       "  {'paperId': '53735f354ab315dc46cd97984ae3bad82b13037f',\n",
       "   'title': 'An Investigation Into Explainable Audio Hate Speech Detection'},\n",
       "  {'paperId': 'a0d60446cdee8ead3a4b2f7b9a54312a7df5ffd0',\n",
       "   'title': 'LOLgorithm: Integrating Semantic,Syntactic and Contextual Elements for Humor Classification'},\n",
       "  {'paperId': '8ec82236f226785bf803d09e65677f71b79ff509',\n",
       "   'title': 'LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid Library'},\n",
       "  {'paperId': '17d46cfdd0a8b76829a3888d433f2a1a1a2f91da',\n",
       "   'title': 'Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models'},\n",
       "  {'paperId': 'f268ce6997f0cfa6d13fbf47678951617a511d16',\n",
       "   'title': 'Generative Large Language Models in Electronic Health Records for Patient Care Since 2023: A Systematic Review'},\n",
       "  {'paperId': '28f2d200e056b214d8e038d86fb7c57744df7c02',\n",
       "   'title': 'The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI'},\n",
       "  {'paperId': '860a9a14f7b20d7ca6014c566212ff4a4f84e798',\n",
       "   'title': 'Adapting a Foundation Model for Space-based Tasks'},\n",
       "  {'paperId': '02f4ef057b9da10557e0c268462b8fa988731a4d',\n",
       "   'title': 'DiagESC: Dialogue Synthesis for Integrating Depression Diagnosis into Emotional Support Conversation'},\n",
       "  {'paperId': '3c9f9a65bb00c12076a3a77e7223d8577084d9e0',\n",
       "   'title': 'Hyperion: Unveiling DApp Inconsistencies using LLM and Dataflow-Guided Symbolic Execution'},\n",
       "  {'paperId': '057b3980bce0d57c80d44799056f6db6772eb1af',\n",
       "   'title': 'Enhancing Dialogue Speech Recognition with Robust Contextual Awareness via Noise Representation Learning'},\n",
       "  {'paperId': '1fb0c8695ff4d4a8ab8fca722fd7e7731e64551c',\n",
       "   'title': 'Multi-scale contrastive adaptor learning for segmenting anything in underperformed scenes'},\n",
       "  {'paperId': 'dc4b89ab5200a97248ee822c83d23a6894e1d3c0',\n",
       "   'title': 'Deep Multimodal Collaborative Learning for Polyp Re-Identification'},\n",
       "  {'paperId': '23845587dce8d947a7c6c533881d8fd8167df729',\n",
       "   'title': 'A RAG-Based Question-Answering Solution for Cyber-Attack Investigation and Attribution'},\n",
       "  {'paperId': '48fcd6cde48590d2efd87c0ef0e839bcd19a921c',\n",
       "   'title': 'RISurConv: Rotation Invariant Surface Attention-Augmented Convolutions for 3D Point Cloud Classification and Segmentation'},\n",
       "  {'paperId': 'e13130d8fe49e520d69893b00e2a856121326127',\n",
       "   'title': 'Is it a work or leisure travel? Applying text classification to identify work-related travel on social networks'},\n",
       "  {'paperId': 'c5fde70819eece74fd3a7a2492f1eddcd2120b07',\n",
       "   'title': 'ARPA: A Novel Hybrid Model for Advancing Visual Word Disambiguation Using Large Language Models and Transformers'},\n",
       "  {'paperId': '71f570bba213de89bc9497316a5138da6e09a926',\n",
       "   'title': 'FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection'},\n",
       "  {'paperId': 'e4ec4dea3e6f26e69f39a96d00d228070f4bde60',\n",
       "   'title': 'ControlNeXt: Powerful and Efficient Control for Image and Video Generation'},\n",
       "  {'paperId': 'a06eacdd4d9838ac582cf3fe56bdf260f62f0abb',\n",
       "   'title': 'Arlo: Serving Transformer-based Language Models with Dynamic Input Lengths'},\n",
       "  {'paperId': '7d44918747e21cdc41c2b47b1c4ea69181cd51df',\n",
       "   'title': 'Unleash The Power of Pre-Trained Language Models for Irregularly Sampled Time Series'},\n",
       "  {'paperId': 'd9eb6000fd6aeaf8121e1eacf81835099f04f5d7',\n",
       "   'title': 'Improving Source Code Similarity Detection Through GraphCodeBERT and Integration of Additional Features'},\n",
       "  {'paperId': '5b2a73d8ef62a4137df350b38e10dbfeaf4c03ff',\n",
       "   'title': 'Towards Effective Authorship Attribution: Integrating Class-Incremental Learning'},\n",
       "  {'paperId': '8f39cd8820523fdd3323a42adc019f761dc8ca0b',\n",
       "   'title': 'Large Investment Model'},\n",
       "  {'paperId': '008092c741dd2c78a62940373038e1e5c5a6089e',\n",
       "   'title': 'Prompto: An open source library for asynchronous querying of LLM endpoints'},\n",
       "  {'paperId': 'cc96a2943e126f4b32f2bd98fc3e79eae7943d93',\n",
       "   'title': 'An Empirical Study on Teaching Management and Quality Feedback Based on Human Behavior Detection'},\n",
       "  {'paperId': '0f2fa45cf285c09d5a5e85b7081235a0093e37ff',\n",
       "   'title': 'A Deep Learning Approach to Predicting Sandstone Permeability'},\n",
       "  {'paperId': 'c0e816d7fbe95a714f10d5944d7d814c333f642c',\n",
       "   'title': 'Research on Image Translation Problems Based on Multimodal Data Set Fusion'},\n",
       "  {'paperId': '6dfc0cbfeb626be01a1cc3ddc1ffb8f7c4bd8800',\n",
       "   'title': 'BERT Model with Fuzzy Logic Optimization on Multivariate Sentiment Analysis Tasks'},\n",
       "  {'paperId': 'c76818bcae0aa3fc328b27de3d26a50a705e089c',\n",
       "   'title': 'Research on Keywords Extraction of Film Reviews Based on the KeyBERT Model'},\n",
       "  {'paperId': 'a5c80bb40a6af6c3e4e72f2775a581a5315a94c4',\n",
       "   'title': 'HiLight: A Hierarchy-aware Light Global Model with Hierarchical Local ConTrastive Learning'},\n",
       "  {'paperId': '8c2142406a1d1712f034aca134282b508aa40879',\n",
       "   'title': 'Efficient Diffusion Transformer with Step-wise Dynamic Attention Mediators'},\n",
       "  {'paperId': 'aae473aa5836f19f858f06d736aadf567d9902a3',\n",
       "   'title': 'Sampling Foundational Transformer: A Theoretical Perspective'},\n",
       "  {'paperId': 'c112b238f649bdd2ce0bb97122ae2702c643629e',\n",
       "   'title': 'Iterative Improvement of an Additively Regularized Topic Model'},\n",
       "  {'paperId': '832df5e1cb4ed0a6868e162f697779a28871f069',\n",
       "   'title': 'VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing'},\n",
       "  {'paperId': 'd298732b71adcadfb1a652fc36251dc9c3b4aa0b',\n",
       "   'title': 'Hotfixing Large Language Models for Code: How Far Can Parameter-Efficient Fine-Tuning Go?'},\n",
       "  {'paperId': '5d942a8cbef40c596db6b9ea094e49d3349c11f8',\n",
       "   'title': 'GraphTransfer: A Generic Feature Fusion Framework for Collaborative Filtering'},\n",
       "  {'paperId': 'ecce8e9e9966f2f9fa6625c333ba3affa825a64e',\n",
       "   'title': 'HySparK: Hybrid Sparse Masking for Large Scale Medical Image Pre-Training'},\n",
       "  {'paperId': '4704b12fb63b9f5aff8d589fc963a35669d16403',\n",
       "   'title': 'People over trust AI-generated medical responses and view them to be as valid as doctors, despite low accuracy'},\n",
       "  {'paperId': '82e5604aac27ef18f05e6f3dd1b091476d577c2c',\n",
       "   'title': 'Multitask Fine-Tuning and Generative Adversarial Learning for Improved Auxiliary Classification'},\n",
       "  {'paperId': '12ae6a928b8c8b17e5480c857020d56c254844a3',\n",
       "   'title': 'MABR: A Multilayer Adversarial Bias Removal Approach Without Prior Bias Knowledge'},\n",
       "  {'paperId': 'b25a0a1f0d1f9f62cb1debf1b80fb741be19fe91',\n",
       "   'title': \"WiDe-analysis: Enabling One-click Content Moderation Analysis on Wikipedia's Articles for Deletion\"},\n",
       "  {'paperId': '02f2227ba3b3037fe8c6951f2e8a6d792fb898b2',\n",
       "   'title': 'LaiDA: Linguistics-aware In-context Learning with Data Augmentation for Metaphor Components Identification'},\n",
       "  {'paperId': '6daf0de70d5b3ab555c3bf87460be2295f9a5522',\n",
       "   'title': 'Mapping vaccine names in clinical trials to vaccine ontology using cascaded fine-tuned domain-specific language models'},\n",
       "  {'paperId': '6925c4ae502a103f1434911cf40ddf6e577e1217',\n",
       "   'title': 'UrFound: Towards Universal Retinal Foundation Models via Knowledge-Guided Masked Modeling'},\n",
       "  {'paperId': 'bca796f85c77f21df1d8c9c48c9b24cc94bc895f',\n",
       "   'title': 'SAMSA: Efficient Transformer for Many Data Modalities'},\n",
       "  {'paperId': '9552f25c35458ef4353aed424d4d6f690a66185d',\n",
       "   'title': 'PersonViT: Large-scale Self-supervised Vision Transformer for Person Re-Identification'},\n",
       "  {'paperId': '99e4ad46c03144dfc7b8e1e19a9e17c90dfc5014',\n",
       "   'title': 'Scene123: One Prompt to 3D Scene Generation via Video-Assisted and Consistency-Enhanced MAE'},\n",
       "  {'paperId': '63f05ba4d7ec1af82c756348e7fb6c90c8ab4c4e',\n",
       "   'title': 'Multi-agent Planning using Visual Language Models'},\n",
       "  {'paperId': '2ca2332434413affd2747697c4b7c4ffda139471',\n",
       "   'title': 'Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation'},\n",
       "  {'paperId': 'e76567345cb4f1cb0f83f2c20bf849066a7e8346',\n",
       "   'title': 'SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning'},\n",
       "  {'paperId': '13541a07094af87a65dcbaa848e2fabc2445e8c3',\n",
       "   'title': 'Multi-layer Sequence Labeling-based Joint Biomedical Event Extraction'},\n",
       "  {'paperId': '5d4227ea61804cf62832c01b57567e0df17bd496',\n",
       "   'title': 'Density Matrices for Metaphor Understanding'},\n",
       "  {'paperId': '517f9708323b1fa5c05195d985a2a9a8536d6e03',\n",
       "   'title': 'Exploring the Socio-Technical Imaginary of Artificial General Intelligence in The Bard Large Language Model: A Narrative Analysis on Perspectives and Dialectics'},\n",
       "  {'paperId': '5e22209e45bfed370d0a0b57abe2cbf0d4c76f98',\n",
       "   'title': 'Enhancing Representation Learning of EEG Data with Masked Autoencoders'},\n",
       "  {'paperId': '0c0b4f402e9b0e81073f946ff0e7cfe5f7145e44',\n",
       "   'title': 'Retrieval-augmented code completion for local projects using large language models'},\n",
       "  {'paperId': 'ea6d491e4aa996553bee00c2b4d24371d4ae07b9',\n",
       "   'title': 'Generating novel experimental hypotheses from language models: A case study on cross-dative generalization'},\n",
       "  {'paperId': '1d1ec17c32128e7befbb3bab6411559a0a157e42',\n",
       "   'title': 'reCSE: Portable Reshaping Features for Sentence Embedding in Self-supervised Contrastive Learning'},\n",
       "  {'paperId': 'a53826982d153fca63800ee161119ee5e5203ba5',\n",
       "   'title': 'Deep-change at AXOLOTL-24: Orchestrating WSD and WSI Models for Semantic Change Modeling'},\n",
       "  {'paperId': '54deebcc48119b88d2dc08a891a6b7f41260cf1d',\n",
       "   'title': 'A high-frequency sense list'},\n",
       "  {'paperId': '59e603da8671bedb66a7f1b9ff7c531ed27302b4',\n",
       "   'title': 'MSG-Chart: Multimodal Scene Graph for ChartQA'},\n",
       "  {'paperId': '11e794835a1f7d46560666d90c8eab3c24d84c05',\n",
       "   'title': 'An Evaluation of Standard Statistical Models and LLMs on Time Series Forecasting'},\n",
       "  {'paperId': 'ace558874af64e96b5458e80d7950438b0f4ef7b',\n",
       "   'title': 'Coding-PTMs: How to Find Optimal Code Pre-trained Models for Code Embedding in Vulnerability Detection?'},\n",
       "  {'paperId': 'fdcd41bb264916c8e10598bf4ac55375b0be69bb',\n",
       "   'title': 'A GNN Model with Adaptive Weights for Session-Based Recommendation Systems'},\n",
       "  {'paperId': 'a32d0f94d410a4dca838cfbf18247cf7020ac9b4',\n",
       "   'title': 'AttackER: Towards Enhancing Cyber-Attack Attribution with a Named Entity Recognition Dataset'},\n",
       "  {'paperId': '882e04a34a30a47c95a16d02d13fec6d05bc4ba6',\n",
       "   'title': 'Investigating a Benchmark for Training-set free Evaluation of Linguistic Capabilities in Machine Reading Comprehension'},\n",
       "  {'paperId': 'da781c69dd6a3aab514b418c8ffbbe3893376a2d',\n",
       "   'title': 'Ensemble BERT: A student social network text sentiment classification model based on ensemble learning and BERT architecture'},\n",
       "  {'paperId': '55849444b4e0b3d0e22fec70b7e81862234e23cc',\n",
       "   'title': 'A Survey of NL2SQL with Large Language Models: Where are we, and where are we going?'},\n",
       "  {'paperId': '57eb3939fcb81b0e36b99a70ba04353d74de09c9',\n",
       "   'title': 'Weak-Annotation of HAR Datasets using Vision Foundation Models'},\n",
       "  {'paperId': '34246853a93a9b4175ee5eda4ce326fb40615876',\n",
       "   'title': 'Separating Style from Substance: Enhancing Cross-Genre Authorship Attribution through Data Selection and Presentation'},\n",
       "  {'paperId': 'ea87ffa2c7b5b78d871477ef59fc1b310427867e',\n",
       "   'title': 'PTrajM: Efficient and Semantic-rich Trajectory Learning with Pretrained Trajectory-Mamba'},\n",
       "  {'paperId': 'e833a3f83ba12fb3665633d181b799ce9dff7136',\n",
       "   'title': 'Scaling Deep Learning Computation over the Inter-Core Connected Intelligence Processor'},\n",
       "  {'paperId': '8d0b010d21923ebc0dc6554e2db6beee59f6c4e1',\n",
       "   'title': 'Generalisation First, Memorisation Second? Memorisation Localisation for Natural Language Classification Tasks'},\n",
       "  {'paperId': 'a525a70bf9f96964fe14480b7b2e787056fb0bb5',\n",
       "   'title': 'Instruction Tuning-free Visual Token Complement for Multimodal LLMs'},\n",
       "  {'paperId': '9b77dd63ee13953147317fb779fa04f5f9272b6c',\n",
       "   'title': 'In Defense of Lazy Visual Grounding for Open-Vocabulary Semantic Segmentation'},\n",
       "  {'paperId': 'e2c0c0bce9320ca14b6e936e24375f1682d1fb97',\n",
       "   'title': 'Range Membership Inference Attacks'},\n",
       "  {'paperId': '6e2fcd24cb5f543d7d171f8e44ce51bbff2b0243',\n",
       "   'title': 'Semi-Supervised One-Shot Imitation Learning'},\n",
       "  {'paperId': '7ccf1d9c4105390d7acb220c64a5a7654e49341d',\n",
       "   'title': 'Logically Constrained Robotics Transformers for Enhanced Perception-Action Planning'},\n",
       "  {'paperId': 'b56408f4019cfbd18c908c5e038461de83e4649b',\n",
       "   'title': 'A Psychology-based Unified Dynamic Framework for Curriculum Learning'},\n",
       "  {'paperId': '35df2351ca79fd7e353ab5e33dae0aa9d98f4960',\n",
       "   'title': 'MUSE: Multi-Knowledge Passing on the Edges, Boosting Knowledge Graph Completion'},\n",
       "  {'paperId': '5d01c286154e33fa7aa0d376e3205af0d6881d5b',\n",
       "   'title': 'TrajFM: A Vehicle Trajectory Foundation Model for Region and Task Transferability'},\n",
       "  {'paperId': 'f09683ac10f645234fa4c3b9262046bc08bd55b5',\n",
       "   'title': 'Automated Evaluation of Antibiotic Prescribing Guideline Concordance in Pediatric Sinusitis Clinical Notes'},\n",
       "  {'paperId': '2a63b499c6fa52d06379d9b8edf229d743be0266',\n",
       "   'title': 'VideoQA in the Era of LLMs: An Empirical Study'},\n",
       "  {'paperId': '4528ba823b40c9032bbd75ad27a032135450aa17',\n",
       "   'title': 'Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency, Performance, and Adversarial Robustness'},\n",
       "  {'paperId': '72f8c25d0f72322c59efd7681155f221c7440c33',\n",
       "   'title': 'Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models'},\n",
       "  {'paperId': '4144369ae4cff07847fa63171b876de4a262fed9',\n",
       "   'title': 'Synchronous Multi-modal Semantic Communication System with Packet-level Coding'},\n",
       "  {'paperId': '47ccb3c7aa7c20c2a5edbd4fd9c1e04f0ba72e04',\n",
       "   'title': 'MemeMind at ArAIEval Shared Task: Spotting Persuasive Spans in Arabic Text with Persuasion Techniques Identification'},\n",
       "  {'paperId': '9cd44409f0bb4826bb2e3c4a4abeea59987e5dc9',\n",
       "   'title': 'SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals'},\n",
       "  {'paperId': 'f70a4712fa94bb3d44a0206ada4c6033d959924b',\n",
       "   'title': 'Emotional Cues Extraction and Fusion for Multi-modal Emotion Prediction and Recognition in Conversation'},\n",
       "  {'paperId': '6f42199dea78c969f78d7eaa7effccd7b43e8bb3',\n",
       "   'title': 'LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP'},\n",
       "  {'paperId': 'ea23d4dbb1391e3c61717b0ddb915dcb79ad499f',\n",
       "   'title': 'Semantics or spelling? Probing contextual word embeddings with orthographic noise'},\n",
       "  {'paperId': 'ae736a9d8629b98dcbe7d9acb8591bb1d2113f33',\n",
       "   'title': 'MU-MAE: Multimodal Masked Autoencoders-Based One-Shot Learning'},\n",
       "  {'paperId': 'd449cf88a1863b736f906bf368960b5e6b33b500',\n",
       "   'title': 'Analyzing Consumer Reviews for Understanding Drivers of Hotels Ratings: An Indian Perspective'},\n",
       "  {'paperId': '3ac74f36131f7db49d5bd8d61ddb01f2fe1e5edb',\n",
       "   'title': 'Explicating the Implicit: Argument Detection Beyond Sentence Boundaries'},\n",
       "  {'paperId': 'c828e63ec92a2ef2d15beeb273f267d590d47e7a',\n",
       "   'title': 'Analysis of Argument Structure Constructions in the Large Language Model BERT'},\n",
       "  {'paperId': 'e50798817acb71c91b63e5f49769cf4dcb7786ca',\n",
       "   'title': 'Enhancing Healthcare through Large Language Models: A Study on Medical Question Answering'},\n",
       "  {'paperId': 'a833d4959f4690823cf0ca7156a9619b52fb3e30',\n",
       "   'title': 'UNMuTe: Unifying Navigation and Multimodal Dialogue-like Text Generation'},\n",
       "  {'paperId': '06170d3ebeeb6cec55f46996c0bb7858eeebf9fe',\n",
       "   'title': 'MMREC: LLM Based Multi-Modal Recommender System'},\n",
       "  {'paperId': 'd7d2ac9b5dcc361e9b2c44e58158fbe8628a65ac',\n",
       "   'title': 'Crowd Intelligence for Early Misinformation Prediction on Social Media'},\n",
       "  {'paperId': '63a1f123bf50eb38b67cca469b15fcf92160b670',\n",
       "   'title': 'Pairing Clustered Inverted Indexes with kNN Graphs for Fast Approximate Retrieval over Learned Sparse Representations'},\n",
       "  {'paperId': '8de747fe55a556348f11304e718b06e2a3162d27',\n",
       "   'title': 'Enhancing Journalism with AI: A Study of Contextualized Image Captioning for News Articles using LLMs and LMMs'},\n",
       "  {'paperId': 'fde9f622cdaf72b5b0462ce1781a8470575fda02',\n",
       "   'title': 'Random Walk Diffusion for Efficient Large-Scale Graph Generation'},\n",
       "  {'paperId': '827b1fcc89920d5e8efa75f513355508596d2112',\n",
       "   'title': 'Understanding and Modeling Job Marketplace with Pretrained Language Models'},\n",
       "  {'paperId': '79667429ae4dc008fc38966f7616d5ac8199ca2c',\n",
       "   'title': 'wav2graph: A Framework for Supervised Learning Knowledge Graph from Speech'},\n",
       "  {'paperId': '50125310e75d94aafe3ddeeb2997cae23062881a',\n",
       "   'title': 'LitGene: a transformer-based model that uses contrastive learning to integrate textual information into gene representations'},\n",
       "  {'paperId': '15d9d8cf93cbdb86241b523b3bbebf854b211cd5',\n",
       "   'title': 'An associative text analyzer to\\xa0facilitate effectiveness of exploring historical texts for digital humanities'},\n",
       "  {'paperId': '09c7809b95e3e15006710aa66848d84ecfc4bbd1',\n",
       "   'title': 'The overview of the BioRED (Biomedical Relation Extraction Dataset) track at BioCreative VIII'},\n",
       "  {'paperId': 'e42a88d5b55b507b42bcd17f740bdc4086f217c5',\n",
       "   'title': 'Elucidating the Hierarchical Nature of Behavior with Masked Autoencoders'},\n",
       "  {'paperId': 'ec98ce1c0041b46e03416538fd9c4253830efa8a',\n",
       "   'title': 'Towards Linguistic Neural Representation Learning and Sentence Retrieval from Electroencephalogram Recordings'},\n",
       "  {'paperId': '0ed8aa43670b2d0f49a5ed168b87e76f71b60ee8',\n",
       "   'title': 'Survey: Transformer-based Models in Data Modality Conversion'},\n",
       "  {'paperId': 'f5e60dd3154096bbda859985c014f27152ac1d9f',\n",
       "   'title': 'AraQA-BERT: Towards an Arabic Question Answering System using Pre-trained BERT Models'},\n",
       "  {'paperId': '5d2b77db076779b4a4058acf056e473faf4f7fdd',\n",
       "   'title': 'Deep Generative Models in Robotics: A Survey on Learning from Multimodal Demonstrations'},\n",
       "  {'paperId': 'e2e6c48f2ee2fc974e66d4eb9414ec1680eaa3fc',\n",
       "   'title': 'Evaluation of AlphaFold 3’s Protein–Protein Complexes for Predicting Binding Free Energy Changes upon Mutation'},\n",
       "  {'paperId': '5064ba52891df8ebb9b0f242dba52a7909e53878',\n",
       "   'title': 'In2Core: Leveraging Influence Functions for Coreset Selection in Instruction Finetuning of Large Language Models'},\n",
       "  {'paperId': 'd3abc2cd07e02f6349f335dfc27eb80413e54445',\n",
       "   'title': 'Local Topology Measures of Contextual Language Model Latent Spaces With Applications to Dialogue Term Extraction'},\n",
       "  {'paperId': 'aaa7baa1c12b7368bb36ade6a77335b3cca4f62a',\n",
       "   'title': 'Why transformers are obviously good models of language'},\n",
       "  {'paperId': '9e6390bc66a8224e536d96591477f4ec1a0b6861',\n",
       "   'title': 'Generative Language Models with Retrieval Augmented Generation for Automated Short Answer Scoring'},\n",
       "  {'paperId': 'dfd1e3ca8931fd4032c823d22309bfeca323b362',\n",
       "   'title': 'Harnessing the Power of LLMs in Source Code Vulnerability Detection'},\n",
       "  {'paperId': '6385d61f20a72f3ddf238444adc99c1c00ccc694',\n",
       "   'title': 'mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest Neighbor Search'},\n",
       "  {'paperId': '4c1df9d03c13639ebc62ebe90ffb116abbbbe575',\n",
       "   'title': 'PackMamba: Efficient Processing of Variable-Length Sequences in Mamba training'},\n",
       "  {'paperId': '4b5c60cc1259b6d0339fc1ffe2f5f070cc79f2d7',\n",
       "   'title': 'Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning'},\n",
       "  {'paperId': '7f8f33642a5273146174609eee737cfb2c391481',\n",
       "   'title': 'Knowledge Probing for Graph Representation Learning'},\n",
       "  {'paperId': '7c41bf457b780cb8f5643e56ab3176661747a243',\n",
       "   'title': 'A Reproducible Analysis of Sequential Recommender Systems'},\n",
       "  {'paperId': '9f9f341cb5668194ea845b43c11e793985813625',\n",
       "   'title': 'SwinShadow: Shifted Window for Ambiguous Adjacent Shadow Detection'},\n",
       "  {'paperId': '6a132fc518ea22fca04d1b22d2a3e5feb5fc4ea2',\n",
       "   'title': 'Improving Retrieval-Augmented Code Comment Generation by Retrieving for Generation'},\n",
       "  {'paperId': '9a0f54833d843cca892bc452e70bf1fc47f541c5',\n",
       "   'title': 'D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion Methods'},\n",
       "  {'paperId': '7ab003c6fd354a033e38d149bcb05334c22a8db5',\n",
       "   'title': 'Task-oriented Sequential Grounding in 3D Scenes'},\n",
       "  {'paperId': 'f673bc4e31475e0da989e5a54b5cc0e2e4df3470',\n",
       "   'title': 'PowerPM: Foundation Model for Power Systems'},\n",
       "  {'paperId': '7fea734a46880093f994b6d69413fedc9461055b',\n",
       "   'title': 'Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference'},\n",
       "  {'paperId': '0261ca1f0c56e0790c69ece50fe14fbe6d943ee6',\n",
       "   'title': 'ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling'},\n",
       "  {'paperId': '13e8def66f890c7bb65d6f13a6545b4a689bddc2',\n",
       "   'title': 'Natural language processing with transformers: a review'},\n",
       "  {'paperId': '35d2beb2e61f9ef967360788ffc4408d226359e5',\n",
       "   'title': 'Improving fake job description detection using deep learning-based NLP techniques'},\n",
       "  {'paperId': '61eb0076cf53cfcdc5fc581007372af836c5b808',\n",
       "   'title': 'MathBridge: A Large Corpus Dataset for Translating Spoken Mathematical Expressions into $LaTeX$ Formulas for Improved Readability'},\n",
       "  {'paperId': '57c6ff9c0e02b733bdd26545d0ed73e8fc8d049e',\n",
       "   'title': 'Editable Fairness: Fine-Grained Bias Mitigation in Language Models'},\n",
       "  {'paperId': 'df299f73f2d17322db4cf8c2a8a02b3823a1b1c9',\n",
       "   'title': 'A Debiased Nearest Neighbors Framework for Multi-Label Text Classification'},\n",
       "  {'paperId': '22e8d20de01454552fa0d10abdc731f9ca147750',\n",
       "   'title': 'Self-attention with temporal prior: can we learn more from the arrow of time?'},\n",
       "  {'paperId': '6ab7b62e07132b0b7bd1a4d1c64400b3d622199b',\n",
       "   'title': 'Enhancing Complex Causality Extraction via Improved Subtask Interaction and Knowledge Fusion'},\n",
       "  {'paperId': 'f1961024ac4c1c32e0175223ec3635b4d1228aa8',\n",
       "   'title': 'Analysis of Argument Structure Constructions in a Deep Recurrent Language Model'},\n",
       "  {'paperId': '19a3628be4d49301f62aa49a3db19382068dc867',\n",
       "   'title': 'Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization'},\n",
       "  {'paperId': '7e05833d34eb3dd9a3a4d8eff8a83dfcef2f1804',\n",
       "   'title': 'Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement'},\n",
       "  {'paperId': 'a370077174ccae0636fb93df242e859369d1f141',\n",
       "   'title': 'Topic Modeling with Fine-tuning LLMs and Bag of Sentences'},\n",
       "  {'paperId': '00764884538ef7e1ef736706dfe9d5c417527899',\n",
       "   'title': 'Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations'},\n",
       "  {'paperId': 'caee5d9d976b52ad76ff52e48d782b7cc8183763',\n",
       "   'title': 'Training LLMs to Recognize Hedges in Spontaneous Narratives'},\n",
       "  {'paperId': 'fbeb068e367362892a8a01301ed9fd88ae1ec9fe',\n",
       "   'title': 'Adversarial Robustness of Open-source Text Classification Models and Fine-Tuning Chains'},\n",
       "  {'paperId': 'acc90a192b06f0dcc9d3a0d3931c4449b51d8133',\n",
       "   'title': 'An Analysis of Automated Use Case Component Extraction from Scenarios using ChatGPT'},\n",
       "  {'paperId': '9b2e40370f115ae3c136fe089e1a6ea57f4f8018',\n",
       "   'title': 'ULLME: A Unified Framework for Large Language Model Embeddings with Generation-Augmented Learning'},\n",
       "  {'paperId': 'f2147150c580f1f83d11ad756b9532b423deb547',\n",
       "   'title': 'Open Brain AI and language assessment'},\n",
       "  {'paperId': 'e20ee5312ef3988bbbed13245b506a9c8cecfb53',\n",
       "   'title': 'Enhancing the design of voting advice applications with BERT language model'},\n",
       "  {'paperId': '2f625a39139551f037fe57088337e1f4e1264154',\n",
       "   'title': 'LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations'},\n",
       "  {'paperId': '615cab34abada51059e7bf812011aa18b664bad3',\n",
       "   'title': 'Explainable Graph Neural Networks: An Application to Open Statistics Knowledge Graphs for Estimating House Prices'},\n",
       "  {'paperId': '8e132a4ab0e8712fd7cead7361653f522a02cbe4',\n",
       "   'title': 'Enhancing Clinical Relevance of Pretrained Language Models Through Integration of External Knowledge: Case Study on Cardiovascular Diagnosis From Electronic Health Records'},\n",
       "  {'paperId': '964f761daa7f949fa9955f66a05cf6d728cb88e5',\n",
       "   'title': 'Occupation classification model based on DistilKoBERT: using the 5th and 6th Korean Working Condition Surveys'},\n",
       "  {'paperId': '9c4eba1c807077ca4b9a7e76634dede12da68cd4',\n",
       "   'title': 'FusionAraSA: Fusion-based Model for Accurate Arabic Sentiment Analysis'},\n",
       "  {'paperId': '2d4d410ab72fd58c75bc208f9c8f9400b9764325',\n",
       "   'title': 'Probing structural constraints of negation in Pretrained Language Models'},\n",
       "  {'paperId': '77eeca8a7a20397620e8addaf9f7136cba0314ed',\n",
       "   'title': 'A Metric Driven Approach to Mixed Precision Training'},\n",
       "  {'paperId': '134693c869402e06d1faea94e10303c790151fbc',\n",
       "   'title': 'BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba'},\n",
       "  {'paperId': 'bc2284cf2ad25daec489135e5b0c38cb8bd1a683',\n",
       "   'title': 'PTM4Tag+: Tag Recommendation of Stack Overflow Posts with Pre-trained Models'},\n",
       "  {'paperId': '987c5ab507b2fce644a9b73a8f8338c80cba944d',\n",
       "   'title': 'From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future'},\n",
       "  {'paperId': 'fb8221d52bddf161d063bbbe4df4979657eea1c4',\n",
       "   'title': 'Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models'},\n",
       "  {'paperId': '653d71b5c7971f87e9bdd12de407109947356850',\n",
       "   'title': 'Advancing Post-OCR Correction: A Comparative Study of Synthetic Data'},\n",
       "  {'paperId': '35d7b4da699a58e057eecdf257dfa00bdf40ddfb',\n",
       "   'title': 'To Aggregate or Not to Aggregate. That is the Question: A Case Study on Annotation Subjectivity in Span Prediction'},\n",
       "  {'paperId': 'ec407d42cf622115b0ecafd0e93f49d4e71c1071',\n",
       "   'title': 'Voice-Augmented Virtual Reality Interface for Serious Games'},\n",
       "  {'paperId': '513fa860159744f107efd99402d3400e78913b5d',\n",
       "   'title': 'Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection'},\n",
       "  {'paperId': '56398dc4e5ddc20c88f7d251e0643d878805a2a9',\n",
       "   'title': 'Command-line Obfuscation Detection using Small Language Models'},\n",
       "  {'paperId': '0939dfb686cd730f7203a2039a2bdceda0a00166',\n",
       "   'title': 'Stem-JEPA: A Joint-Embedding Predictive Architecture for Musical Stem Compatibility Estimation'},\n",
       "  {'paperId': '641594d32ee4b96ff20db56694f087f95e76c379',\n",
       "   'title': 'A Lean Transformer Model for Dynamic Malware Analysis and Detection'},\n",
       "  {'paperId': '2fa33ebb53176200cfb0b8b0cabc2814e559dcd6',\n",
       "   'title': 'Large Language Model Aided QoS Prediction for Service Recommendation'},\n",
       "  {'paperId': '6ab5459e36af84d4c7218a52a1d0213fbe2af933',\n",
       "   'title': 'From Generalist to Specialist: Exploring CWE-Specific Vulnerability Detection'},\n",
       "  {'paperId': '1d3b6ea8e077d2d16d4069936dabcd3ec43cbb9c',\n",
       "   'title': 'Contrastive Learning-based Multi Modal Architecture for Emoticon Prediction by Employing Image-Text Pairs'},\n",
       "  {'paperId': 'e5860d9fb7753fed0de097e37665a2a84a8f20c9',\n",
       "   'title': 'Steer-by-prior Editing of Symbolic Music Loops'},\n",
       "  {'paperId': 'bce77eaed016f94df57d8e720a45f936305c3eaa',\n",
       "   'title': 'FPT+: A Parameter and Memory Efficient Transfer Learning Method for High-resolution Medical Image Classification'},\n",
       "  {'paperId': '6d605af3466c12c5886b2d9ed1070f47a5558dd3',\n",
       "   'title': 'Curriculum learning based pre-training using Multi-Modal Contrastive Masked Autoencoders'},\n",
       "  {'paperId': '04cef426bd97666c6574f89b771605f3feb53dc0',\n",
       "   'title': 'A Multi-Source Heterogeneous Knowledge Injected Prompt Learning Method for Legal Charge Prediction'},\n",
       "  {'paperId': '6a5c05ed57bf4f76c2b81f06cfaa2984d597e66b',\n",
       "   'title': 'Infusing Emotions into Task-oriented Dialogue Systems: Understanding, Management, and Generation'},\n",
       "  {'paperId': 'dfc39c8d6e7850089572283efdf765bea516ef34',\n",
       "   'title': 'Improving the quality of Persian clinical text with a novel spelling correction system'},\n",
       "  {'paperId': 'e88d624a4d50940bb9b8137ff8cc4fdb17794d72',\n",
       "   'title': 'Development of REGAI: Rubric Enabled Generative Artificial Intelligence'},\n",
       "  {'paperId': '91e448f9c8b808bbd00443fb8e6bf3dd4f04b025',\n",
       "   'title': 'From Recognition to Prediction: Leveraging Sequence Reasoning for Action Anticipation'},\n",
       "  {'paperId': '1465f3bb4ab048e3171499dafe79484068c76413',\n",
       "   'title': 'Entity Retrieval for Answering Entity-Centric Questions'},\n",
       "  {'paperId': '69ccce44213a79c679ccde2615c5dfb1fa4f6406',\n",
       "   'title': 'Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream Machine Learning Services'},\n",
       "  {'paperId': '0d23ca8aa1f6e718483a630d39dd310d750f2c10',\n",
       "   'title': 'Dataset of miRNA–disease relations extracted from textual data using transformer-based neural networks'},\n",
       "  {'paperId': 'fb59b372e1194538d4a69599dd3c487c6568a3cb',\n",
       "   'title': 'IQAGPT: computed tomography image quality assessment with vision-language and ChatGPT models'},\n",
       "  {'paperId': '4d8b082ec9f5663e79312faca2fee9e7e358ecc1',\n",
       "   'title': 'An improved data augmentation approach and its application in medical named entity recognition'},\n",
       "  {'paperId': '5edc5c9ef281534ce0fed98908a40dcb94595837',\n",
       "   'title': 'Mixture-of-languages Routing for Multilingual Dialogues'},\n",
       "  {'paperId': '6370c6bf8aa48d385b22508f176ee244a0302b71',\n",
       "   'title': 'Recent Advances in Multi-Choice Machine Reading Comprehension: A Survey on Methods and Datasets'},\n",
       "  {'paperId': '51f0c3b06861c79fbb283022df109818ce4ebe51',\n",
       "   'title': 'Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models'},\n",
       "  {'paperId': '07695ee0ee340be7e31cc3b4c909c4c5487282a6',\n",
       "   'title': 'Open Technology Management for Maximizing the Public Value of Large Language Models'},\n",
       "  {'paperId': 'c8e2a2514995d9d169afcee45bbc93dfa3f89bf8',\n",
       "   'title': 'Cross-layer Attention Sharing for Large Language Models'},\n",
       "  {'paperId': 'a88353f9070c4e9ba63e410b351f193728eb21ef',\n",
       "   'title': 'Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process'},\n",
       "  {'paperId': '4e90cfeafa150995319e6964a0beff201f5cd503',\n",
       "   'title': 'Towards Automatic Hands-on-Keyboard Attack Detection Using LLMs in EDR Solutions'},\n",
       "  {'paperId': '50f03453c11b2a224545a7fbe223594014bca902',\n",
       "   'title': 'Past Movements-Guided Motion Representation Learning for Human Motion Prediction'},\n",
       "  {'paperId': 'f5ab74836bf660f69797de667a9feea8a31327cd',\n",
       "   'title': 'Visual Grounding for Object-Level Generalization in Reinforcement Learning'},\n",
       "  {'paperId': '20aba65648abbde5dc5c777fd6eaa2b6de13be16',\n",
       "   'title': 'Fine-tuning multilingual language models in Twitter/X sentiment analysis: a study on Eastern-European V4 languages'},\n",
       "  {'paperId': '65010f7d68c06ccd00f0d94e8622224ba4fc7d30',\n",
       "   'title': 'LEGO: Self-Supervised Representation Learning for Scene Text Images'},\n",
       "  {'paperId': '89c0bb9c733b83a9f964e267005faa55c64bb247',\n",
       "   'title': 'PRIME-DP: Pre-trained Integrated Model for Earthquake Data Processing'},\n",
       "  {'paperId': 'bfefaae8618613cc7d6e4938f8b311cb98815098',\n",
       "   'title': 'The Implications of Open Generative Models in Human-Centered Data Science Work: A Case Study with Fact-Checking Organizations'},\n",
       "  {'paperId': '57e04753439f1babc6e0ee87b74d98d431be95ca',\n",
       "   'title': 'Annual Pharmaceutical Sales Estimates Using Patents'},\n",
       "  {'paperId': '17e11e5d8064c3ecf1059a0cd3730711a443cf14',\n",
       "   'title': 'The New Era of Knowledge Retrieval: Multi-Agent Systems Meet Generative AI'},\n",
       "  {'paperId': 'd5af10e4e23c70b6af5e99f2b374ba15bb447e8d',\n",
       "   'title': 'Artificial Intelligence Applications in Ionospheric Irregularities: A Bibliometric Analysis'},\n",
       "  {'paperId': '52b47f1418eca80bb6aedeb489978224f5c40b8f',\n",
       "   'title': 'Transforming Slot Schema Induction with Generative Dialogue State Inference'},\n",
       "  {'paperId': '58cade16b9e5991583c49f476fc5421e744a13d1',\n",
       "   'title': 'CoEdPilot: Recommending Code Edits with Learned Prior Edit Relevance, Project-wise Awareness, and Interactive Nature'},\n",
       "  {'paperId': '7eb3e07ac68dae9c83bdbdbc79c6951893bf7bc6',\n",
       "   'title': \"A transformer-based unified multimodal framework for Alzheimer's disease assessment.\"},\n",
       "  {'paperId': '4be1a84166220aa7db32284140017e7bea3b370d',\n",
       "   'title': 'Stimulating Imagination: Towards General-purpose Object Rearrangement'},\n",
       "  {'paperId': 'ebe9756678c804f9d2d0e053354cee5e2bffa073',\n",
       "   'title': 'MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented Generation for Pharmacovigilance'},\n",
       "  {'paperId': '24c7e6b41ddae4c1fb7e52bd8b30c08fbb33a648',\n",
       "   'title': 'Efficacy of Large Language Models in Systematic Reviews'},\n",
       "  {'paperId': 'bfeda55ff8ea6abaef6b71a91a38f7ca95c70e42',\n",
       "   'title': 'Distinguishing Chatbot from Human'},\n",
       "  {'paperId': '3adf28700073128ae971c9561126ce96bdf0ba69',\n",
       "   'title': 'Topic Modeling of the 2024 Election Using the BERTopic Method on Detik.com News Articles'},\n",
       "  {'paperId': '1708622a06f2b21b9ea21874853ded6b81ba5e0f',\n",
       "   'title': 'Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models'},\n",
       "  {'paperId': '8a9dc173a27c7522965a72411ef951d1ae2d38d6',\n",
       "   'title': 'Piculet: Specialized Models-Guided Hallucination Decrease for MultiModal Large Language Models'},\n",
       "  {'paperId': 'b80693b6402eae6b945c606cef7b9d4dc7e1bcad',\n",
       "   'title': 'Deep Learning based Visually Rich Document Content Understanding: A Survey'},\n",
       "  {'paperId': 'fd6e8cd6aeadb12b861cc360bd5413fae221de15',\n",
       "   'title': 'PETA: evaluating the impact of protein transfer learning with sub-word tokenization on downstream applications'},\n",
       "  {'paperId': '871da4d678c79ce520f0b7ec5a5631660dfa436a',\n",
       "   'title': 'SaprotHub: Making Protein Modeling Accessible to All Biologists'},\n",
       "  {'paperId': '2a2b877b9cd3677692d92d96848fb0e9af7c3cd1',\n",
       "   'title': 'Multisource information fusion method for vegetable disease detection'},\n",
       "  {'paperId': 'fc6141aa20c194eeba62d231ce35dbb74e65bf74',\n",
       "   'title': 'Automatic exploit assessment based on deep learning methods'},\n",
       "  {'paperId': '7e1c6b03f446e9975ee2c7df0be266e363d5d6a1',\n",
       "   'title': 'BFNet: a full-encoder skip connect way for medical image segmentation'},\n",
       "  {'paperId': '1378e662cf9b67e44a0b04025dba72878707e01e',\n",
       "   'title': 'Enhancing Financial Market Predictions: Causality-Driven Feature Selection'},\n",
       "  {'paperId': '642014b01529a396cda620e59fefde7ac92b2cb8',\n",
       "   'title': 'An Encoding--Searching Separation Perspective on Bi-Encoder Neural Search'},\n",
       "  {'paperId': 'ec29f2eecd1ae07a4318398fba1fd76e2828c1f0',\n",
       "   'title': 'Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with Accelerated LLMs'},\n",
       "  {'paperId': '74b1e2a9a67361f0cbf6b3a83b67b6a1bc1a04a6',\n",
       "   'title': 'PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy Correspondence Learning in Cross-Modal Retrieval'},\n",
       "  {'paperId': '9a7ef90b3b85e4572c928b88d73f8abfa0736f72',\n",
       "   'title': 'UNER: A Unified Prediction Head for Named Entity Recognition in Visually-rich Documents'},\n",
       "  {'paperId': 'ea212b068d291294aba25bd1956f32dc140d9bde',\n",
       "   'title': 'High-Throughput Phenotyping of Clinical Text Using Large Language Models'},\n",
       "  {'paperId': 'f8436c8ccfe6a9fe2a79d84918724b0587f01a13',\n",
       "   'title': 'Coalitions of Large Language Models Increase the Robustness of AI Agents'},\n",
       "  {'paperId': '5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996',\n",
       "   'title': 'A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks'},\n",
       "  {'paperId': 'd888e46803ff5365f089f514719d1036d13e3417',\n",
       "   'title': 'Privacy-Preserving Split Learning with Vision Transformers using Patch-Wise Random and Noisy CutMix'},\n",
       "  {'paperId': 'e125b9231ed304e7cbae4e264216b7d6930cb8e2',\n",
       "   'title': 'An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding'},\n",
       "  {'paperId': '2470e4bebe8efd779964ef5799413dbc8f4c9907',\n",
       "   'title': 'Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding'},\n",
       "  {'paperId': '160db59d6ccca58930d02c95cc5cd0739d3252b7',\n",
       "   'title': 'Fairness in Large Language Models in Three Hours'},\n",
       "  {'paperId': 'a4443b55c37bc7b93c4f85f1cd47ae59f91817b3',\n",
       "   'title': 'Contribution-based Low-Rank Adaptation with Pre-training Model for Real Image Restoration'},\n",
       "  {'paperId': 'f6737a9039ff8661feea36ed8c8e4fb6ea982b7b',\n",
       "   'title': 'META-ANOVA: Screening interactions for interpretable machine learning'},\n",
       "  {'paperId': 'a64397f561482f1717abdddd7c35f0db1296e3fc',\n",
       "   'title': 'VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling'},\n",
       "  {'paperId': '71f809ba13de0035a259cee8903b5552baf160a3',\n",
       "   'title': 'Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting'},\n",
       "  {'paperId': '727be807bd7a832b902c4f1c861885f96fcd3394',\n",
       "   'title': 'Actra: Optimized Transformer Architecture for Vision-Language-Action Models in Robot Learning'},\n",
       "  {'paperId': '29ed78ea73d88dbc2c7f89368bd9e8db7882913a',\n",
       "   'title': 'NOLO: Navigate Only Look Once'},\n",
       "  {'paperId': '59b988fda9c1737465921a9bade731d511500718',\n",
       "   'title': 'The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability'},\n",
       "  {'paperId': '8613f4041024f3a6105d743019ab594b90f3aa4f',\n",
       "   'title': 'Leveraging Large Language Models for Mobile App Review Feature Extraction'},\n",
       "  {'paperId': 'ebc1496820fc440c8363a060714969bd3fa94cbe',\n",
       "   'title': 'SceneMotion: From Agent-Centric Embeddings to Scene-Wide Forecasts'},\n",
       "  {'paperId': '54aae7bab2e1339c92fd03b8c0beae0f8adc0334',\n",
       "   'title': 'Distillation vs. Sampling for Efficient Training of Learning to Rank Models'},\n",
       "  {'paperId': '3601bf5c1b852238d07bdda357f156b6ef58b007',\n",
       "   'title': 'Towards Trustworthy AI-Enabled Decision Support Systems: Validation of the Multisource AI Scorecard Table (MAST)'},\n",
       "  {'paperId': '505b5cce3acf8625201d6270576230fc27269e55',\n",
       "   'title': 'Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors for a Robotics Course'},\n",
       "  {'paperId': 'ec13de165371881269a68049b40b0528795b421e',\n",
       "   'title': 'Tc-llama 2: fine-tuning LLM for technology and commercialization applications'},\n",
       "  {'paperId': 'a28589f69de28e3e69925be47fdabcc01ac1ac73',\n",
       "   'title': 'Cross-domain Named Entity Recognition via Graph Matching'},\n",
       "  {'paperId': '8e1ac8ba624ba056348257b99d21ae71231c5930',\n",
       "   'title': 'gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling (Extended Abstract)'},\n",
       "  {'paperId': 'fca1ad2e38d3c6784b3eab0629a8e62df8698e3a',\n",
       "   'title': 'A Multidimensional Communication Scheduling Method for Hybrid Parallel DNN Training'},\n",
       "  {'paperId': '0b8e758cb9f938787597a668506435dfcafe3a67',\n",
       "   'title': 'Downstream bias mitigation is all you need'},\n",
       "  {'paperId': 'b4617f1ea72793c6ab50839cf97fa30101641df4',\n",
       "   'title': 'Prompt Learning with Extended Kalman Filter for Pre-trained Language Models'},\n",
       "  {'paperId': 'f58ca2439b60dc91b8398af2b253a4e7794aab28',\n",
       "   'title': 'Multi-TA: Multilevel Temporal Augmentation for Robust Septic Shock Early Prediction'},\n",
       "  {'paperId': '5a93ccde5fc2dc39e306d4519fc54ed63bc63125',\n",
       "   'title': 'Fake News Detection Using Enhanced BERT'},\n",
       "  {'paperId': '1de6a65a49b35ef716545b238e47f9637c7631c1',\n",
       "   'title': 'Confidence-Driven Contrastive Learning for Document Classification without Annotated Data'},\n",
       "  {'paperId': '076477b1e35da20c3eb0153183a966ff2710d7cd',\n",
       "   'title': 'CoDA: A Co-Design Framework for Versatile and Efficient Attention Accelerators'},\n",
       "  {'paperId': '46d63c69d76c1c84ea237f4799ee5789835e9535',\n",
       "   'title': 'Disentangling Dense Embeddings with Sparse Autoencoders'},\n",
       "  {'paperId': 'e0f920d5fa96239ce474c48fd30147d0d7b0be67',\n",
       "   'title': 'Automatic Pull Request Description Generation Using LLMs: A T5 Model Approach'},\n",
       "  {'paperId': '896fafb06761a15f5ea2be2f3b91ef68e6767bb5',\n",
       "   'title': 'BertSNR: an interpretable deep learning framework for single-nucleotide resolution identification of transcription factor binding sites based on DNA language model'},\n",
       "  {'paperId': '75d4eb50ee359e80a2b45c90ed7486f832b68d04',\n",
       "   'title': 'A Literature Review on Detecting, Verifying, and Mitigating Online Misinformation'},\n",
       "  {'paperId': '09030ab6ab8ef3e680f5b00e428f43d6185f355b',\n",
       "   'title': 'Incorporating external knowledge for text matching model'},\n",
       "  {'paperId': '3be79f2987e5f6ea290f161b39d5631f9ccce45f',\n",
       "   'title': 'Zero-shot stance detection based on multi-perspective transferable feature fusion'},\n",
       "  {'paperId': '28d90877934cf2257e65a2c92a954aedba205eb1',\n",
       "   'title': 'Transdisciplinary fine-grained citation content analysis: A multi-task learning perspective for citation aspect and sentiment classification'},\n",
       "  {'paperId': 'eb553e827d242c6a40498eac838bb6e22978c194',\n",
       "   'title': 'Knowledge is power: Open-world knowledge representation learning for knowledge-based visual reasoning'},\n",
       "  {'paperId': 'f6a67132d2f22dedcdaab07d821667aaca9d74f9',\n",
       "   'title': 'Evaluating NLP models with written and spoken L2 samples'},\n",
       "  {'paperId': 'bfbed088f1bdb0ba15a6994f78896547b0937afd',\n",
       "   'title': 'Image Retrieval with Self-Supervised Divergence Minimization and Cross-Attention Classification'},\n",
       "  {'paperId': '32d95d741f5ab10c3159167d274ebd61dceb17b7',\n",
       "   'title': 'Designing a deep learning-based application for detecting fake online reviews'},\n",
       "  {'paperId': 'e22df6ef067a7663c9fcc46c4e7c2510e8eec8c3',\n",
       "   'title': 'Email subjects generation with large language models: GPT-3.5, PaLM 2, and BERT'},\n",
       "  {'paperId': '848a049060f1228fb4428c20841dafa424390d81',\n",
       "   'title': 'A comprehensive review on transformer network for natural and medical image analysis'},\n",
       "  {'paperId': '42f203647a9a9de04e58b6c55d00c1dc71af70a5',\n",
       "   'title': 'Extending Binary Neural Networks to Bayesian Neural Networks with Probabilistic Interpretation of Binary Weights'},\n",
       "  {'paperId': 'f3109b17462d5bf1bb00204526293df7818408a1',\n",
       "   'title': 'Cross-modal feature learning and alignment network for text-image person re-identification'},\n",
       "  {'paperId': '12a8d575f06abb688c5ae3570451431c1ee9db0f',\n",
       "   'title': 'Context-Aware Commonsense Knowledge Graph Reasoning With Path-Guided Explanations'},\n",
       "  {'paperId': '60c129762be8a25c7fae2f2ec8e5015880832d11',\n",
       "   'title': 'TensorMap: A Deep RL-Based Tensor Mapping Framework for Spatial Accelerators'},\n",
       "  {'paperId': '68f81c9aa9de699bddae940fabb4b6c8dbb005c5',\n",
       "   'title': 'Supporting Your Idea Reasonably: A Knowledge-Aware Topic Reasoning Strategy for Citation Recommendation'},\n",
       "  {'paperId': '487316130a8fe882948b92f4bbe163be50d288be',\n",
       "   'title': 'Proxy-Based Graph Convolutional Hashing for Cross-Modal Retrieval'},\n",
       "  {'paperId': 'c114a30f40c2928e925835769bb87794d6085a99',\n",
       "   'title': 'BERT-Based Semantic-Aware Heterogeneous Graph Embedding Method for Enhancing App Usage Prediction Accuracy'},\n",
       "  {'paperId': '92a7da0df9e1c2383a5f90954f17a297400c2ded',\n",
       "   'title': 'Parmesan: Efficient Partitioning and Mapping Flow for DNN Training on General Device Topology'},\n",
       "  {'paperId': '58367a57c8aba01df7f962e06b6b5cf827f6ff13',\n",
       "   'title': 'The Dimensions of dimensionality'},\n",
       "  {'paperId': '0496f01a428e96c1e65843215964138806226225',\n",
       "   'title': 'Graph Structure Enhanced Pre-Training Language Model for Knowledge Graph Completion'},\n",
       "  {'paperId': '4e54f8ecb383555c9d3b8b102adb69e13d969754',\n",
       "   'title': 'Parameter-Efficient Transfer Learning for Medical Visual Question Answering'},\n",
       "  {'paperId': 'a6fd3967b18c10d14da689b2459d0cfbc597224f',\n",
       "   'title': 'A Semantics-Based Approach on Binary Function Similarity Detection'},\n",
       "  {'paperId': '1e2c0657649290c6a6a3f199d34e1386aff561dc',\n",
       "   'title': 'Scalable Heterogeneous Scheduling Based Model Parallelism for Real-Time Inference of Large-Scale Deep Neural Networks'},\n",
       "  {'paperId': '2a65917dd758f72a96e71fdea754c65de36e75f0',\n",
       "   'title': 'SP-PIM: A Super-Pipelined Processing-In-Memory Accelerator With Local Error Prediction for Area/Energy-Efficient On-Device Learning'},\n",
       "  {'paperId': '48263812d56f3af9978ff191e23de5cd6fc2f868',\n",
       "   'title': 'News-MESI: A Dataset for Multimodal News Excerpt Segmentation and Identification'},\n",
       "  {'paperId': '1f0610e3c39d0128d18dc3eb34f6e309ebad26f6',\n",
       "   'title': 'Sentiment Analysis of YouTube Users on Blackpink Kpop Group Using IndoBERT'},\n",
       "  {'paperId': '8877d3da0e5ebfa84026f735bdaca78e399bd45a',\n",
       "   'title': 'A Multiobjective Genetic Algorithm to Evolving Local Interpretable Model-Agnostic Explanations for Deep Neural Networks in Image Classification'},\n",
       "  {'paperId': '5f3b5ea06487f1936724b82a1c671adc82aeb9fa',\n",
       "   'title': 'Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning'},\n",
       "  {'paperId': '5b53fa35349e9391116fabddaaaf23e23a343938',\n",
       "   'title': 'Intermittent Semi-working Mask: A New Masking Paradigm for LLMs'},\n",
       "  {'paperId': '1c80e8bf4606c7a4b7a91a10faaab3fb82a3ed91',\n",
       "   'title': 'Enhanced Structured State Space Models via Grouped FIR Filtering and Attention Sink Mechanisms'},\n",
       "  {'paperId': '71afe0e90e04ae84bb8fa55e5edb79504a486bd1',\n",
       "   'title': 'Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network'},\n",
       "  {'paperId': 'f631ff4f13b6b85f67d06f5c050c5ef270f89267',\n",
       "   'title': 'Text-Guided Video Masked Autoencoder'},\n",
       "  {'paperId': 'fd0739994ce8ab862ce16b57806d22640d9d9721',\n",
       "   'title': 'Adversarial Text Rewriting for Text-aware Recommender Systems'},\n",
       "  {'paperId': 'dcb6173ae827bfd1b88c2604c45e92828fee51f2',\n",
       "   'title': 'Ontological Relations from Word Embeddings'},\n",
       "  {'paperId': 'cec3d58e34c298f51e22cb01e71e3a8f5396e3e4',\n",
       "   'title': 'MotionFix: Text-Driven 3D Human Motion Editing'},\n",
       "  {'paperId': '2c3abddf25904387f29dbf3ba730cc85c640531d',\n",
       "   'title': 'Are Bigger Encoders Always Better in Vision Large Models?'},\n",
       "  {'paperId': '5091046cdf4cfd7c3e1330f5ad7c2f3d911f0726',\n",
       "   'title': 'Block-Operations: Using Modular Routing to Improve Compositional Generalization'},\n",
       "  {'paperId': '8e56cb8f1089b80a5ab9dbbe7f5432896037137d',\n",
       "   'title': 'GalleryGPT: Analyzing Paintings with Large Multimodal Models'},\n",
       "  {'paperId': '822f94dadb65d7cc0ab680efd65301aa26cd09f8',\n",
       "   'title': 'ReSi: A Comprehensive Benchmark for Representational Similarity Measures'},\n",
       "  {'paperId': '5d1bfee024f90a8aa9d5d01c1ed1708dfed693f3',\n",
       "   'title': 'What comes after transformers? - A selective survey connecting ideas in deep learning'},\n",
       "  {'paperId': '490ef757c3b9c92250d68eab72eaf0f777404869',\n",
       "   'title': 'Leveraging Entailment Judgements in Cross-Lingual Summarisation'},\n",
       "  {'paperId': '8cee54df89130080164d7d71ee286e67c065300b',\n",
       "   'title': 'Towards Reliable Advertising Image Generation Using Human Feedback'},\n",
       "  {'paperId': 'e3b56937dcc2f437febb2fdedd3535b93dffd153',\n",
       "   'title': 'A new approach for encoding code and assisting code understanding'},\n",
       "  {'paperId': 'ab947248942ec3c59646b7cb3b7b9b2376c14ffc',\n",
       "   'title': 'Pre-training General User Representation with Multi-type APP Behaviors'},\n",
       "  {'paperId': '0c5df2135ad1a11b12c8e7ad37a59b655aab7f45',\n",
       "   'title': 'Upgrading Search Applications in the Era of LLMs: A Demonstration with Practical Lessons'},\n",
       "  {'paperId': '5c9c31685612edbed00d5fa63df3f6a5b00e7b74',\n",
       "   'title': 'A Goal-Directed Dialogue System for Assistance in Safety-Critical Application'},\n",
       "  {'paperId': '55cf4ce01ebd529ba3bd2551acb5f0e74a68212a',\n",
       "   'title': 'Span-based Unified Named Entity Recognition Framework via Contrastive Learning'},\n",
       "  {'paperId': '0f9f38c2274b3693600c0f00d57cd5d93281a4c0',\n",
       "   'title': 'Transformer-Based Reinforcement Learning for Scalable Multi-UAV Area Coverage'},\n",
       "  {'paperId': 'fab802f85167269f5219ac51a34fe523ee65b5e1',\n",
       "   'title': 'Modeling Selective Feature Attention for Lightweight Text Matching'},\n",
       "  {'paperId': '7cbb1192f9598bc8bff756994e64fa8b54fda0a3',\n",
       "   'title': 'A Semi-supervised Molecular Learning Framework for Activity Cliff Estimation'},\n",
       "  {'paperId': 'f0adf3ea29845dd327e11e919e38ae4cb044e05e',\n",
       "   'title': 'Balancing Multimodal Learning via Online Logit Modulation'},\n",
       "  {'paperId': 'ff010246d80a5470414f2fbb9d73b4e84af3074b',\n",
       "   'title': 'MNAT-Net: Multi-Scale Neighborhood Aggregation Transformer Network for Point Cloud Classification and Segmentation'},\n",
       "  {'paperId': '70c94141b45dbf2c27a9578b482c443861d9c8e3',\n",
       "   'title': 'Attention Based Document-level Relation Extraction with None Class Ranking Loss'},\n",
       "  {'paperId': '750b1f1030021bd6bb9049e4529695f5f57a16be',\n",
       "   'title': 'Making LLMs as Fine-Grained Relation Extraction Data Augmentor'},\n",
       "  {'paperId': 'd0c13530d710e6af2a53153ece80994268066167',\n",
       "   'title': 'A K-Shape Clustering Based Transformer-Decoder Model for Predicting Multi-Step Potentials of Urban Mobility Field'},\n",
       "  {'paperId': 'fae5cf850f27546189a2c482b4e2fb345da2efb1',\n",
       "   'title': 'QFormer: An Efficient Quaternion Transformer for Image Denoising'},\n",
       "  {'paperId': 'b8d77a0ce1a2d638606161209e14052257a2ee23',\n",
       "   'title': 'Heterogeneous Graph Transformer with Poly-Tokenization'},\n",
       "  {'paperId': '770469113c01b056bbfe544693f553172e005c67',\n",
       "   'title': 'The Orthogonality of Weight Vectors: The Key Characteristics of Normalization and Residual Connections'},\n",
       "  {'paperId': 'abeb4d63dc0c8744bd61bec3e10d07d2cc7216a0',\n",
       "   'title': 'An LLM-enhanced Agent-based Simulation Tool for Information Propagation'},\n",
       "  {'paperId': '2ac13d53e452e6d6c24a6176c6fbeac0846ac5a0',\n",
       "   'title': 'Enhancing Length Generalization for Attention Based Knowledge Tracing Models with Linear Biases'},\n",
       "  {'paperId': '07dd8063c615981a81b15030107a8d977594688c',\n",
       "   'title': 'Exploring Urban Semantics: A Multimodal Model for POI Semantic Annotation with Street View Images and Place Names'},\n",
       "  {'paperId': '3f5682337717d200435d7c5c30edbb476a1471b6',\n",
       "   'title': 'PDENNEval: A Comprehensive Evaluation of Neural Network Methods for Solving PDEs'},\n",
       "  {'paperId': '6273798c1c8dd6bb8f0abb39c5b7049463423d81',\n",
       "   'title': 'Learning Label Dependencies for Visual Information Extraction'},\n",
       "  {'paperId': '49595bb422ca750963c2e1d9ea906c1fa7554c5d',\n",
       "   'title': 'Attention Shifting to Pursue Optimal Representation for Adapting Multi-granularity Tasks'},\n",
       "  {'paperId': 'c9f072f768a1b6c140bc9ecf435f0d2f11799c2b',\n",
       "   'title': 'AK4Prompts: Aesthetics-driven Automatically Keywords-Ranking for Prompts in Text-To-Image Models'},\n",
       "  {'paperId': '90dac08a440f0671daf2aeaec85528bbb1adce4c',\n",
       "   'title': 'Graph Collaborative Expert Finding with Contrastive Learning'},\n",
       "  {'paperId': '731bd174ea343c0e658b02cdf354a185f65f4886',\n",
       "   'title': 'Multi-Modal Sarcasm Detection Based on Dual Generative Processes'},\n",
       "  {'paperId': 'bbded8533f8bff09988af68302e52dc0a9f6525d',\n",
       "   'title': 'Automated Essay Scoring: Recent Successes and Future Directions'},\n",
       "  {'paperId': 'edd903ba5b58d707280dbf5bb6d46ae344b91468',\n",
       "   'title': 'Learning Hierarchy-Enhanced POI Category Representations Using Disentangled Mobility Sequences'},\n",
       "  {'paperId': 'b9223567dc5cd355c15b693f4bc1b3aeb913b9dc',\n",
       "   'title': 'Diffusion Mask-Driven Visual-language Tracking'},\n",
       "  {'paperId': 'bb1381c9944e94f6f8ca3ac8d38519e0c4cdd8be',\n",
       "   'title': 'Enhancing Multimodal Knowledge Graph Representation Learning through Triple Contrastive Learning'},\n",
       "  {'paperId': '196328beb9372cbe77064b52b4c75d3c79cd7231',\n",
       "   'title': 'KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph'},\n",
       "  {'paperId': 'cde11c5ead9a72c9c964a058d8277369f81b4bf5',\n",
       "   'title': 'Prompt-enhanced Network for Hateful Meme Classification'},\n",
       "  {'paperId': 'e7373192e3c3277dc2d058d9ba25c19c722df2fb',\n",
       "   'title': 'An NCDE-based Framework for Universal Representation Learning of Time Series'},\n",
       "  {'paperId': '7647410aae79510248c12e70ae09eca7d5239f46',\n",
       "   'title': 'Cross-modal Generation and Alignment via Attribute-guided Prompt for Unsupervised Text-based Person Retrieval'},\n",
       "  {'paperId': '7c1c800370b3e7f4cede749168195e26f446c4ee',\n",
       "   'title': 'Contrastive Transformer Cross-Modal Hashing for Video-Text Retrieval'},\n",
       "  {'paperId': '3eb65c912db29e01e292515e67b37d69ba4d23a7',\n",
       "   'title': 'FineFMPL: Fine-grained Feature Mining Prompt Learning for Few-Shot Class Incremental Learning'},\n",
       "  {'paperId': '9491cf4c97a4cadc60e4acaea0bec634812e24ff',\n",
       "   'title': 'SCTrans: Multi-scale scRNA-seq Sub-vector Completion Transformer for Gene-selective Cell Type Annotation'},\n",
       "  {'paperId': '7aa66b78c1230770937898c165ae9efd717dba7b',\n",
       "   'title': 'CoAtFormer: Vision Transformer with Composite Attention'},\n",
       "  {'paperId': '282818d462591250e084a8469d1997c0836bf7d6',\n",
       "   'title': 'Dialogue Cross-Enhanced Central Engagement Attention Model for Real-Time Engagement Estimation'},\n",
       "  {'paperId': '2c0b5ad3d525a2669f05a35e68ffb36c77f8fcf4',\n",
       "   'title': 'Peptide Sequencing Via Protein Language Models'},\n",
       "  {'paperId': '12959b619828731d03f57653b561cd84c7020f9a',\n",
       "   'title': 'Modeling Personalized Retweeting Behaviors for Multi-Stage Cascade Popularity Prediction'},\n",
       "  {'paperId': 'ea1cc40cc0e3816e921f48fb3a07195117ad7374',\n",
       "   'title': 'Contrastive Representation Learning for Self-Supervised Taxonomy Completion'},\n",
       "  {'paperId': '62917f180f12405293773a5f3689236727699caf',\n",
       "   'title': 'Skip-Timeformer: Skip-Time Interaction Transformer for Long Sequence Time-Series Forecasting'},\n",
       "  {'paperId': '5a257f6a4d4ad58ea7ac15cfc0b61b46384e7061',\n",
       "   'title': 'Do the best of all together: Hierarchical spatial-frequency fusion transformers for animal re-identification'},\n",
       "  {'paperId': '44a035a5eaa2a075e722f23e97e915ae514766ab',\n",
       "   'title': \"Fact Check: Assessing the Response of ChatGPT to Alzheimer's Disease Myths.\"},\n",
       "  {'paperId': 'e8be6548e3a194dfd16fdca7b3bbfba9f669bbad',\n",
       "   'title': 'Generate Synthetic Text Approximating the Private Distribution with Differential Privacy'},\n",
       "  {'paperId': '35d2827f04f248631fb9d93f65319985d60514b1',\n",
       "   'title': 'Using Large Language Models to Improve Query-based Constraint Acquisition'},\n",
       "  {'paperId': '03b0e16a5cb30f0e350e8b5d5bc153e7fb2db037',\n",
       "   'title': 'UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation'},\n",
       "  {'paperId': 'bbafa7e0dae95ec44179cac3cdcbf0e69f9a4bb3',\n",
       "   'title': 'Convergence Analysis of Natural Gradient Descent for Over-parameterized Physics-Informed Neural Networks'},\n",
       "  {'paperId': '46c2e08057ab959faa51277df255c61f8823eea7',\n",
       "   'title': 'An empirical study on divergence of differently-sourced LLVM IRs'},\n",
       "  {'paperId': 'e0234eb192ddbc697ab78de0e453650c73a08727',\n",
       "   'title': 'Enhancing IMF Economics Training: AI-Powered Analysis of Qualitative Learner Feedback'},\n",
       "  {'paperId': '75e892811b44222ff23b8df8a0cba26b1a0b33be',\n",
       "   'title': 'SentenceVAE: Enable Next-sentence Prediction for Large Language Models with Faster Speed, Higher Accuracy and Longer Context'},\n",
       "  {'paperId': '87b1a0987998f97bb953e341ddc32591fe2aa0aa',\n",
       "   'title': 'Dual-enhanced generative model with graph attention network and contrastive learning for aspect sentiment triplet extraction'},\n",
       "  {'paperId': '2af68cdcefdbd3795f9d51e8e8c291bdfde08923',\n",
       "   'title': 'Scalable, explainable, adaptive information extraction from structure-aware nearest neighbor'},\n",
       "  {'paperId': '974493028cc171544d96db1cf2866c24f43b07a9',\n",
       "   'title': 'Automatic language ability assessment method based on natural language processing'},\n",
       "  {'paperId': '0f125d94462a419c095e1dea8b661801be1d915a',\n",
       "   'title': 'Why concepts are (probably) vectors'},\n",
       "  {'paperId': '387960e19804cb2c291ef359c634f07eff59b21a',\n",
       "   'title': 'A Multi-Embedding Fusion Network for attributed graph clustering'},\n",
       "  {'paperId': '5243f7164fe25baa981c76d924bdb5bef843865d',\n",
       "   'title': 'Addressing class-imbalance challenges in cross-lingual aspect-based sentiment analysis: Dynamic weighted loss and anti-decoupling'},\n",
       "  {'paperId': 'c5fc91510c3b5de818d9189fbe7df54dbf4a4aee',\n",
       "   'title': 'Syntactic Modeling and Neural-Based Parsing for Multifunction Radar Signal Interpretation'},\n",
       "  {'paperId': '8fa48085b885d37ebf1c57b8f09184c2d2dead94',\n",
       "   'title': 'Enhancing Customer Satisfaction Analysis Using Advanced Machine Learning Techniques in Fintech Industry'},\n",
       "  {'paperId': 'afa50e73f0d63e2b97bf3ba0d3eadc55236b6acb',\n",
       "   'title': 'A method for phase estimation of X-ray pulsar signals: combining a transformer network structure and a two-dimensional profile map'},\n",
       "  {'paperId': '4549cec42e4cc945e133123b0cf9c128463f3faf',\n",
       "   'title': 'Adapting the Segment Anything Model for Multi-modal Retinal Anomaly Detection and Localization'},\n",
       "  {'paperId': '3cce585d86f66beeffd23737ea9453af54ea33d6',\n",
       "   'title': 'Improved FasterViT Model for Citrus Disease Diagnosis'},\n",
       "  {'paperId': 'e0445e3afb65c9f770c1e9465885c8882c3e9013',\n",
       "   'title': 'LGR-NET: Language Guided Reasoning Network for Referring Expression Comprehension'},\n",
       "  {'paperId': '074da752a4fe8493d267c71a8f88e93ca5e34b27',\n",
       "   'title': 'Self-Supervised Forecasting in Electronic Health Records With Attention-Free Models'},\n",
       "  {'paperId': 'b6179430541931e4c5c4f8902ce727f02e951aef',\n",
       "   'title': 'DTCM: Deep Transformer Capsule Mutual Distillation for Multivariate Time Series Classification'},\n",
       "  {'paperId': '5ce20a1c2466d2d4c9efe778a309fa8d33b40b94',\n",
       "   'title': 'Rate-Distortion Optimized Cross Modal Compression With Multiple Domains'},\n",
       "  {'paperId': '537ae23f255ffcef0c7831a5dd0840fd19af2e15',\n",
       "   'title': 'Inferring Video Streaming Quality of Real-Time Communication Inside Network'},\n",
       "  {'paperId': '60829b096a3c47fd4a4cd415c55c6dceb3843d43',\n",
       "   'title': 'EViT: Privacy-Preserving Image Retrieval via Encrypted Vision Transformer in Cloud Computing'},\n",
       "  {'paperId': '8d3318aae8d1a101e7a9b5e3b77fe4411a142f4e',\n",
       "   'title': 'API2Vec++: Boosting API Sequence Representation for Malware Detection and Classification'},\n",
       "  {'paperId': '35dab3d0ebb5c3195d1d482f90e44145dde78bb9',\n",
       "   'title': 'One for All: A Unified Generative Framework for Image Emotion Classification'},\n",
       "  {'paperId': 'e7ffb2635bc38c5dfe3f9684906b3983d4bc9422',\n",
       "   'title': 'Aggregating Global and Local Representations via Hybrid Transformer for Video Deraining'},\n",
       "  {'paperId': '68fbde275af6cd62b9ae60a566b1c2bdfd6d1c88',\n",
       "   'title': 'MtArtGPT: A Multi-Task Art Generation System With Pre-Trained Transformer'},\n",
       "  {'paperId': '7432caea9d81f7391b5e6d40a2bcc2cdc1c9fe4b',\n",
       "   'title': 'PANNER: POS-Aware Nested Named Entity Recognition Through Heterogeneous Graph Neural Network'},\n",
       "  {'paperId': '6e0a9a475caf71cdcf855dda37dc36c5eeb8d721',\n",
       "   'title': 'Semi-Supervised Dimensional Media Sentiment Analysis via Exploring Sample Relationships'},\n",
       "  {'paperId': 'a848778ab503ff9aa8e3c3e8d275ce8e3173e1dc',\n",
       "   'title': 'Hateful Sentiment Detection in Real-Time Tweets: An LSTM-Based Comparative Approach'},\n",
       "  {'paperId': 'a73f1d5c5c2ca71093ceb4915fe99ee17991da0e',\n",
       "   'title': 'MEFaND: A Multimodel Framework for Early Fake News Detection'},\n",
       "  {'paperId': '484648453ebd0dca50a059a2ff87874190577727',\n",
       "   'title': 'Natural Language Processing Applied to Forensics Information Extraction With Transformers and Graph Visualization'},\n",
       "  {'paperId': '3018fffa96ab291a2f27eef65a3381c1643a52a2',\n",
       "   'title': 'Cross-Modal Attention Network for Detecting Multimodal Misinformation From Multiple Platforms'},\n",
       "  {'paperId': '28f7b4471bdaa9041237465a4b97f10840c60fc4',\n",
       "   'title': 'A Meta-Analysis of State-of-the-Art Automated Fake News Detection Methods'},\n",
       "  {'paperId': '7565eecd98c48ba844556729645b233c9f3c5d9d',\n",
       "   'title': 'UCF-PKS: Unforeseen Consumer Fraud Detection With Prior Knowledge and Semantic Features'},\n",
       "  {'paperId': '44c5a3a73903712154e81e8bf7c6a03b6a4f7f97',\n",
       "   'title': 'Toxic Fake News Detection and Classification for Combating COVID-19 Misinformation'},\n",
       "  {'paperId': '1512fd67eeaa8de986a53275392de15ec714801e',\n",
       "   'title': 'Do Sentence-Level Sentiment Interactions Matter? Sentiment Mixed Heterogeneous Network for Fake News Detection'},\n",
       "  {'paperId': '3910acd54e04823b00827cd623c92b5a7b6e3c69',\n",
       "   'title': 'Graph Contrastive Learning With Feature Augmentation for Rumor Detection'},\n",
       "  {'paperId': 'b49edd2ff3cfeb23107b10996a4d2798b2c9cb44',\n",
       "   'title': 'Hybrid Text Representation for Explainable Suicide Risk Identification on Social Media'},\n",
       "  {'paperId': 'd8ab1d317e07750b257c6a377266abd0f8212534',\n",
       "   'title': 'Accelerating Distributed Training With Collaborative In-Network Aggregation'},\n",
       "  {'paperId': '94450536b372d0da05d5300183a27a4a4d2208f4',\n",
       "   'title': 'ECF-IDS: An Enhanced Cuckoo Filter-Based Intrusion Detection System for In-Vehicle Network'},\n",
       "  {'paperId': 'eaf9750bd83dede41267bb433e09f5ba9c397b56',\n",
       "   'title': 'MITREtrieval: Retrieving MITRE Techniques From Unstructured Threat Reports by Fusion of Deep Learning and Ontology'},\n",
       "  {'paperId': '1e02e8820960d35109f8d5cb89c50b1168b8c9ba',\n",
       "   'title': 'An explainable language model for antibody specificity prediction using curated influenza hemagglutinin antibodies.'},\n",
       "  {'paperId': 'b26e0f63231d936242a070eb85f096896c44c78c',\n",
       "   'title': 'Enhanced EEG seizure recognition after hypoxia-ischemia in fetal sheep using transformer-based wavelet-scalogram deep learning'},\n",
       "  {'paperId': '729acac960e59a2497cc9a5ac81862bc0ff7b9ab',\n",
       "   'title': 'Feasibility of big data analytics in disaster psychiatry: Impact of Seoul Itaewon tragedy on sentiment distribution on Twitter.'},\n",
       "  {'paperId': '7991f914515ef15312fec9f7f823f8e0ad0660b0',\n",
       "   'title': 'Adversarial Entity Graph Convolutional Networks for multi-hop inference question answering'},\n",
       "  {'paperId': '157b28ddbee04819dd178bf8097d4bb9e4028edd',\n",
       "   'title': 'Intelligent Emergency Assisted Decision-Making Method Based on Standard Digitalization: Hazardous Chemical Accidents in Industrial Parks'},\n",
       "  {'paperId': '2a4e05811a21c6e449c1fcb1788bfac4806c8e5c',\n",
       "   'title': 'FedCLCC: A personalized federated learning algorithm for edge cloud collaboration based on contrastive learning and conditional computing'},\n",
       "  {'paperId': 'ae0e23b6a5402049cc9e2f7e8d25649fa2465723',\n",
       "   'title': 'Dynamic elicitation and forecasting innovation requirement of smart product-service system via user-manufacturer value co-creation perspective using multi-source data'},\n",
       "  {'paperId': '06a14542392ef22dcf3e53ff6037a472c70faa2d',\n",
       "   'title': 'Accelerating drug discovery, development, and clinical trials by artificial intelligence.'},\n",
       "  {'paperId': '04fa8ede350d31871dfcae85619c43f3046243ab',\n",
       "   'title': 'Research status and application of artificial intelligence large models in the oil and gas industry'},\n",
       "  {'paperId': 'a1ba0015374811e9bd3ed1f70bcbaa97dd7780e7',\n",
       "   'title': 'AI-driven transcriptomic encoders: From explainable models to accurate, sample-independent cancer diagnostics'},\n",
       "  {'paperId': '2b9e5cf825a9ec10931dfde2464588894c2a1734',\n",
       "   'title': 'The impact of AI-enabled CRM systems on organizational competitive advantage: A mixed-method approach using BERTopic and PLS-SEM'},\n",
       "  {'paperId': 'f5e3201db48521e0efb3dc615e1828490c0615f5',\n",
       "   'title': 'How Photovoltaic Industry Policies Foster the Development of Silicon Solar Cell Manufacturing Technology: Based on Self-Attention Mechanism'},\n",
       "  {'paperId': 'd06c4b7992f5a72477f507b2cadf9b226734f9b4',\n",
       "   'title': 'Language conditioned multi-scale visual attention networks for visual grounding'},\n",
       "  {'paperId': 'bf2517616dc403577dbbdb0149baa7f3cbdf39e7',\n",
       "   'title': 'Introducing MagBERT: A language model for magnesium textual data mining and analysis'},\n",
       "  {'paperId': '1b9d44d19590d468d3e55e9f3140b8e087f21b5c',\n",
       "   'title': 'How to Find Helpful Health-related Knowledge in the Online Healthcare Community'},\n",
       "  {'paperId': '06c46178c61320142deba3e31f8a83161e2aa734',\n",
       "   'title': 'Incorporating Patient Values in Large Language Model Recommendations for Surrogate and Proxy Decisions'},\n",
       "  {'paperId': 'd90f4687a1bca45f57a4c2c8459eab8f058cf46e',\n",
       "   'title': 'Decoupled Cross-Modal Transformer for Referring Video Object Segmentation'},\n",
       "  {'paperId': '3e0858a41b35e96ba35b82558f3679c3176624cc',\n",
       "   'title': 'Symmetrical Siamese Network for pose-guided person synthesis'},\n",
       "  {'paperId': '2546130e5b5a82a927cd1503712b43486bcb7480',\n",
       "   'title': 'Multimodal fusion network for ICU patient outcome prediction.'},\n",
       "  {'paperId': '86c0d33f8a41f217345442b1525656cfbe27fc10',\n",
       "   'title': 'Movie trailer genre classification using multimodal pretrained features'},\n",
       "  {'paperId': '9e652faf619cfa6a8c06d30b545c39a660c30900',\n",
       "   'title': 'Copiously Quote Classics: Improving Chinese Poetry Generation with historical allusion knowledge'},\n",
       "  {'paperId': '322ad957128539ba52036212175bfc8f1bcf1e6f',\n",
       "   'title': 'Meta-learning triplet contrast network for few-shot text classification'},\n",
       "  {'paperId': '4d90594a6bf59961c7c761b40a37d2f993b662c9',\n",
       "   'title': 'f-KGQA: A Fuzzy Question Answering System for Knowledge Graphs'},\n",
       "  {'paperId': '811040159c9bd5c5e9ef799eaeaeb71071554471',\n",
       "   'title': 'Semi-supervised regression based on Representation Learning for fermentation processes'},\n",
       "  {'paperId': '71d3011ff2bfd8384336bc79576e1f672768bab4',\n",
       "   'title': 'Enhancing Chinese–Braille translation: A two-part approach with token prediction and segmentation labeling'},\n",
       "  {'paperId': '486918b70571563b1c7c5405539cc6425ed98c13',\n",
       "   'title': 'Automated Behavioral Coding to Enhance the Effectiveness of Motivational Interviewing in a Chat-Based Suicide Prevention Helpline: Secondary Analysis of a Clinical Trial'},\n",
       "  {'paperId': '29be00f327e4b739ddb7ec06e547d66b90873175',\n",
       "   'title': 'Insights Into the Patient Experience of Hormone Therapy for Early Breast Cancer Treatment Using Patient Forum Discussions and Natural Language Processing'},\n",
       "  {'paperId': '14dd1f63832c6c5fa57cf3f9de8c164927393fac',\n",
       "   'title': 'The Crowdless Future? How Generative AI Is Shaping the Future of Human Crowdsourcing'},\n",
       "  {'paperId': '57420384d2c37d4634c698fb91452649d18bccec',\n",
       "   'title': 'An Approach for Identifying Complementary Patents Based on Deep Learning'},\n",
       "  {'paperId': 'a675d3218d56aeccbd570e5cbd4be43d6e9664e3',\n",
       "   'title': 'Big Cooperative Learning'},\n",
       "  {'paperId': '401893e853d71b8b6afa53bc34397a23ad791f36',\n",
       "   'title': 'ANALISIS KINERJA ALGORITMA KLASIFIKASI TEKS BERT DALAM MENDETEKSI BERITA HOAKS'},\n",
       "  {'paperId': 'c463bddf223d91f86b65d8cb3dbb601b79730300',\n",
       "   'title': 'Self-supervised Learning and Masked Language Model for Code-switching Automatic Speech Recognition'},\n",
       "  {'paperId': '1d6a315e4379168669a02a041f90c6a27c0cfe36',\n",
       "   'title': 'Joint extraction of Chinese medical entities and relations based on RoBERTa and single-module global pointer'},\n",
       "  {'paperId': 'b44df84fd797663ad4002d0a1fa770e16273b5cd',\n",
       "   'title': \"KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government Financial Data and Regulations to Enhance Decision Making\"},\n",
       "  {'paperId': 'c2835d88f2f5b4ffcaa2055cb137a3f79b22bff1',\n",
       "   'title': 'FTuner: A Fast Dynamic Shape Tensors Program Auto-Tuner for Deep Learning Compilers'},\n",
       "  {'paperId': 'ed8c45dce4d9e3f98ec3b245f53a3831711667cd',\n",
       "   'title': 'Responsive ML inference in multi-tenanted environments using AQUA'},\n",
       "  {'paperId': 'c650d58aaf13cb0847e2a10dc955843ef3e709b6',\n",
       "   'title': 'ShieldGemma: Generative AI Content Moderation Based on Gemma'},\n",
       "  {'paperId': '06992e29f3f9438fb0b625f488e2c927eb32806a',\n",
       "   'title': 'State-observation augmented diffusion model for nonlinear assimilation'},\n",
       "  {'paperId': '12e770e042112d23580ceec6d5962815b5ae4d78',\n",
       "   'title': 'EdgeLLM: A Highly Efficient CPU-FPGA Heterogeneous Edge Accelerator for Large Language Models'},\n",
       "  {'paperId': '57d288508838a433a122e0fccf99fc540c73fff2',\n",
       "   'title': 'Multi-Level Querying using A Knowledge Pyramid'},\n",
       "  {'paperId': '4757e3d37d4753af0df749e71de0faec7d19d054',\n",
       "   'title': 'TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities'},\n",
       "  {'paperId': '5ab98a8eccd0903f0af7d01b0ce2c4be5766f046',\n",
       "   'title': 'Learning Effective Representations for Retrieval Using Self-Distillation with Adaptive Relevance Margins'}],\n",
       " 'references': [{'paperId': '26b47e35fe6e4260fdf7b7cc98f279a73c277494',\n",
       "   'title': 'Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering'},\n",
       "  {'paperId': '27e98e09cf09bc13c913d01676e5f32624011050',\n",
       "   'title': 'U-Net: Machine Reading Comprehension with Unanswerable Questions'},\n",
       "  {'paperId': '0c47cad9729c38d9db1f75491b1ee4bd883a5d4e',\n",
       "   'title': 'Semi-Supervised Sequence Modeling with Cross-View Training'},\n",
       "  {'paperId': 'ac11062f1f368d97f4c826c317bf50dcc13fdb59',\n",
       "   'title': 'Dissecting Contextual Word Embeddings: Architecture and Representation'},\n",
       "  {'paperId': 'af5c4b80fbf847f69a202ba5a780a3dd18c1a027',\n",
       "   'title': 'SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference'},\n",
       "  {'paperId': 'b9de9599d7241459db9213b5cdd7059696f5ef8d',\n",
       "   'title': 'Character-Level Language Modeling with Deeper Self-Attention'},\n",
       "  {'paperId': '421fc2556836a6b441de806d7b393a35b6eaea58',\n",
       "   'title': 'Contextual String Embeddings for Sequence Labeling'},\n",
       "  {'paperId': 'cb0f3ee1e98faf92429d601cdcd76c69c1e484eb',\n",
       "   'title': 'Neural Network Acceptability Judgments'},\n",
       "  {'paperId': '451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c',\n",
       "   'title': 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding'},\n",
       "  {'paperId': '3febb2bed8865945e7fddc99efd791887bb7e14f',\n",
       "   'title': 'Deep Contextualized Word Representations'},\n",
       "  {'paperId': 'ad31866da7f14ae21bd38df0a3b1ffd1a1438122',\n",
       "   'title': 'An efficient framework for learning sentence representations'},\n",
       "  {'paperId': '8c1b00128e74f1cd92aede3959690615695d5101',\n",
       "   'title': 'QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension'},\n",
       "  {'paperId': '7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d',\n",
       "   'title': 'MaskGAN: Better Text Generation via Filling in the ______'},\n",
       "  {'paperId': '1e077413b25c4d34945cc2707e17e46ed4fe784a',\n",
       "   'title': 'Universal Language Model Fine-tuning for Text Classification'},\n",
       "  {'paperId': '3c78c6df5eb1695b6a399e346dde880af27d1016',\n",
       "   'title': 'Simple and Effective Multi-Paragraph Reading Comprehension'},\n",
       "  {'paperId': 'bc8fa64625d9189f5801837e7b133e7fe3c581f7',\n",
       "   'title': 'Learned in Translation: Contextualized Word Vectors'},\n",
       "  {'paperId': 'a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096',\n",
       "   'title': 'SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation'},\n",
       "  {'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776',\n",
       "   'title': 'Attention is All you Need'},\n",
       "  {'paperId': 'e0222a1ae6874f7fff128c3da8769ab95963da04',\n",
       "   'title': 'Reinforced Mnemonic Reader for Machine Reading Comprehension'},\n",
       "  {'paperId': 'ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c',\n",
       "   'title': 'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data'},\n",
       "  {'paperId': 'f010affab57b5fcf1cd6be23df79d8ec98c7289c',\n",
       "   'title': 'TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension'},\n",
       "  {'paperId': 'a97dc52807d80454e78d255f9fbd7b0fab56bd03',\n",
       "   'title': 'Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning'},\n",
       "  {'paperId': '5ded2b8c64491b4a67f6d39ce473d4b9347a672e',\n",
       "   'title': 'A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference'},\n",
       "  {'paperId': '0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38',\n",
       "   'title': 'Semi-supervised sequence tagging with bidirectional language models'},\n",
       "  {'paperId': '3a7b63b50c64f4ec3358477790e84cbd6be2a0b4',\n",
       "   'title': 'Bidirectional Attention Flow for Machine Comprehension'},\n",
       "  {'paperId': 'c6850869aa5e78a107c378d2e8bfa39633158c0c',\n",
       "   'title': \"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\"},\n",
       "  {'paperId': '59761abc736397539bdd01ad7f9d91c8607c0457',\n",
       "   'title': 'context2vec: Learning Generic Context Embedding with Bidirectional LSTM'},\n",
       "  {'paperId': '4361e64f2d12d63476fdc88faf72a0f70d9a2ffb',\n",
       "   'title': 'Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units'},\n",
       "  {'paperId': '05dd7254b632376973f3a1b4d39485da17814df5',\n",
       "   'title': 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'},\n",
       "  {'paperId': '2cd8e8f510c89c7c18268e8ad51c061e459ad321',\n",
       "   'title': 'A Decomposable Attention Model for Natural Language Inference'},\n",
       "  {'paperId': '26e743d5bd465f49b9538deaf116c15e61b7951f',\n",
       "   'title': 'Learning Distributed Representations of Sentences from Unlabelled Data'},\n",
       "  {'paperId': '4aa9f5150b46320f534de4747a2dd0cd7f3fe292',\n",
       "   'title': 'Semi-supervised Sequence Learning'},\n",
       "  {'paperId': 'f04df4e20a18358ea2f689b4c129781628ef7fc1',\n",
       "   'title': 'A large annotated corpus for learning natural language inference'},\n",
       "  {'paperId': '6e795c6e9916174ae12349f5dc3f516570c17ce8',\n",
       "   'title': 'Skip-Thought Vectors'},\n",
       "  {'paperId': '0e6824e137847be0599bb0032e37042ed2ef5045',\n",
       "   'title': 'Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books'},\n",
       "  {'paperId': '081651b38ff7533550a3adfc1c00da333a8fe86c',\n",
       "   'title': 'How transferable are features in deep neural networks?'},\n",
       "  {'paperId': 'f37e1b62a767a307c046404ca96bc140b3e68cb5',\n",
       "   'title': 'GloVe: Global Vectors for Word Representation'},\n",
       "  {'paperId': 'f3de86aeb442216a8391befcacb49e58b478f512',\n",
       "   'title': 'Distributed Representations of Sentences and Documents'},\n",
       "  {'paperId': '5d833331b0e22ff359db05c62a8bca18c4f04b68',\n",
       "   'title': 'One billion word benchmark for measuring progress in statistical language modeling'},\n",
       "  {'paperId': '87f40e6f3022adbc1f1905e3e506abad05a9964f',\n",
       "   'title': 'Distributed Representations of Words and Phrases and their Compositionality'},\n",
       "  {'paperId': '687bac2d3320083eb4530bf18bb8f8f721477600',\n",
       "   'title': 'Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank'},\n",
       "  {'paperId': '128cb6b891aee1b5df099acb48e2efecfcff689f',\n",
       "   'title': 'The Winograd Schema Challenge'},\n",
       "  {'paperId': '8492070dc4031ed825e95e4803781752bb5e909f',\n",
       "   'title': 'Word Representations: A Simple and General Method for Semi-Supervised Learning'},\n",
       "  {'paperId': '7be3afdb7b7894321027ec90ea0a990aa7a0f266',\n",
       "   'title': 'Natural Language Understanding'},\n",
       "  {'paperId': 'd2c733e34d48784a37d717fe43d9e93277a8c53e',\n",
       "   'title': 'ImageNet: A large-scale hierarchical image database'},\n",
       "  {'paperId': 'a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb',\n",
       "   'title': 'A Scalable Hierarchical Distributed Language Model'},\n",
       "  {'paperId': '57458bc1cffe5caa45a885af986d70f723f406b4',\n",
       "   'title': 'A unified architecture for natural language processing: deep neural networks with multitask learning'},\n",
       "  {'paperId': '843959ffdccf31c6694d135fad07425924f785b1',\n",
       "   'title': 'Extracting and composing robust features with denoising autoencoders'},\n",
       "  {'paperId': '9fa8d73e572c3ca824a04a5f551b602a17831bc5',\n",
       "   'title': 'Domain Adaptation with Structural Correspondence Learning'},\n",
       "  {'paperId': '944e1a7b2c5c62e952418d7684e3cade89c76f87',\n",
       "   'title': 'A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data'},\n",
       "  {'paperId': '10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb',\n",
       "   'title': 'Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition'},\n",
       "  {'paperId': '3de5d40b60742e3dfa86b19e7f660962298492af',\n",
       "   'title': 'Class-Based n-gram Models of Natural Language'},\n",
       "  {'paperId': '766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd',\n",
       "   'title': '“Cloze Procedure”: A New Tool for Measuring Readability'},\n",
       "  {'paperId': None,\n",
       "   'title': 'Improving language understanding with unsupervised learning'},\n",
       "  {'paperId': None, 'title': 'Corpus of linguistic acceptability'},\n",
       "  {'paperId': '8ff46c88964a36985f2b45933a3d47b81bd87bd0',\n",
       "   'title': 'Quora Question Pairs'},\n",
       "  {'paperId': None,\n",
       "   'title': '2016) rather than the standard relu, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood'},\n",
       "  {'paperId': '0f8468de03ee9f12d693237bec87916311bf1c24',\n",
       "   'title': 'The Seventh PASCAL Recognizing Textual Entailment Challenge'},\n",
       "  {'paperId': 'db8885a0037fe47d973ade79d696586453710233',\n",
       "   'title': 'The Sixth PASCAL Recognizing Textual Entailment Challenge'},\n",
       "  {'paperId': '351ec42df2b60c6042addf96e6b98673bbaf4dfd',\n",
       "   'title': 'The Fourth PASCAL Recognizing Textual Entailment Challenge'},\n",
       "  {'paperId': '475354f10798f110d34792b6d88f31d6d5cb099e',\n",
       "   'title': 'Automatically Constructing a Corpus of Sentential Paraphrases'},\n",
       "  {'paperId': 'e808f28d411a958c5db81ceb111beb2638698f47',\n",
       "   'title': 'The PASCAL Recognising Textual Entailment Challenge'},\n",
       "  {'paperId': None,\n",
       "   'title': 'We provide examples of the pre-training tasks in the following'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
