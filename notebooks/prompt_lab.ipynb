{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place for experimenting the progressive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.utils as U\n",
    "from model_discovery.configs.gam_config import GAMConfig, GAMConfig_14M\n",
    "from model_discovery.model.composer import GABTree,ROOT_UNIT_TEMPLATE,GAUBase\n",
    "# from model_discovery.evolution import  BuildEvolution\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "db_dir = U.pjoin(ckpt_dir, 'test_composer', 'db')\n",
    "test_tree = GABTree('TestTree', db_dir)\n",
    "\n",
    "prompts_dir='../model_discovery/agents/prompts/'\n",
    "gab_py = U.read_file(U.pjoin(prompts_dir,'gab_template.py'))\n",
    "gam_py = U.read_file(U.pjoin(prompts_dir,'gam_prompt.py'))\n",
    "GAU_TEMPLATE = U.read_file(U.pjoin(prompts_dir,'gau_template.py'))\n",
    "GAU_BASE=inspect.getsource(GAUBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# TestTree.py\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom model_discovery.model.utils.modules import GABUnit # DO NOT CHANGE THIS IMPORT STATEMENT #\\n\\n\\n# YOU CAN IMPORT MORE MODULES HERE #\\n\\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\\n\\n\\nclass TestTree(GABUnit): \\n    \"\"\"Generalized Autoregressive Block\\n        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\\n        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\\n        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\\n    \"\"\"\\n    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\\n        # argv: list of hyperparameters\\n        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\\n        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\\n        \\n        # COMPLETING THE CODE HERE #\\n\\n\\n    # YOU CAN ADD MORE FUNCTIONS HERE #\\n\\n\\n    def _forward(self, X, **Z): \\n\\n        # COMPLETING THE CODE HERE #\\n        \\n        return X\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tree.path\n",
    "test_tree.get_source('TestTree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a professional AI researcher focusing on discovering the best\n",
      "autoregressive language model block. You goal is to design a novel block\n",
      "following the Generalized Autoregressive Block (GAB) structure defined in the\n",
      "following base class:\n",
      "\n",
      "```python \n",
      "class GABBase(nn.Module):\n",
      "    \"\"\" Base class for Generalized Autoregressive Block \"\"\"\n",
      "    def __init__(self,embed_dim: int, block_loc: tuple): \n",
      "        super().__init__()\n",
      "        self.embed_dim = embed_dim\n",
      "        self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\n",
      "\n",
      "    def _forward(self,X,**kwargs): \n",
      "        raise NotImplementedError\n",
      "     \n",
      "    # YOU ARE NOT ALLOW TO OVERRIDE THIS METHOD #\n",
      "    def forward(self,X,**Z):\n",
      "        \"\"\"Forward pass of the model\"\"\"\n",
      "        assert X.shape[-1] == self.embed_dim\n",
      "        Y_=self._forward(X,**Z)\n",
      "        if isinstance(Y_,tuple):\n",
      "            Y, Z = Y_\n",
      "        else:\n",
      "            Z = {}\n",
      "        assert Y.shape == X.shape\n",
      "        return Y, Z\n",
      " ```\n",
      "\n",
      "\n",
      "The GAB will be used to construct a Generalized Autoregressive Model (GAM)\n",
      "defined as follows:\n",
      "\n",
      "```python from transformers.modeling_outputs import CausalLMOutput\n",
      "from transformers import PreTrainedModel, PretrainedConfig\n",
      "from dataclasses import dataclass\n",
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "\n",
      "from gab import GAB, gab_config\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class GAMConfig(PretrainedConfig):\n",
      "    '''Configurations for Generalized Autoregressive Models.'''\n",
      "\n",
      "    d_model: int\n",
      "    n_block: int\n",
      "    batch_tokens: int \n",
      "    vocab_size: int = None\n",
      "\n",
      "\n",
      "class GAM(nn.Module):\n",
      "    ''' Generalized Autoregressive Models\n",
      "        Input:        X: (batch, seqlen, embed_dim)\n",
      "        Output:       Y: (batch, seqlen, embed_dim)\n",
      "    '''\n",
      "    def __init__(\n",
      "        self,\n",
      "        d_model: int,\n",
      "        n_block: int,\n",
      "        vocab_size: int = 50277,\n",
      "        norm_epsilon: float = 1e-5,\n",
      "        device = None,\n",
      "        dtype = None,\n",
      "    ) -> None:\n",
      "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      "        super().__init__()\n",
      "        self.d_model = d_model\n",
      "        self.embedding = nn.Embedding(vocab_size, d_model, **self.factory_kwargs)\n",
      "\n",
      "        block_config = gab_config()\n",
      "        self.blocks = nn.ModuleList(\n",
      "            [\n",
      "                GAB(\n",
      "                    embed_dim=d_model, \n",
      "                    block_loc=(layer_idx,n_block),\n",
      "                    device=device, \n",
      "                    dtype=dtype, \n",
      "                    **block_config\n",
      "                )\n",
      "                for layer_idx in range(n_block)\n",
      "            ]\n",
      "        )\n",
      "        self.norm_out = nn.LayerNorm(\n",
      "            d_model, eps=norm_epsilon, **self.factory_kwargs\n",
      "        )\n",
      "\n",
      "    def forward(self, input_ids):\n",
      "        hidden_states = self.embedding(input_ids)\n",
      "        intermediate_vars = {}\n",
      "        for block in self.blocks:\n",
      "            hidden_states, intermediate_vars = block(\n",
      "                hidden_states,\n",
      "                **intermediate_vars\n",
      "            )\n",
      "        hidden_states = self.norm_out(hidden_states)\n",
      "        return hidden_states\n",
      "\n",
      "\n",
      "class ModisLMHeadModel(PreTrainedModel):\n",
      "    ''' Generalized Autoregressive Models with LM Head '''\n",
      "    config_class = GAMConfig\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: GAMConfig,\n",
      "        device=None,\n",
      "        dtype=None,\n",
      "    ) -> None:\n",
      "        super().__init__(config)\n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      "        self.backbone = GAM(\n",
      "            d_model=config.d_model,\n",
      "            n_block=config.n_block,\n",
      "            vocab_size=config.vocab_size,\n",
      "            **factory_kwargs,\n",
      "        )\n",
      "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False, **factory_kwargs)\n",
      "\n",
      "    def forward(self, input_ids, **gab_kwargs):\n",
      "        hidden_states = self.backbone(input_ids, **gab_kwargs)\n",
      "        lm_logits = self.lm_head(hidden_states)\n",
      "        return CausalLMOutput(logits=lm_logits)\n",
      " ```\n",
      "\n",
      "The produced language model will be pretrained with the corpus and then be\n",
      "applied for downstream tasks. The new model is expected to have a low\n",
      "perplexity, high accuracy, robustness, efficiency, and most importantly, good\n",
      "scalability. You have two roles 1) to propose ideas, analyze the problems,\n",
      "design the model and implement it and; 2) to write the reports that justify your\n",
      "ideas. You do not need to immediately do everything at one response, following\n",
      "the provided instructions, and finish those tasks step by step in the coming\n",
      "multi-round dialog. \n",
      "\n",
      "Since the autoregressive model design is complicated, so we will break it down\n",
      "into smaller parts. We represent a block as multiple nested units, the\n",
      "Generalized Autoregressive Unit (GAU). Each GAU accepts a sequence of embeddings\n",
      "X and a dictionary of intermediate variables Z as input, and outputs a sequence\n",
      "of embeddings Y and a dictionary of new or updated intermediate variables Z_. Z_\n",
      "is optional, when it is provided, it will be used to update Z for the next unit\n",
      "by Z.update(Z_). A GAU is defined in the following base class:\n",
      "\n",
      "```python class GAUBase(nn.Module): \n",
      "    \"\"\" \n",
      "    Instead of directly giving the full implementation of a GAB block, the agent need to \n",
      "    design a series of nested GAB units and construct the full GAB block as a pipeline of these units.\n",
      "\n",
      "    GAB is fractal, like GAB itself, each GAB unit accepts X and Z as input and returns Y and Z as output.\n",
      "    \"\"\" \n",
      "    def __init__(self, embed_dim: int):\n",
      "        super().__init__()\n",
      "        self.embed_dim = embed_dim\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        raise NotImplementedError\n",
      "    \n",
      "    def forward(self, X, Z):\n",
      "        assert len(X.shape) == 3, \"Input shape must be (batch, seqlen, embed_dim)\"\n",
      "        assert X.shape[-1] == self.embed_dim\n",
      "        _params = inspect.signature(self._forward).parameters\n",
      "        _Z = {k: v for k, v in Z.items() if k in _params}\n",
      "        Y = self._forward(X, **_Z)\n",
      "        if isinstance(Y, tuple):\n",
      "            Y, Z_ = Y\n",
      "        else:\n",
      "            Z_ = {}\n",
      "        assert Y.shape == X.shape, f\"GAB Unit must has a sequence with the same shape as input in output, got {Y.shape} instead\"\n",
      "        assert isinstance(Z_, dict), \"Intermediate variables must be stored in a tuple\"\n",
      "        Z.update(Z_) # the new intermediate variables are updated to the current Z\n",
      "        return Y, Z\n",
      " ```\n",
      "\n",
      "You will design a GAU by completing the blanks marked in this template, which\n",
      "includes the initialization where you can define your custom arguments with\n",
      "optional default values, the forward function where you can define convenient\n",
      "functions or classes in the GAB class such as caches, notice that you are only\n",
      "allowed to have only one GAU which inherited from the GAUBase class in the file:\n",
      " \n",
      "```python # gab.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
      "\n",
      "\n",
      "# YOU CAN IMPORT MORE MODULES HERE #\n",
      "\n",
      "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
      "\n",
      "\n",
      "class GAU(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\n",
      "    \"\"\"Generalized Autoregressive Block Unit\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
      "        # argv: list of hyperparameters\n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\n",
      "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
      "        \n",
      "        # COMPLETING THE CODE HERE #\n",
      "\n",
      "        raise NotImplementedError\n",
      "\n",
      "\n",
      "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
      "\n",
      "\n",
      "    def _forward(self, X, **Z): \n",
      "        \n",
      "        # THE CODE HERE MUST BE COMPLETED #\n",
      "\n",
      "        raise NotImplementedError\n",
      "     ```\n",
      "\n",
      "In a GAU, you can call other GAUs, as such, you can create a complicated GAB\n",
      "block by nesting multiple GAUs. However, each GAU should be not too complex, if\n",
      "you want to create complex block, you should break it down into smaller GAUs and\n",
      "nest them. As such, you should design a GAB block in a top-down manner. \n",
      "\n",
      "Notice that, everytime you are only allowed to edit within one GAU. You can\n",
      "leave placeholder definition and calls of the GAUs that you wish to implement\n",
      "later in your GAU. The system will automatically create an initial GAU code for\n",
      "the placeholders. Once a GAU is provided, it will be inserted into the entire\n",
      "GAB composed based on the tree of GAUs under your design and tested for\n",
      "correctness then reviewed for novelty and quality. You will need to ensure the\n",
      "correctness of all the GAUs in the final GAB at the end.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import model_discovery.agents.prompts.prompts as P\n",
    "importlib.reload(P)\n",
    "\n",
    "gu_system_prompt=P.GU_DESIGNER_SYSTEM_prompt.format(GAB_BASE=P.GAB_BASE,GAM_PY=gam_py,GAU_BASE=GAU_BASE,GAU_TEMPLATE=GAU_TEMPLATE)\n",
    "\n",
    "print(gu_system_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from model_discovery.agents.flow.gau_flows import GAUReformer   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
