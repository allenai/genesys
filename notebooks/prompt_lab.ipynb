{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place for experimenting the progressive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.utils as U\n",
    "from model_discovery.configs.gam_config import GAMConfig, GAMConfig_14M\n",
    "# from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "from model_discovery.model.composer import GAUBase, GAUTree, check_tree_name, GABComposer\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "from model_discovery.evolution import BuildEvolution\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "\n",
    "prompts_dir='../model_discovery/agents/prompts/'\n",
    "gab_py = U.read_file(U.pjoin(prompts_dir,'gab_template.py'))\n",
    "gam_prompt_path = os.path.join(prompts_dir,'gam_prompt.py')\n",
    "gau_template_path = os.path.join(prompts_dir,'gau_template.py')\n",
    "GAM_TEMPLATE=open(gam_prompt_path).read()\n",
    "GAU_TEMPLATE=open(gau_template_path).read()\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n",
    "\n",
    "GAB_BASE=inspect.getsource(GABBase)\n",
    "GAB_COMPOSER=inspect.getsource(GABComposer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os,sys\n",
    "import importlib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.flow.gau_utils\n",
    "importlib.reload(model_discovery.agents.flow.gau_utils)\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "import model_discovery.agents.prompts.prompts as P\n",
    "import model_discovery.model.composer\n",
    "importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUTree,GAUBase,GAUNode\n",
    "import model_discovery.utils\n",
    "importlib.reload(model_discovery.utils)\n",
    "import model_discovery.utils as U\n",
    "\n",
    "\n",
    "\n",
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "from model_discovery.model.utils.modules import gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "# Placeholder classes for future implementation\n",
    "class MemoryAccessUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, memory_size, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "class DownsamplingUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, downsample_factor, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "\n",
    "class XXX(GAUBase,object):\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "\n",
    "class XGAU(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
    "        # argv: list of hyperparameters\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\n",
    "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        self.a1 = kwarg_all.get('a1', 1)\n",
    "        self.a2: int = kwarg_all.get('a2', 2)\n",
    "\n",
    "        # COMPLETING THE CODE HERE #\n",
    "        self.token_scorer = TokenScoringGAU(embed_dim, **factory_kwargs)\n",
    "        self.dual_path: GAUBase = DualPathGAU(embed_dim, **factory_kwargs)\n",
    "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim, **factory_kwargs)\n",
    "\n",
    "        self.stages: nn.ModuleList = nn.ModuleList([CompressionStageGAU(embed_dim, device=\n",
    "            device, dtype=dtype, **kwargs) for _ in range(2)])\n",
    "\n",
    "        self.modelx=nn.ModuleList([\n",
    "            nn.Conv2d(1,20,5),\n",
    "            nn.ReLU(),\n",
    "            LatentAttentionGAU(),\n",
    "            nn.Conv2d(20,64,5),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "            \n",
    "\n",
    "        self.model1 = (\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          LatentAttentionGAU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('gau2', DualPathGAU()),\n",
    "                ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "                ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "        )\n",
    "\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU(),\n",
    "            'gau3': CompressionStageGAU()\n",
    "        })\n",
    "\n",
    "        self.model2 = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('gau2', DualPathGAU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "    def init_something(self):\n",
    "        self.modelx[3]=LatentAttentionGAU()\n",
    "        self.modelx.append(LatentAttentionGAU())\n",
    "        self.modelx.insert(0,LatentAttentionGAU())\n",
    "        self.activations['gau1']=LatentAttentionGAU()\n",
    "        self.model2.add_module('gau4',LatentAttentionGAU())\n",
    "\n",
    "        X=self.dual_path(X)\n",
    "    \n",
    "    def _forward(self, X, **Z): \n",
    "\n",
    "        # THE CODE HERE MUST BE COMPLETED #\n",
    "        # Step 1: Score tokens\n",
    "        X, Z = self.token_scorer(X, **Z)\n",
    "        # Step 2: Route through dual paths\n",
    "        # Step 3: Apply latent attention\n",
    "        Y, Z = self.latent_attention(X, A=A, B=B, **Z)\n",
    "        for stage in self.stages:\n",
    "            X = stage(X, **Z)\n",
    "        \n",
    "        X, Z = self.dual_path(X, **Z)\n",
    "        X=self.activations['gau3'](X)\n",
    "\n",
    "        return Y, Z\n",
    "\n",
    "@gau_test\n",
    "def test1(a=1,device=None):\n",
    "    print('test1')\n",
    "\n",
    "    \n",
    "@gau_test\n",
    "def test2(device=None):\n",
    "    print('test2')\n",
    "\n",
    "\n",
    "CHILDREN_DECLARATIONS = {\n",
    "    'XGAU':[\n",
    "        UnitDecl(\n",
    "            unitname='TokenScoringGAU', \n",
    "            requirements='Token scoring GAU',\n",
    "            inputs=[],\n",
    "            outputs=[],\n",
    "        ),\n",
    "        UnitDecl(\n",
    "            unitname='DualPathGAU',\n",
    "            requirements='Dual path GAU',\n",
    "            inputs=[],\n",
    "            outputs=[],\n",
    "        ),\n",
    "        UnitDecl(\n",
    "            unitname='LatentAttentionGAU',\n",
    "            requirements='Latent attention GAU',\n",
    "            inputs=[],\n",
    "            outputs=[],\n",
    "        ),\n",
    "        UnitDecl(\n",
    "            unitname='CompressionStageGAU',\n",
    "            requirements='Compression stage GAU',\n",
    "            inputs=[],\n",
    "            outputs=[],\n",
    "        ),\n",
    "    ]\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "class MHA(GAUBase):\n",
    "    \"\"\"\n",
    "    MHA combines hierarchical gating mechanisms from HGRN2 with state space modeling (SSM) to improve long-range dependency capture and computational efficiency.\n",
    "\n",
    "    This class replaces the traditional multi-head attention mechanism with a more efficient approach that maintains linear complexity with respect to sequence length.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): The embedding dimension of the input sequence.\n",
    "        block_loc (tuple): The location of the block within the network.\n",
    "        kwarg_all (dict): Dictionary of additional keyword arguments for initialization.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        expand_ratio (int): The ratio for expansion layers.\n",
    "        d_state (int): The dimension of the state in the state space model.\n",
    "\n",
    "    Returns:\n",
    "        Y (Tensor): The output sequence with the same shape as the input.\n",
    "\n",
    "    Example:\n",
    "        >>> mha = MHA(embed_dim=512, block_loc=(0, 0), kwarg_all={}, num_heads=8, expand_ratio=4, d_state=64)\n",
    "        >>> X = torch.randn(2, 10, 512)\n",
    "        >>> Y, Z = mha(X)\n",
    "\n",
    "    Note:\n",
    "        This implementation maintains interpretability through the `get_attention_map` method, which provides a way to visualize effective attention patterns.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, num_heads: int, expand_ratio: int, d_state: int, device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.d_state = d_state\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # HGRN2-inspired projections\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "        # SSM components\n",
    "        self.ssm_A = nn.Parameter(torch.randn(num_heads, d_state, d_state))\n",
    "        self.ssm_B = nn.Parameter(torch.randn(num_heads, d_state, 1))\n",
    "        self.ssm_C = nn.Parameter(torch.randn(num_heads, 1, d_state))\n",
    "\n",
    "        # Expansion layers\n",
    "        self.expand = nn.Linear(embed_dim, embed_dim * expand_ratio, bias=True)\n",
    "        self.contract = nn.Linear(embed_dim * expand_ratio, embed_dim, bias=True)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "        # Layernorm for stability\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize SSM components\n",
    "        nn.init.orthogonal_(self.ssm_A)\n",
    "        nn.init.normal_(self.ssm_B, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.ssm_C, mean=0.0, std=0.02)\n",
    "\n",
    "        # Initialize other weights\n",
    "        nn.init.xavier_uniform_(self.q_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.k_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.v_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.expand.weight)\n",
    "        nn.init.xavier_uniform_(self.contract.weight)\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        B, L, D = X.shape\n",
    "        H = self.num_heads\n",
    "\n",
    "        # HGRN2-inspired projections\n",
    "        Q = self.q_proj(X).view(B, L, H, -1)\n",
    "        K = self.k_proj(X).view(B, L, H, -1)\n",
    "        V = self.v_proj(X).view(B, L, H, -1)\n",
    "\n",
    "        # Expansion\n",
    "        X_exp = self.expand(X).view(B, L, H, -1)\n",
    "\n",
    "        # SSM state update\n",
    "        state = torch.zeros(B, H, self.d_state, 1, device=X.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(L):\n",
    "            # Update state\n",
    "            state = torch.matmul(self.ssm_A, state) + self.ssm_B * K[:, t, :, :, None]\n",
    "\n",
    "            # Compute output\n",
    "            output = torch.matmul(self.ssm_C, state).squeeze(-1)\n",
    "            output = output * Q[:, t, :, :]\n",
    "            output = torch.sum(output * V[:, t, :, :], dim=-1)\n",
    "\n",
    "            # Apply expansion\n",
    "            output = output * X_exp[:, t, :, :]\n",
    "\n",
    "            outputs.append(output)\n",
    "\n",
    "        output = torch.stack(outputs, dim=1)\n",
    "        output = output.view(B, L, -1)\n",
    "\n",
    "        # Contraction\n",
    "        output = self.contract(output)\n",
    "\n",
    "        # Output projection and residual connection\n",
    "        output = self.out_proj(output)\n",
    "        output = self.norm(X + output)\n",
    "\n",
    "        return output, Z\n",
    "\n",
    "    def get_attention_map(self, X):\n",
    "        B, L, D = X.shape\n",
    "        H = self.num_heads\n",
    "\n",
    "        Q = self.q_proj(X).view(B, L, H, -1)\n",
    "        K = self.k_proj(X).view(B, L, H, -1)\n",
    "\n",
    "        attention_map = torch.zeros(B, H, L, L, device=X.device)\n",
    "        state = torch.zeros(B, H, self.d_state, 1, device=X.device)\n",
    "\n",
    "        for t in range(L):\n",
    "            state = torch.matmul(self.ssm_A, state) + self.ssm_B * K[:, t, :, :, None]\n",
    "            output = torch.matmul(self.ssm_C, state).squeeze(-1)\n",
    "            attention_map[:, :, t, :t+1] = torch.sum(output * Q[:, :t+1, :, :], dim=-1).transpose(1, 2)\n",
    "\n",
    "        return attention_map\n",
    "\n",
    "\n",
    "# WRITE YOUR UNIT TEST FUNCTIONS HERE #\n",
    "\n",
    "@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON'T BE RECOGNIZED AS A UNIT TEST #\n",
    "def test_mha(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\n",
    "    # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\n",
    "    embed_dim = 128\n",
    "    block_loc = (0, 0)\n",
    "    kwarg_all = {}\n",
    "    num_heads = 8\n",
    "    expand_ratio = 4\n",
    "    d_state = 64\n",
    "    mha = MHA(embed_dim, block_loc, kwarg_all, num_heads, expand_ratio, d_state, device=device, dtype=dtype)\n",
    "    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "    y, Z_ = mha(x, **Z)\n",
    "    assert y.shape == (1, 100, 128)\n",
    "\n",
    "    # WRITE ASSERTIONS TO PERFORM THE TEST, USE PRINT TO DEBUG #\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "    # raise NotImplementedError # YOU MUST IMPLEMENT THIS FUNCTION #\n",
    "'''\n",
    "children=['ProjectionGAU','SimilarityComputationGAU','NormalizationGAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_name = \"MHA\"  # Provide the unit_name to rename GAU class\n",
    "reformatted_code, new_args, gau_tests, errors, warnings, fetal_errors,docstring,children_decl = check_and_reformat_gau_code(code, unit_name)\n",
    "print(\"Reformatted Code:\\n\" + reformatted_code)\n",
    "print(\"Errors:\\n\", '\\n\\t'.join(errors+fetal_errors))\n",
    "print(\"Warnings:\\n\", '\\n\\t'.join(warnings))\n",
    "print(\"New Arguments:\\n\", new_args)\n",
    "print(\"Docstring:\\n\", docstring)\n",
    "print(\"Children Declarations:\\n\", children_decl)\n",
    "\n",
    "\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "spec=P.UnitSpec(\n",
    "   unitname = unit_name,\n",
    "   document = 'Test',\n",
    "   inputs = [],\n",
    "   outputs = [],\n",
    ")\n",
    "# test_tree.add_unit(\n",
    "#     spec, reformatted_code, new_args, None, None, None, children, gau_tests, None\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[{'role': 'system', 'content': '\\nYou are a professional AI researcher focusing on discovering the best\\nautoregressive language model block. Your goal is to design a novel block\\nfollowing the Generalized Autoregressive Block (GAB) structure defined in the\\nfollowing base class:\\n\\n```python class GABBase(nn.Module):\\n \"\"\" Base class for Generalized Autoregressive Block \"\"\"\\n def __init__(self,embed_dim: int, block_loc: tuple):\\n super().__init__()\\n self.embed_dim = embed_dim\\n self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\\n\\n def _forward(self, X, **Z): \\n raise NotImplementedError\\n \\n # YOU ARE NOT ALLOW TO OVERRIDE THIS METHOD #\\n def forward(self, X, **Z): # kwargs not parsable by torchscript but more flexible\\n \"\"\"Forward pass of the model\"\"\"\\n assert len(X.shape) == 3, f\"Input shape must be (batch, seqlen, embed_dim), got {len(X.shape)}D instead\"\\n assert X.shape[-1] == self.embed_dim, f\"Input shape must be (batch, seqlen, embed_dim), got {X.shape} instead\"\\n Y = self._forward(X, **Z)\\n if isinstance(Y, tuple):\\n Y, Z_ = Y\\n else:\\n Z_ = {}\\n assert Y.shape == X.shape, f\"GAB Output shape must be the same as input shape of {X.shape}, got {Y.shape} instead\"\\n assert isinstance(Z, dict), \"Intermediate variables must be stored in a dict\"\\n Z.update(Z_) # the new intermediate variables are updated to the current Z\\n return Y, Z\\n ```\\n\\nThe GAB will be used to construct a Generalized Autoregressive Model (GAM)\\ndefined as follows:\\n\\n```python from transformers.modeling_outputs import CausalLMOutput\\nfrom transformers import PreTrainedModel, PretrainedConfig\\nfrom dataclasses import dataclass\\n\\nimport torch\\nfrom torch import nn\\n\\nfrom gab import GAB, gab_config\\n\\n\\n@dataclass\\nclass GAMConfig(PretrainedConfig):\\n \\'\\'\\'Configurations for Generalized Autoregressive Models.\\'\\'\\'\\n\\n d_model: int\\n n_block: int\\n batch_tokens: int \\n vocab_size: int = None\\n\\n\\nclass GAM(nn.Module):\\n \\'\\'\\' Generalized Autoregressive Models\\n Input: X: (batch, seqlen, embed_dim)\\n Output: Y: (batch, seqlen, embed_dim)\\n \\'\\'\\'\\n def __init__(\\n self,\\n d_model: int,\\n n_block: int,\\n vocab_size: int = 50277,\\n norm_epsilon: float = 1e-5,\\n device = None,\\n dtype = None,\\n ) -> None:\\n self.factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n super().__init__()\\n self.d_model = d_model\\n self.embedding = nn.Embedding(vocab_size, d_model, **self.factory_kwargs)\\n\\n block_config = gab_config()\\n self.blocks = nn.ModuleList(\\n [\\n GAB(\\n embed_dim=d_model, \\n block_loc=(layer_idx,n_block),\\n device=device, \\n dtype=dtype, \\n **block_config\\n )\\n for layer_idx in range(n_block)\\n ]\\n )\\n self.norm_out = nn.LayerNorm(\\n d_model, eps=norm_epsilon, **self.factory_kwargs\\n )\\n\\n def forward(self, input_ids):\\n hidden_states = self.embedding(input_ids)\\n intermediate_vars = {}\\n for block in self.blocks:\\n hidden_states, intermediate_vars = block(\\n hidden_states,\\n **intermediate_vars\\n )\\n hidden_states = self.norm_out(hidden_states)\\n return hidden_states\\n\\n\\nclass GLMHeadModel(PreTrainedModel):\\n \\'\\'\\' Generalized Autoregressive Models with LM Head \\'\\'\\'\\n config_class = GAMConfig\\n\\n def __init__(\\n self,\\n config: GAMConfig,\\n device=None,\\n dtype=None,\\n ) -> None:\\n super().__init__(config)\\n factory_kwargs = {\"device\": device, \"dtype\": dtype}\\n self.backbone = GAM(\\n d_model=config.d_model,\\n n_block=config.n_block,\\n vocab_size=config.vocab_size,\\n **factory_kwargs,\\n )\\n self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False, **factory_kwargs)\\n\\n def forward(self, input_ids, **gab_kwargs):\\n hidden_states = self.backbone(input_ids, **gab_kwargs)\\n lm_logits = self.lm_head(hidden_states)\\n return CausalLMOutput(logits=lm_logits)\\n ```\\n\\nThe produced language model will be pretrained with the corpus and then be\\napplied for downstream tasks. The new model is expected to have a low\\nperplexity, high accuracy, robustness, efficiency, and most importantly, good\\nscalability. \\n\\nSince the autoregressive model design is complicated, so we will break it down\\ninto smaller parts. We represent a block as multiple nested units, the\\nGeneralized Autoregressive Unit (GAU). Each GAU accepts a sequence of embeddings\\nX and a dictionary of intermediate variables Z as input, and outputs a sequence\\nof embeddings Y and a dictionary of new or updated intermediate variables Z_. Z_\\nis optional, when it is provided, it will be used to update Z for the next unit\\nby Z.update(Z_). A GAU is defined in the following base class:\\n\\n```python class GAUBase(nn.Module): \\n \"\"\" \\n Instead of directly giving the full implementation of a GAB block, the agent need to \\n design a series of nested GAB units and construct the full GAB block as a pipeline of these units.\\n\\n GAB is fractal, like GAB itself, each GAB unit accepts X and Z as input and returns Y and Z as output.\\n \"\"\" \\n def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict):\\n super().__init__()\\n self.embed_dim = embed_dim\\n self.block_loc = block_loc # location of a block within the network, (layer_idx, n_block)\\n self.kwarg_all = kwarg_all # dict of all kwargs, use it to initialize the children units\\n\\n def _forward(self, X, **Z): # you can define the arguments instead of using **Z\\n raise NotImplementedError\\n \\n def forward(self, X, **Z):\\n assert len(X.shape) == 3 and X.shape[-1] == self.embed_dim, f\"Input X must be a sequence of shape (batch, seqlen, embed_dim), all other kinds of variables should be passed by Z. Got {X.shape} instead.\"\\n _params = inspect.signature(self._forward).parameters\\n X=X.to(**self.factory_kwargs)\\n _Z = {k: v for k, v in Z.items() if k in _params}\\n Y = self._forward(X, **_Z)\\n if isinstance(Y, tuple):\\n Y, Z_ = Y\\n else:\\n Z_ = {}\\n assert Y.shape == X.shape, f\"GAU output Y must be a sequence with the same shape as input of {X.shape}, got {Y.shape} instead, all other kinds of variables should be passed by Z.\"\\n assert isinstance(Z_, dict), \"Intermediate variables must be stored in a dict\"\\n Z.update(Z_) # the new intermediate variables are updated to the current Z\\n return Y, Z\\n ```\\n\\nYou will design a GAU by completing the blanks marked in this template, which\\nincludes the initialization where you can define your custom arguments with\\noptional default values, the forward function where you can define convenient\\nfunctions or classes in the GAB class such as caches, notice that you are only\\nallowed to have only one GAU which inherited from the GAUBase class in the file:\\n \\n```python # gau.py # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\\n\\nimport torch\\nimport torch.nn as nn\\n\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test # DO NOT CHANGE THIS IMPORT STATEMENT #\\n\\n\\n# YOU CAN IMPORT MORE MODULES HERE #\\n\\n# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\\n\\n\\nclass UnitName(GAUBase):\\n \"\"\"Generalized Autoregressive Block Unit\\n Input: X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \\n Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\\n Constraints: Causal, differentiable, parameter number, complexity, parallelizable...\\n\\n embed_dim: The dimension of the input embeddings\\n block_loc: The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\\n kwarg_all: A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\\n \"\"\"\\n def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT KEEP THE ORIGINAL ONES #\\n self.factory_kwargs = {\"device\": device, \"dtype\": dtype} # DO NOT CHANGE THIS LINE, REMEMBER TO PASS IT #\\n super().__init__(embed_dim, block_loc, kwarg_all) # DO NOT CHANGE THIS LINE #\\n \\n # COMPLETING THE CODE HERE #\\n\\n raise NotImplementedError\\n\\n\\n # YOU CAN ADD MORE FUNCTIONS HERE #\\n\\n\\n def _forward(self, X, **Z): \\n \\n # THE CODE HERE MUST BE COMPLETED #\\n\\n raise NotImplementedError\\n\\n\\n# WRITE YOUR UNIT TEST FUNCTIONS HERE #\\n\\n@gau_test # DO NOT CHANGE THIS DECORATOR, OTHERWISE IT WON\\'T BE RECOGNIZED AS A UNIT TEST #\\ndef unit_test_name(device=None, dtype=None)->None: # RENAME THIS FUNCTION, DO NOT CHANGE THE ARGUMENTS, IT SHOULD ALSO NOT RETURN ANYTHING #\\n # AN AVAILABLE DEVICE AND DTYPE ARE PASSED AS ARGUMENTS, USE THEM TO INITIALIZE YOUR GAU AND MOCK INPUTS #\\n\\n # WRITE ASSERTIONS TO PERFORM THE TEST, USE PRINT TO DEBUG #\\n \\n raise NotImplementedError # YOU MUST IMPLEMENT THIS FUNCTION #\\n ```\\n\\nIn a GAU, you can call other GAUs, as such, you can create a complicated GAB\\nblock by nesting multiple GAUs. However, each GAU should be not too complex, if\\nyou want to create complex block, you should break it down into smaller GAUs and\\nnest them. As such, you should design a GAB block in a top-down manner. \\n\\nInstead of starting from scratch, you will start from an existing design and\\nimprove it. You will be provided with the full information of the design,\\nincluding the proposal, the tree structure, the implementations of the GAUs. You\\nare only allowed to modify *one GAU* from the existing design. \\n\\nYou need to select one GAU to modify, you can define new children GAUs, however\\nyou need to guarantee that your modification wont affect the correctness of the\\noverall design. \\n\\nYou will start by writing down an overal proposal for the design you want to\\nhave, the proposal decides a direction, phylosophy and the plan of the design,\\nand the analysis of the problem and how you gonna solve it by modifying one GAU\\nfrom the existing design. You will be provided with one or multiple references\\nto consider that may inspire you if there are references provided.\\n\\nYour response should include: \\n\\n1. The proposal, it should include but not restrict to the following parts: a. A\\n title with the name of the design in the level 1 header format. You shuld\\n have only one level 1 header in your response which is the name of the\\n design.\\n\\n b. Your motivation of the design. What problem you want to solve based on the\\n insights or observations you have about the autoregressive models today,\\n and any inspirations you may have from the references. \\n\\n c. The analysis of the problem.\\n\\n d. The core idea and phylosophy behind of your design that may solve the\\n problem you proposed. \\n\\n e. The plan of the design. You should include subsections of that describe\\n the details of each part of the design with the justifications. The\\n selection of the GAU to modify and the reasoning of the selection. \\n\\n f. A conclution of the proposal. \\n\\n g. Optional, the references you used in your proposal, should be in the right\\n format.\\n2. The name of the variant of the model you are going to design.\\n3. The selection of the GAU to modify.\\n\\nThe proposal will be reviewed and you will be asked to modify it if it is not\\npassed. You can start to implement the design after the proposal is passed. \\n\\nThe proposal should be as detailed as possible, DO NOT WORRY IF THE PROPOSAL IS\\nTOO LONG, BUT ALSO DO NOT FILL IN BY REDUNDANT WORDS, USE PRECISE AND CONCRETE\\nLANGUAGE, the proposal will be the guideline for the entire design process so it\\nshould be clear and detailed. \\n'}]\n",
    "print(x[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code=test_tree.compose_unit('XAU')\n",
    "code=U.add_line_num(code)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code='''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class DilatedReformerRootGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.dilated_attention = DilatedAttentionGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.reversible_residual = ReversibleResidualGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **\n",
    "            self.factory_kwargs, **kwarg_all)\n",
    "        self.lsh_attention = LSHAttentionGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.dilated_attention(X, **Z)\n",
    "        X, Z = self.reversible_residual(X, **Z)\n",
    "        X, Z = self.lsh_attention(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class LSHAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class DilatedAttentionGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "class ReversibleResidualGAU(GAUBase): \n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict, device=None, dtype=None, **kwargs): \n",
    "        self.factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\n",
    "\n",
    "@gau_test\n",
    "def test_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau(device=\n",
    "    None, dtype=None) ->None:\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    embed_dim = 16\n",
    "    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n",
    "    Z = {}\n",
    "    gau = DilatedReformerRootGAU(embed_dim=embed_dim, block_loc=(0, 6), kwarg_all={}, device=\n",
    "        device, dtype=dtype)\n",
    "    Y, Z_ = gau(X, **Z)\n",
    "    assert Y.shape == X.shape, 'Output shape should match input shape'\n",
    "    assert isinstance(Z_, dict), 'Z_ should be a dictionary'\n",
    "    print('Input X:', X)\n",
    "    print('Output Y:', Y)\n",
    "    print('Intermediate variables Z_:', Z_)\n",
    "\n",
    "\n",
    "def run_DilatedReformerRootGAU_tests():\n",
    "\ttry:\n",
    "\t\ttest_DilatedReformerRootGAU_unit_test_dilated_reformer_root_gau()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"Error in running unit_test_dilated_reformer_root_gau:\", e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\trun_DilatedReformerRootGAU_tests()\n",
    "'''\n",
    "\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "output=io.StringIO()\n",
    "with redirect_stdout(output):\n",
    "    exec(code)\n",
    "captured = str(output.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ResonanceLOCOSTGAMRoot(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase, gau_test\n",
    "\n",
    "\n",
    "class ResonanceLOCOSTGAMRoot(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.resonance_rope = ResonanceRoPEUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.state_space = StateSpaceUnit(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all)\n",
    "        self.inter_block_comm = InterBlockCommUnit(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y, Z_ = self.resonance_rope(X, **Z)\n",
    "        Y, Z_ = self.state_space(Y, **Z_)\n",
    "        Y, Z_ = self.inter_block_comm(Y, **Z_)\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class ResonanceRoPEUnit(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.freqs = self._init_frequencies(embed_dim)\n",
    "\n",
    "    def _init_frequencies(self, embed_dim):\n",
    "        half_dim = embed_dim // 2\n",
    "        freqs = torch.exp(-torch.arange(half_dim, dtype=torch.float32,\n",
    "            device=self.factory_kwargs['device']) * (math.log(10000.0) / (\n",
    "            half_dim - 1)))\n",
    "        return freqs\n",
    "\n",
    "    def _apply_rope(self, X):\n",
    "        batch_size, seqlen, _ = X.size()\n",
    "        position_ids = torch.arange(seqlen, dtype=X.dtype, device=X.device)\n",
    "        sinusoid_inp = torch.einsum('i,j->ij', position_ids, self.freqs)\n",
    "        sin, cos = sinusoid_inp.sin(), sinusoid_inp.cos()\n",
    "        X1, X2 = X[..., ::2], X[..., 1::2]\n",
    "        Y1 = X1 * cos - X2 * sin\n",
    "        Y2 = X1 * sin + X2 * cos\n",
    "        return torch.stack([Y1, Y2], dim=-1).flatten(-2)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = self._apply_rope(X)\n",
    "        Z_ = Z\n",
    "        return Y, Z_\n",
    "\n",
    "\n",
    "class StateSpaceUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class InterBlockCommUnit(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "    \n",
    "\n",
    "\n",
    "gab_config = {}\n",
    "\n",
    "\n",
    "# Check if to device still work!\n",
    "\n",
    "gab = GAB(128, (0, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_device(model):\n",
    "    \"\"\"\n",
    "    Returns the device of the first parameter found in the given nn.Module.\n",
    "    If the module has no parameters, returns None.\n",
    "    \"\"\"\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "get_module_device(gab.root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gab.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children={\n",
    "    'unit1': 'MockUnit1',\n",
    "    'unit2': 'MockUnit2',\n",
    "}\n",
    "called=['unit1', 'unit2', 'unit1']\n",
    "test_tree.units['DualPathGAU'] = GAUNode('DualPathGAU', None, None, None, called, None, None, children, None)\n",
    "children={\n",
    "    'unit1': 'MockUnit3',\n",
    "    'unit2': 'MockUnit4',\n",
    "}\n",
    "called=['unit2', 'unit2']\n",
    "test_tree.units['MockUnit1'] = GAUNode('MockUnit1', None, None, None, called, None, None, children, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self,r,d,e,**Z):\n",
    "        print('Args in A:',d,e)\n",
    "        print('Kwargs in A:',Z)\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self,_Z,c=1,d=2,e=3,**Z):\n",
    "        args2={'d':d,'e':e}\n",
    "        super().__init__(c,**_Z,**args2)\n",
    "        print('Kwargs in B:',Z)\n",
    "\n",
    "cfg={\n",
    "    'a':1,\n",
    "    'b':2,\n",
    "    'c':3,\n",
    "}\n",
    "\n",
    "B(cfg,**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(X,Z,**kwargs):\n",
    "    print(X)\n",
    "    print(Z)\n",
    "    print(kwargs)\n",
    "\n",
    "def B(X,Z,a,**kwargs):\n",
    "    A(X,Z,**Z)\n",
    "\n",
    "Z={'a':3}\n",
    "B(1,Z,**Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "    '''\n",
    "    GAU Tree Map of Butterfly-AFT Generalized Autoregressive Block (BAGAB):\n",
    "    ButterflyAFTGAU (Rating: 3.5/5)\n",
    "        |- AFTMechanismGAU (Rating: 3.5/5)\n",
    "            |- PositionBiasGAU (Rating: 3.5/5)\n",
    "            |- ElementWiseOperationGAU (Rating: 3.5/5)\n",
    "        |- ButterflyTransformGAU (Rating: 3.5/5)\n",
    "            |- ButterflyLayerGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "            |- ButterflyInitializationGAU (Rating: 3.5/5)\n",
    "                |- ButterflyStageGAU (Rating: 3.5/5)\n",
    "                    |- ButterflyParameterGAU (Unimplemented)\n",
    "                |- ButterflyParameterGAU (Unimplemented)\n",
    "\n",
    "    Implemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ElementWiseOperationGAU, ButterflyAFTGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    Unimplemented Units: AFTMechanismGAU, ButterflyLayerGAU, PositionBiasGAU, ButterflyTransformGAU, ButterflyParameterGAU, ElementWiseOperationGAU, ButterflyInitializationGAU, ButterflyStageGAU\n",
    "    '''\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = ButterflyAFTGAU(embed_dim=embed_dim, block_loc=\n",
    "            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase\n",
    "\n",
    "\n",
    "class ButterflyAFTGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.aft_mechanism = AFTMechanismGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.butterfly_transform = ButterflyTransformGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        aft_output, Z = self.aft_mechanism(X, **Z)\n",
    "        butterfly_output, Z = self.butterfly_transform(aft_output, **Z)\n",
    "        return butterfly_output, Z\n",
    "\n",
    "\n",
    "class ButterflyTransformGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_layer = ButterflyLayerGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_init = ButterflyInitializationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.butterfly_init(X, **Z)\n",
    "        Y, Z = self.butterfly_layer(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ButterflyLayerGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.stages = nn.ModuleList([ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all) for _ in range(kwarg_all.get('num_stages', 4))])\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for stage in self.stages:\n",
    "            X, Z = stage(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "class ButterflyMatrixGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ButterflyInitializationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_size = kwarg_all.get('butterfly_size', embed_dim)\n",
    "        self.butterfly_matrices = nn.ParameterList([nn.Parameter(torch.\n",
    "            randn(self.butterfly_size, self.butterfly_size, **self.\n",
    "            factory_kwargs)) for _ in range(int(torch.log2(torch.tensor(\n",
    "            self.butterfly_size))))])\n",
    "        self.butterfly_stage = ButterflyStageGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "        self.butterfly_parameter = ButterflyParameterGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        for matrix in self.butterfly_matrices:\n",
    "            X = F.linear(X, matrix)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "class ButterflyParameterGAU(GAUBase):\n",
    "    \"\"\"\n",
    "    Generalized Autoregressive Block Unit for learning parameters of Butterfly Factorization.\n",
    "    Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "    Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "    Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "    embed_dim:    The dimension of the input embeddings\n",
    "    block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "    kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.param_size = kwarg_all.get('param_size', 128)\n",
    "        self.butterfly_weights = nn.Parameter(torch.randn(self.param_size,\n",
    "            embed_dim, **self.factory_kwargs))\n",
    "        nn.init.xavier_uniform_(self.butterfly_weights)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X = torch.matmul(X, self.butterfly_weights)\n",
    "        Z_ = {'butterfly_weights': self.butterfly_weights}\n",
    "        return transformed_X, Z_\n",
    "        # transformed_X = F.linear(X, self.butterfly_weights.T)\n",
    "        # return transformed_X, {}\n",
    "\n",
    "\n",
    "class ButterflyComputationGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class ButterflyStageGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.butterfly_params = ButterflyParameterGAU(embed_dim=embed_dim,\n",
    "            block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs,\n",
    "            **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        transformed_X, Z = self.butterfly_params(X, **Z)\n",
    "        assert transformed_X.shape == X.shape, f'Output shape {transformed_X.shape} does not match input shape {X.shape}'\n",
    "        return transformed_X, Z\n",
    "\n",
    "class AFTMechanismGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = PositionBiasGAU(embed_dim=embed_dim, block_loc\n",
    "            =block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all\n",
    "            )\n",
    "        self.element_wise_operation = ElementWiseOperationGAU(embed_dim=\n",
    "            embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.\n",
    "            factory_kwargs, **kwarg_all)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.position_bias(X, **Z)\n",
    "        Y, Z = self.element_wise_operation(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "class ElementWiseOperationGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, embed_dim, **self.\n",
    "            factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        queries = Z.get('queries', X)\n",
    "        keys = Z.get('keys', X)\n",
    "        values = Z.get('values', X)\n",
    "        elementwise_product = queries * keys\n",
    "        elementwise_sum = elementwise_product + values + self.position_bias\n",
    "        return elementwise_sum, {}\n",
    "\n",
    "\n",
    "class PositionBiasGAU(GAUBase):\n",
    "    \"\"\"Position Bias Generalized Autoregressive Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "        embed_dim:    The dimension of the input embeddings\n",
    "        block_loc:    The location of the block within the network, (layer_idx, n_block)\n",
    "        kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "        device=None, dtype=None, **kwargs):\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "        self.position_bias = nn.Parameter(torch.zeros(1, 1, embed_dim, **\n",
    "            self.factory_kwargs))\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        Y = X + self.position_bias\n",
    "        return Y, {}\n",
    "    \n",
    "\n",
    "class GABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100, **kwargs):\n",
    "        super().__init__()\n",
    "        self.gab = GAB(embed_dim, (0,1), **kwargs)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, Z = self.gab(X, **Z)\n",
    "        return self.lm_head(X),X\n",
    "\n",
    "\n",
    "\n",
    "gab_config = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGABLM(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size=100):\n",
    "        super().__init__()\n",
    "        self.butterfly_param = ButterflyParameterGAU(embed_dim, (0,1), {})\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, X, **Z):\n",
    "        X, _ = self.butterfly_param(X, **Z)\n",
    "        return self.lm_head(X), X\n",
    "\n",
    "# Test the simplified model\n",
    "simple_model = GABLM(128)\n",
    "mock_input = torch.randn(1, 10, 128)\n",
    "mock_y = torch.randint(0, 100, (1, 10))\n",
    "\n",
    "optimizer = optim.Adam(simple_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for _ in range(5):  # Run for a few iterations\n",
    "    optimizer.zero_grad()\n",
    "    y, X = simple_model(mock_input)\n",
    "    loss = criterion(y.view(-1, 100), mock_y.view(-1))\n",
    "    loss.backward()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    for name, param in simple_model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"Gradient for {name}: {param.grad.abs().mean()}\")\n",
    "        else:\n",
    "            print(f\"No gradient for {name}\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim \n",
    "\n",
    "model=GABLM(128, **gab_config)\n",
    "vocab_size=100\n",
    "DEFAULT_CONTEXT_LENGTH=10\n",
    "\n",
    "mock_input=torch.randn(1, DEFAULT_CONTEXT_LENGTH, 128)\n",
    "mock_y=torch.randint(0, 100, (1, 10))\n",
    "mock_Z={}\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "y,X=model(mock_input, **mock_Z)\n",
    "loss=nn.CrossEntropyLoss()(y.view(-1, 100), mock_y.view(-1))\n",
    "loss.backward()\n",
    "\n",
    "used_params = set()\n",
    "\n",
    "# Hook to register which parameters are used during the forward pass\n",
    "def hook_fn(module, input, output):\n",
    "    for param in module.parameters():\n",
    "        used_params.add(param)\n",
    "\n",
    "# Register hooks for all submodules\n",
    "hooks = []\n",
    "for submodule in model.modules():\n",
    "    hook = submodule.register_forward_hook(hook_fn)\n",
    "    hooks.append(hook)\n",
    "\n",
    "# Forward pass again to collect used parameters\n",
    "model(mock_input)\n",
    "\n",
    "# Remove all hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "all_gradients_present = True\n",
    "\n",
    "# Check gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if param in used_params and param.grad is None:\n",
    "            # Used parameter without gradient -> Error\n",
    "            print(f\"Error: Used parameter {name} requires gradients but has none.\")\n",
    "            all_gradients_present = False\n",
    "        elif param not in used_params:\n",
    "            # Unused parameter -> Warning\n",
    "            print(f\"Warning: Parameter {name} was not used in the forward pass.\")\n",
    "\n",
    "if all_gradients_present:\n",
    "    print('Differentiability test passed')\n",
    "else:\n",
    "    print('Differentiability test failed due to missing gradients.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INIT_DESIGN_INPUT='''\n",
    "Below is the specification for the GAU you need to refine:\n",
    "Specification: Unit Name: GatedMLP The DynamicGatedMLP is an enhanced version of the GatedMLP that introduces a dynamic gating mechanism capable of selecting between multiple activation functions. This design aims to improve the expressiveness and adaptability of the model by allowing it to dynamically choose the most suitable activation function for different inputs. The unit maintains the same input-output interface as the original GatedMLP, ensuring seamless integration with the existing GPT-2 architecture.\n",
    "\n",
    "\n",
    "Inputs: X\n",
    "Outputs: Y\n",
    "Children list: []\n",
    "\n",
    "Current Implementation: import torch import torch.nn as nn from model_discovery.model.utils.modules import GAUBase, gau_test import torch.nn.functional as F\n",
    "\n",
    "class GatedMLP(GAUBase): \"\"\"Generalized Autoregressive Block Unit Input: X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of new intermediate variables to update the current Z} Constraints: Causal, differentiable, parameter number, complexity, parallelizable...\n",
    "\n",
    "embed_dim:    The dimension of the input embeddings\n",
    "    block_loc:    The location of the block to be composed within the network, (layer_idx, n_block), e.g. (0, 6) for the first block in a network with 6 blocks in total\n",
    "    kwarg_all:    A dictionary of all hyperparameters across all units, use it instead of kwargs to initialize the children units\n",
    "\"\"\"\n",
    "\n",
    "def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n",
    "    device=None, dtype=None, hidden_features=None, out_features=None,\n",
    "    bias=False, multiple_of=128, **kwargs):\n",
    "    self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    super().__init__(embed_dim, block_loc, kwarg_all)\n",
    "    out_features = out_features if out_features is not None else embed_dim\n",
    "    hidden_features = (hidden_features if hidden_features is not None else\n",
    "        int(8 * embed_dim / 3))\n",
    "    hidden_features = (hidden_features + multiple_of - 1\n",
    "        ) // multiple_of * multiple_of\n",
    "    self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n",
    "        self.factory_kwargs)\n",
    "    self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n",
    "        self.factory_kwargs)\n",
    "    self.activations = [F.relu, F.tanh, F.gelu]\n",
    "    self.gate_weights = nn.Parameter(torch.randn(len(self.activations),\n",
    "        hidden_features, **self.factory_kwargs))\n",
    "\n",
    "def _forward(self, X, **Z):\n",
    "    y = self.fc1(X)\n",
    "    y, gate = y.chunk(2, dim=-1)\n",
    "    gate_scores = F.softmax(self.gate_weights, dim=0)\n",
    "    y = sum(gate_scores[i] * self.activations[i](gate) for i in range(\n",
    "        len(self.activations)))\n",
    "    y = self.fc2(y)\n",
    "    return y, {}\n",
    "\n",
    "Review: ### Highlights:\n",
    "\n",
    "Successful Implementation Fix: The designer has successfully addressed the previous implementation error by initializing the activations attribute before its use, resolving the AttributeError.\n",
    "Dynamic Gating Mechanism: The design continues to leverage a dynamic gating mechanism, which is a novel approach that enhances the model's adaptability and expressiveness by selecting between multiple activation functions.\n",
    "Checker Validation: The updated implementation has passed all checker tests, including forward pass, causality, and differentiability, indicating that the GAU is functioning correctly within the model.\n",
    "Concerns:\n",
    "Efficiency Considerations: While the implementation is now functional, the potential computational overhead of evaluating multiple activation functions simultaneously remains a concern. This could impact efficiency, especially in larger models or datasets.\n",
    "Justification of Activation Functions: The choice of activation functions (ReLU, Tanh, GELU) is reasonable, but further theoretical justification or empirical validation would strengthen the design.\n",
    "Accuracy, Robustness, Efficiency, and Scalability:\n",
    "Accuracy and Robustness: The dynamic selection of activation functions should enhance the model's ability to adapt to various input patterns, potentially improving accuracy and robustness.\n",
    "Efficiency: The design could still benefit from optimizations to reduce computational overhead, ensuring it remains efficient in practice.\n",
    "Scalability: The design appears scalable, as indicated by the successful checker tests, but empirical validation on larger datasets would be beneficial.\n",
    "Novelty:\n",
    "The design introduces a novel dynamic gating mechanism, which is a significant improvement over static activation functions.\n",
    "Clarity and Completeness:\n",
    "The design is now clear and complete, with the previous implementation error resolved.\n",
    "Theoretical Soundness:\n",
    "The theoretical foundation of using dynamic gating is sound, and the implementation aligns well with the proposal's objectives.\n",
    "Implementation Feasibility:\n",
    "The current implementation is feasible and has been validated by the checker, indicating it can be integrated into the broader model.\n",
    "Suggestions:\n",
    "Efficiency Optimization: Consider exploring methods to optimize the dynamic gating mechanism to reduce computational overhead, such as selectively applying it or using more efficient computation techniques.\n",
    "Empirical Validation: Plan for empirical validation to confirm the theoretical benefits of the dynamic gating mechanism and the choice of activation functions in practice.\n",
    "Documentation: Ensure thorough documentation of the design choices and their theoretical justifications to aid future refinements and empirical testing.\n",
    "Rating: 4 out of 5 (Passing score >3)\n",
    "\n",
    "Reviewer Suggestions: 1. Efficiency Optimization: Consider exploring methods to optimize the dynamic gating mechanism to reduce computational overhead, such as selectively applying it or using more efficient computation techniques. 2. Empirical Validation: Plan for empirical validation to confirm the theoretical benefits of the dynamic gating mechanism and the choice of activation functions in practice.\n",
    "\n",
    "Documentation: Ensure thorough documentation of the design choices and their theoretical justifications to aid future refinements and empirical testing.\n",
    "Refinement Process\n",
    "If there is a review provided, you should start by reflecting on the feedback. Otherwise, leave reflection empty. The, proceed with the following:\n",
    "\n",
    "New Analysis and Design: - Provide an updated detailed analysis based on the feedback, including your new design direction and justifications. - Include a high-level pseudocode that captures the core of the new design. You should also provide the updated document of the GAU that allows other people to understand the design and implementation without the need to read the code.\n",
    "\n",
    "Implementation: - Provide the full updated implementation of the GAU, following the specified format and templates.\n",
    "\n",
    "Children list: - Provide the list of the children GAUs that are declared in the current GAU. You can declare new children GAUs or preserve the existing ones. If you do not declare any new children GAUs, you should provide the original children GAUs.\n",
    "\n",
    "Log of Changes: - Summarize the key changes you made during the refinement process. Including all code snippets where you made a change wrapped in python .\n",
    "\n",
    "Key Points to Remember:\n",
    "The bug or issue must always be resolved within the current GAU, as other units are either fully implemented and tested or placeholders that do not perform any computation.\n",
    "Ensure the GAU is self-contained, so you won't need to adjust it later when working on other units.\n",
    "The design must align with the original proposal and follow all instructions, templates, and format requirements.\n",
    "Use a top-down approach: break down complex operations into smaller tasks where necessary and declare each of them as a child GAU. Do not make a single unit overly complex.\n",
    "Remember your final goal is to refine the GAU in a way that enhances the overall design, ensuring both correctness and innovation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.prompts.prompts as P\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DESIGNER_SYSTEM=P.GUE_DESIGNER_SYSTEM(GAB_BASE=GAB_BASE,GAU_BASE=GAU_BASE,GAU_TEMPLATE=GAU_TEMPLATE)\n",
    "system=[{\"type\": \"text\", \"text\": DESIGNER_SYSTEM, \"cache_control\": {\"type\": \"ephemeral\"}}]\n",
    "\n",
    "# messages = [\n",
    "#     SystemMessage(system),\n",
    "#     HumanMessage(TEST_INIT_DESIGN_INPUT)\n",
    "# ]\n",
    "\n",
    "message=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": TEST_INIT_DESIGN_INPUT,\n",
    "            \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "\n",
    "SELECTIONS=['GPT2','RMSNorm','GatedMLP','MHA','RotaryPositionalEmbeddings']\n",
    "lc_model = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20240620\", temperature=0,\n",
    "    extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    ")\n",
    "structured_llm = lc_model.with_structured_output(P.gen_GUE_DESIGN_PROPOSAL(SELECTIONS).format,include_raw=True)\n",
    "tools_args=structured_llm.dict()['first']['steps__']['raw']['kwargs']\n",
    "        # tools[0]['cache_control']={\"type\": \"ephemeral\"}\n",
    "\n",
    "RET=anthropic.Anthropic().messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\", # model in config is ignored\n",
    "    max_tokens=8192,\n",
    "    messages=message, \n",
    "    temperature=0,\n",
    "    system=system, # claude does not has system role, system prompt must be passed separately\n",
    "    extra_headers=extra_headers,\n",
    "    **tools_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RET.dict()['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.gen_GUE_DESIGN_PROPOSAL(SELECTIONS).format.model_validate(RET.content[0].input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RET.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm.dict()['first']['steps__']['raw']['kwargs']['tools'][0]['input_schema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Message(id='msg_01Nvch8XBwzwrrMBETHpch49', content=[TextBlock(text=\"Thank you for providing the seed design and references. I'll now propose a design improvement for the GPT2 model, focusing on modifying one of its GAUs.\", type='text'), ToolUseBlock(id='toolu_01PpbFTQ6NDwJJCfj1cRDvFS', input={'selection': 'RotaryPositionalEmbeddings', 'modelname': 'GPT2-RoPE', 'proposal': \"\\n# GPT2 with Enhanced Rotary Position Embeddings (GPT2-RoPE)\\n\\n## Motivation\\n\\nThe current GPT2 model uses a basic implementation of Rotary Positional Embeddings (RoPE). While this is already an improvement over traditional positional encodings, there's room for enhancement, particularly in handling longer sequences and improving the model's ability to capture relative positions more effectively.\\n\\n## Problem Analysis\\n\\n1. **Limited sequence length handling**: The current implementation may not scale well to very long sequences, which is increasingly important in modern language models.\\n2. **Lack of flexibility**: The existing RoPE implementation doesn't allow for easy adjustment of its properties, such as the rate of position information decay.\\n3. **Potential for improved performance**: While RoPE is effective, recent research suggests that more sophisticated implementations can lead to better model performance, especially in tasks requiring understanding of long-range dependencies.\\n\\n## Core Idea and Philosophy\\n\\nWe propose to enhance the RotaryPositionalEmbeddings GAU by incorporating ideas from the RoFormer paper and the more recent developments in rotary embeddings. The core philosophy is to create a more flexible and powerful positional embedding system that can:\\n\\n1. Handle longer sequences more effectively\\n2. Allow for fine-tuning of the position encoding properties\\n3. Potentially improve the model's performance on tasks requiring long-range understanding\\n\\n## Design Plan\\n\\nWe will modify the RotaryPositionalEmbeddings GAU as follows:\\n\\n1. **Implement a more sophisticated RoPE class**: Based on the RoFormer paper and recent developments, we'll create a new RotaryEmbedding class that offers more flexibility and features.\\n\\n2. **Add support for learned frequencies**: This will allow the model to potentially learn better position representations for specific tasks or domains.\\n\\n3. **Implement interpolation factor**: This will enable the model to handle sequences longer than those seen during training more effectively.\\n\\n4. **Add support for XPos (Extrapolatable Position Embeddings)**: This will further enhance the model's ability to handle longer sequences and improve its understanding of relative positions.\\n\\n5. **Implement caching mechanism**: To improve efficiency, especially for autoregressive generation tasks.\\n\\n6. **Add support for different frequency types**: This will allow the embeddings to be optimized for different types of data (e.g., language, images).\\n\\nHere's a high-level overview of the proposed changes:\\n\\n```python\\nclass RotaryEmbedding(nn.Module):\\n    def __init__(\\n        self,\\n        dim,\\n        custom_freqs = None,\\n        freqs_for = 'lang',\\n        theta = 10000,\\n        max_freq = 10,\\n        num_freqs = 1,\\n        learned_freq = False,\\n        use_xpos = False,\\n        xpos_scale_base = 512,\\n        interpolate_factor = 1.,\\n        theta_rescale_factor = 1.,\\n        seq_before_head_dim = False,\\n        cache_if_possible = True\\n    ):\\n        super().__init__()\\n        # Implementation details...\\n\\n    def rotate_queries_or_keys(self, t, seq_dim = None, offset = 0, scale = None):\\n        # Implementation details...\\n\\n    def rotate_queries_and_keys(self, q, k, seq_dim = None):\\n        # Implementation details...\\n\\n    def get_scale(self, t, seq_len = None, offset = 0):\\n        # Implementation details...\\n\\n    def forward(self, t, seq_len = None, offset = 0):\\n        # Implementation details...\\n```\\n\\nThis new implementation will replace the current RotaryPositionalEmbeddings GAU in the GPT2 model.\\n\\n## Conclusion\\n\\nBy enhancing the RotaryPositionalEmbeddings GAU with these improvements, we expect to see several benefits:\\n\\n1. **Improved handling of long sequences**: The model should be able to maintain performance even on sequences longer than those seen during training.\\n2. **Better capture of relative positions**: The enhanced RoPE implementation should allow the model to better understand and utilize relative position information.\\n3. **Increased flexibility**: The new implementation allows for fine-tuning of various aspects of the positional embeddings, which can be optimized for specific tasks or domains.\\n4. **Potential performance improvements**: These enhancements may lead to improved performance on tasks requiring long-range understanding and more nuanced use of positional information.\\n\\nThis modification maintains the overall structure of the GPT2 model while significantly enhancing its ability to handle and utilize positional information, potentially leading to improved performance across a wide range of natural language processing tasks.\\n\\n## References\\n\\n1. Su, J., Lu, Y., Pan, S., Wen, B., & Liu, Y. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv preprint arXiv:2104.09864.\\n2. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint arXiv:2401.04088.\\n\"}, name='GUE_DESIGN_PROPOSAL_format', type='tool_use')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='tool_use', stop_sequence=None, type='message', usage=Usage(input_tokens=16630, output_tokens=1325))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content='Reflection on Feedback:\\nThe reviewer\\'s suggestions highlight the need for efficiency optimization, empirical validation, and thorough documentation. I\\'ll address these points in the refined design.\\n\\nNew Analysis and Design:\\n\\nThe current GatedMLP design introduces a dynamic gating mechanism that allows the model to select between multiple activation functions. While this approach enhances adaptability, it may introduce computational overhead. To address this, we\\'ll implement a more efficient gating mechanism that maintains the benefits of dynamic activation selection while reducing computational costs.\\n\\nUpdated Design Direction:\\n1. Efficient Gating: Instead of applying all activation functions and then combining them, we\\'ll use a lightweight gating network to predict weights for each activation function. This approach reduces the number of computations while still allowing dynamic selection.\\n\\n2. Sparse Activation: We\\'ll introduce a sparsity factor in the gating mechanism, allowing the model to focus on the most relevant activation functions for each input.\\n\\n3. Adaptive Computation: We\\'ll implement a mechanism to dynamically adjust the number of active gates based on the input complexity, further optimizing computational resources.\\n\\nHigh-level Pseudocode:\\n```\\nfunction GatedMLP(X):\\n    hidden = linear_projection(X)\\n    gate_weights = lightweight_gate_network(hidden)\\n    sparse_gate_weights = apply_sparsity(gate_weights)\\n    activated = sum(sparse_gate_weights[i] * activation[i](hidden) for i in range(num_activations))\\n    output = linear_projection(activated)\\n    return output\\n```\\n\\nUpdated GAU Document:\\nThe GatedMLP unit now incorporates an efficient dynamic gating mechanism that selects between multiple activation functions. Key features include:\\n- Lightweight gating network for predicting activation weights\\n- Sparse activation selection to focus on the most relevant functions\\n- Adaptive computation based on input complexity\\n- Maintains the same input-output interface as the original GatedMLP\\n\\nThis design aims to balance expressiveness and computational efficiency, allowing the model to adapt to various input patterns while managing resource usage effectively.\\n\\nImplementation:\\n\\n```python\\n# gau.py\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom model_discovery.model.utils.modules import GAUBase, gau_test\\n\\nclass GatedMLP(GAUBase):\\n    \"\"\"\\n    Generalized Autoregressive Block Unit with efficient dynamic gating\\n    Input:  X: (batch, seqlen, embed_dim), Z: {dict of all intermediate variables} \\n    Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of new intermediate variables to update the current Z}\\n    \"\"\"\\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\\n                 device=None, dtype=None, hidden_features=None, out_features=None,\\n                 bias=False, multiple_of=128, num_activations=3, sparsity_factor=0.5, **kwargs):\\n        self.factory_kwargs = {\\'device\\': device, \\'dtype\\': dtype}\\n        super().__init__(embed_dim, block_loc, kwarg_all)\\n        \\n        out_features = out_features if out_features is not None else embed_dim\\n        hidden_features = hidden_features if hidden_features is not None else int(8 * embed_dim / 3)\\n        hidden_features = (hidden_features + multiple_of - 1) // multiple_of * multiple_of\\n        \\n        self.fc1 = nn.Linear(embed_dim, hidden_features, bias=bias, **self.factory_kwargs)\\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **self.factory_kwargs)\\n        \\n        self.gate_network = nn.Sequential(\\n            nn.Linear(hidden_features, 64, bias=False, **self.factory_kwargs),\\n            nn.ReLU(),\\n            nn.Linear(64, num_activations, bias=False, **self.factory_kwargs)\\n        )\\n        \\n        self.activations = [F.relu, F.tanh, F.gelu]\\n        self.sparsity_factor = sparsity_factor\\n\\n    def _forward(self, X, **Z):\\n        hidden = self.fc1(X)\\n        gate_weights = self.gate_network(hidden)\\n        \\n        # Apply softmax and sp'\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from core library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import importlib\n",
    "import model_discovery.model.composer \n",
    "# importlib.reload(model_discovery.model.composer)\n",
    "from model_discovery.model.composer import GAUBase, GAUTree, check_tree_name, GABComposer\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "import model_discovery.utils as U\n",
    "\n",
    "# dir='/home/junyanc/model_discovery/model_discovery/model/library/core/gpt2/units'\n",
    "dir='C://ChengJunyan1//Research//model_discovery//model_discovery//model//library//core//gpt2//units'\n",
    "tree=GAUTree.load_from_base(dir)\n",
    "\n",
    "print(tree.view()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "gam_prompt_path = 'C://ChengJunyan1//Research//model_discovery//model_discovery//agents//prompts//gam_prompt.py'\n",
    "gau_template_path = 'C://ChengJunyan1//Research//model_discovery//model_discovery//agents//prompts//gau_template.py'\n",
    "GAM_TEMPLATE=open(gam_prompt_path).read()\n",
    "GAU_TEMPLATE=open(gau_template_path).read()\n",
    "\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n",
    "GAB_BASE=inspect.getsource(GABBase)\n",
    "GAB_COMPOSER=inspect.getsource(GABComposer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GAM_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15:21:22:06,218 INFO     [model_agent.py:184] Agent name=`designer`, model_details={\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"temperature\": 0.3\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized dataset wikitext-2 from C:\\ChengJunyan1\\Research\\model_discovery\\data\\wikitext-2\\tokenized\\meta-llama/Llama-2-7b-hf\\2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15:21:22:07,669 INFO     [model_agent.py:184] Agent name=`debugger`, model_details={\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-15:21:22:08,041 INFO     [model_agent.py:184] Agent name=`claude`, model_details={\n",
      "    \"model_name\": \"gpt-4o\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evoname': 'test_evo_000', 'scales': '14M,31M,70M', 'selection_ratio': '0.25', 'select_method': 'random', 'design_budget': '0'}\n",
      "Evolution system initialized with scales: ['14M', '31M', '70M']\n",
      "Current scale: 0\n",
      "Budgets remaining: {'70M': 1, '31M': 4, '14M': 16}\n",
      "Checkpoint directory: C:\\ChengJunyan1\\Research\\model_discovery\\ckpt\\test_evo_000\n",
      "gpt2 tree loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15:21:22:09,423 INFO     [model_agent.py:184] Agent name=`designer`, model_details={\n",
      "    \"model_name\": \"gpt-4o-2024-08-06\",\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"temperature\": 0.3\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized dataset wikitext-2 from C:\\ChengJunyan1\\Research\\model_discovery\\data\\wikitext-2\\tokenized\\meta-llama/Llama-2-7b-hf\\2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15:21:22:10,210 INFO     [model_agent.py:184] Agent name=`debugger`, model_details={\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-15:21:22:10,479 INFO     [model_agent.py:184] Agent name=`claude`, model_details={\n",
      "    \"model_name\": \"gpt-4o\",\n",
      "    \"max_output_tokens\": 8192\n",
      "}\n",
      "2024-09-15:21:22:10,757 INFO     [discover_namespace_packages.py:12] Discovering subpackages in _NamespacePath(['c:\\\\Users\\\\ChengJunyan1\\\\anaconda3\\\\envs\\\\modis\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "2024-09-15:21:22:10,760 INFO     [discover_plugins.py:9] Looking for plugins in pinecone_plugins.inference\n",
      "2024-09-15:21:22:10,761 INFO     [installation.py:10] Installing plugin inference into PineconeGRPC\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "import os,sys\n",
    "import importlib\n",
    "import arxiv\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from model_discovery.evolution import BuildEvolution\n",
    "import model_discovery.utils as U\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "headers={\"X-API-KEY\": os.environ['S2_API_KEY']}\n",
    "paper_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}'\n",
    "\n",
    "\n",
    "strparams=[\n",
    "    f\"evoname=test_evo_000\",\n",
    "    \"scales=14M,31M,70M\",\n",
    "    \"selection_ratio=0.25\",\n",
    "    \"select_method=random\",\n",
    "    \"design_budget=0\",\n",
    "]\n",
    "evo_system = BuildEvolution(\n",
    "    strparams=';'.join(strparams),\n",
    "    do_cache=False,\n",
    "    # cache_type='diskcache',\n",
    ")\n",
    "\n",
    "ptree = evo_system.ptree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15:17:10:35,109 INFO     [discover_namespace_packages.py:12] Discovering subpackages in _NamespacePath(['c:\\\\Users\\\\ChengJunyan1\\\\anaconda3\\\\envs\\\\modis\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "2024-09-15:17:10:35,111 INFO     [discover_plugins.py:9] Looking for plugins in pinecone_plugins.inference\n",
      "2024-09-15:17:10:35,111 INFO     [installation.py:10] Installing plugin inference into PineconeGRPC\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "import model_discovery.agents.search_utils\n",
    "importlib.reload(model_discovery.agents.search_utils)\n",
    "\n",
    "from model_discovery.agents.search_utils import SuperScholarSearcher, pwc_search_patched\n",
    "\n",
    "sss=SuperScholarSearcher(ptree,evo_system.stream)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ChengJunyan1\\Research\\model_discovery\\notebooks\\..\\model_discovery\\model\\library\\tree\\..\\files\n"
     ]
    }
   ],
   "source": [
    "sss._load_libs()\n",
    "# sss._load_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retry when recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4409 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 66/4409 [00:00<00:10, 403.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 194/4409 [00:00<00:05, 794.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 1039/4409 [00:00<00:01, 2761.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 2841/4409 [00:00<00:00, 3827.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n",
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 3448/4409 [00:01<00:00, 4008.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4409/4409 [00:01<00:00, 3700.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "pdfsp_dir=U.pjoin(sss.files_dir,'pdfsp')\n",
    "\n",
    "for i in tqdm(sss.libp):\n",
    "    if 'externalIds' in sss.libp[i]:\n",
    "        if 'ArXiv' in sss.libp[i]['externalIds']:\n",
    "            arxiv_id=sss.libp[i]['externalIds']['ArXiv']\n",
    "            path=U.pjoin(pdfsp_dir,f'{i}.pdf')\n",
    "            arxiv_pdf=f'https://arxiv.org/pdf/{arxiv_id}'\n",
    "            if U.pexists(path):\n",
    "                continue\n",
    "            try:\n",
    "                pdf=urllib.request.urlretrieve(arxiv_pdf, path)\n",
    "                time.sleep(0.01)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
