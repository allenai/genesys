{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place for experimenting the progressive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.utils as U\n",
    "from model_discovery.configs.gam_config import GAMConfig, GAMConfig_14M\n",
    "from model_discovery.model.composer import GAUTree,GAUBase,GAUNode\n",
    "# from model_discovery.evolution import BuildEvolution\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "\n",
    "prompts_dir='../model_discovery/agents/prompts/'\n",
    "gab_py = U.read_file(U.pjoin(prompts_dir,'gab_template.py'))\n",
    "gam_py = U.read_file(U.pjoin(prompts_dir,'gam_prompt.py'))\n",
    "GAU_TEMPLATE = U.read_file(U.pjoin(prompts_dir,'gau_template.py'))\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "class GAU(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
    "        # argv: list of hyperparameters\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\n",
    "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        # COMPLETING THE CODE HERE #\n",
    "        self.token_scorer: GAUBase = TokenScoringGAU(embed_dim, **factory_kwargs)\n",
    "        self.dual_path: GAUBase = DualPathGAU(embed_dim, **factory_kwargs)\n",
    "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim, **factory_kwargs)\n",
    "\n",
    "\n",
    "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "\n",
    "        # THE CODE HERE MUST BE COMPLETED #\n",
    "        # Step 1: Score tokens\n",
    "        X, Z = self.token_scorer(X, **Z)\n",
    "        # Step 2: Route through dual paths\n",
    "        # Step 3: Apply latent attention\n",
    "        Y, Z = self.latent_attention(X, **Z)\n",
    "        X, Z = self.dual_path(X, **Z)\n",
    "\n",
    "        return Y, Z\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reformatted Code:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from model_discovery.model.utils.modules import GAUBase\n",
      "\n",
      "\n",
      "class XAU(GAUBase):\n",
      "    \"\"\"Generalized Autoregressive Block Unit\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
      "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "        super().__init__(embed_dim)\n",
      "        self.token_scorer: GAUBase = TokenScoringGAU(embed_dim=embed_dim,\n",
      "            device=device, dtype=dtype, **kwargs)\n",
      "        self.dual_path: GAUBase = DualPathGAU(embed_dim=embed_dim, device=\n",
      "            device, dtype=dtype, **kwargs)\n",
      "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim=\n",
      "            embed_dim, device=device, dtype=dtype, **kwargs)\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        X, Z = self.token_scorer(X, **Z)\n",
      "        Y, Z = self.latent_attention(X, **Z)\n",
      "        X, Z = self.dual_path(X, **Z)\n",
      "        return Y, Z\n",
      "\n",
      "Errors:\n",
      " []\n",
      "Warnings:\n",
      " []\n",
      "Children Units:\n",
      " {'token_scorer': 'TokenScoringGAU', 'dual_path': 'DualPathGAU', 'latent_attention': 'LatentAttentionGAU'}\n",
      "New Arguments:\n",
      " {}\n",
      "Called Children:\n",
      " ['token_scorer', 'latent_attention', 'dual_path']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "code2 = \"\"\"\n",
    "# gau.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase\n",
    "\n",
    "# Placeholder classes for future implementation\n",
    "class MemoryAccessUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, memory_size, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "class DownsamplingUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, downsample_factor, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "class XAEU(GAUBase):  # This class will be renamed to the unit_name\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "        self.unit: GAUBase = MemoryAccessUnit(embed_dim=embed_dim, device=device)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, Z\n",
    "\"\"\"\n",
    "\n",
    "unit_name = \"XAU\"  # Provide the unit_name to rename GAU class\n",
    "reformatted_code, children_units, new_args, called, errors, warnings = check_and_reformat_gau_code(code, unit_name)\n",
    "print(\"Reformatted Code:\\n\" + reformatted_code)\n",
    "print(\"Errors:\\n\", errors)\n",
    "print(\"Warnings:\\n\", warnings)\n",
    "print(\"Children Units:\\n\", children_units)\n",
    "print(\"New Arguments:\\n\", new_args)\n",
    "print(\"Called Children:\\n\", called)\n",
    "\n",
    "test_tree.add_unit(\n",
    "    unit_name,reformatted_code,new_args,None,called,None,None,children_units,None\n",
    ")\n",
    "test_tree.root=test_tree.units['XAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "children={\n",
    "    'unit1': 'MockUnit1',\n",
    "    'unit2': 'MockUnit2',\n",
    "}\n",
    "called=['unit1', 'unit2', 'unit1']\n",
    "test_tree.units['DualPathGAU'] = GAUNode('DualPathGAU', None, None, None, called, None, None, children, None)\n",
    "children={\n",
    "    'unit1': 'MockUnit3',\n",
    "    'unit2': 'MockUnit4',\n",
    "}\n",
    "called=['unit2', 'unit2']\n",
    "test_tree.units['MockUnit1'] = GAUNode('MockUnit1', None, None, None, called, None, None, children, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'XAU': GAUNode(name='XAU', code='import torch\\nimport torch.nn as nn\\nfrom model_discovery.model.utils.modules import GAUBase\\n\\n\\nclass XAU(GAUBase):\\n    \"\"\"Generalized Autoregressive Block Unit\\n        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\\n        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\\n        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\\n    \"\"\"\\n\\n    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\\n        factory_kwargs = {\\'device\\': device, \\'dtype\\': dtype}\\n        super().__init__(embed_dim)\\n        self.token_scorer: GAUBase = TokenScoringGAU(embed_dim=embed_dim,\\n            device=device, dtype=dtype, **kwargs)\\n        self.dual_path: GAUBase = DualPathGAU(embed_dim=embed_dim, device=\\n            device, dtype=dtype, **kwargs)\\n        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim=\\n            embed_dim, device=device, dtype=dtype, **kwargs)\\n\\n    def _forward(self, X, **Z):\\n        X, Z = self.token_scorer(X, **Z)\\n        Y, Z = self.latent_attention(X, **Z)\\n        X, Z = self.dual_path(X, **Z)\\n        return Y, Z\\n', args={}, desc=None, path=['token_scorer', 'latent_attention', 'dual_path'], review=None, rating=None, children={'token_scorer': 'TokenScoringGAU', 'dual_path': 'DualPathGAU', 'latent_attention': 'LatentAttentionGAU'}, suggestions=None),\n",
       " 'DualPathGAU': GAUNode(name='DualPathGAU', code=None, args=None, desc=None, path=['unit1', 'unit2', 'unit1'], review=None, rating=None, children={'unit1': 'MockUnit1', 'unit2': 'MockUnit2'}, suggestions=None),\n",
       " 'MockUnit1': GAUNode(name='MockUnit1', code=None, args=None, desc=None, path=['unit2', 'unit2'], review=None, rating=None, children={'unit1': 'MockUnit3', 'unit2': 'MockUnit4'}, suggestions=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tree.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAU Tree Map of TestTree:\n",
      "  root: XAU (Exec path: token_scorer->latent_attention->dual_path)\n",
      "       |- token_scorer: TokenScoringGAU (Unimplemented)\n",
      "       |- dual_path: DualPathGAU (Exec path: unit1->unit2->unit1)\n",
      "           |- unit1: MockUnit1 (Exec path: unit2->unit2)\n",
      "               |- unit1: MockUnit3 (Unimplemented)\n",
      "               |- unit2: MockUnit4 (Unimplemented)\n",
      "           |- unit2: MockUnit2 (Unimplemented)\n",
      "       |- latent_attention: LatentAttentionGAU (Unimplemented)\n",
      "\n",
      "Implemented Units: XAU, MockUnit1, DualPathGAU\n",
      "Unimplemented Units: LatentAttentionGAU, MockUnit4, MockUnit3, MockUnit2, TokenScoringGAU\n"
     ]
    }
   ],
   "source": [
    "def _view(self,_name,obj='root',path='',node=None,pstr='',unimplemented=set()):\n",
    "    # create a string representation of the tree\n",
    "    name=obj+': '+_name\n",
    "    if node is None: \n",
    "        name += ' (Unimplemented)'\n",
    "        unimplemented.add(_name)\n",
    "    else:\n",
    "        name += f' (Exec path: {'->'.join(node.path)})'\n",
    "    if path!='':\n",
    "        level=len(path.split('.'))\n",
    "        name='    '*level+' |- '+name\n",
    "        path+='.'+_name\n",
    "    else:\n",
    "        pstr+=f'GAU Tree Map of {self.name}:\\n'\n",
    "        path=_name\n",
    "    pstr+='  '+name+'\\n'\n",
    "    if node is not None:\n",
    "        for child, child_unit in node.children.items():\n",
    "            child_node = self.units.get(child_unit,None)\n",
    "            pstr,unimplemented=_view(self,child_unit,child,path,child_node,pstr,unimplemented)\n",
    "    return pstr,unimplemented\n",
    "\n",
    "def view(self):\n",
    "    pstr,unimplemented=_view(self,self.root.name,node=self.root)\n",
    "    implemented = set(self.units.keys())\n",
    "    pstr+='\\nImplemented Units: '+', '.join(implemented)\n",
    "    if len(unimplemented)>0:\n",
    "        pstr+='\\nUnimplemented Units: '+', '.join(unimplemented)\n",
    "    else:\n",
    "        pstr+='\\nAll units are implemented.'\n",
    "    return pstr\n",
    "\n",
    "print(view(test_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch\n",
      "import torch.nn as nn\n",
      "from model_discovery.model.utils.modules import GAUBase\n",
      "\n",
      "\n",
      "class XAU(GAUBase):\n",
      "    \"\"\"Generalized Autoregressive Block Unit\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
      "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "        super().__init__(embed_dim)\n",
      "        self.token_scorer: GAUBase = TokenScoringGAU(embed_dim=embed_dim,\n",
      "            device=device, dtype=dtype, **kwargs)\n",
      "        self.dual_path: GAUBase = DualPathGAU(embed_dim=embed_dim, device=\n",
      "            device, dtype=dtype, **kwargs)\n",
      "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim=\n",
      "            embed_dim, device=device, dtype=dtype, **kwargs)\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        X, Z = self.token_scorer(X, **Z)\n",
      "        Y, Z = self.latent_attention(X, **Z)\n",
      "        X, Z = self.dual_path(X, **Z)\n",
      "        return Y, Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(reformatted_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gab_template='''\n",
    "# gab.py    # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GABBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "    def __init__(self,embed_dim: int, block_loc: tuple, device=None,dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
    "        factory_kwargs = {{\"device\": device, \"dtype\": dtype}} # remember to pass it to nn layers\n",
    "        super().__init__(embed_dim, block_loc) # DO NOT CHANGE THIS LINE #\n",
    "        self.root = {ROOT_UNIT_NAME}(embed_dim, embed_dim=embed_dim, device=device, dtype=dtype, **kwargs)\n",
    "\n",
    "    def _forward(self, X, *Z): \n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# gab.py    # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "from model_discovery.model.utils.modules import GABBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
      "\n",
      "\n",
      "class GAB(GABBase):\n",
      "    def __init__(self,embed_dim: int, block_loc: tuple, device=None,dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to nn layers\n",
      "        super().__init__(embed_dim, block_loc) # DO NOT CHANGE THIS LINE #\n",
      "        self.root = XAU(embed_dim, embed_dim=embed_dim, device=device, dtype=dtype, **kwargs)\n",
      "\n",
      "    def _forward(self, X, *Z): \n",
      "        X, Z = self.root(X, **Z)\n",
      "        return X, Z\n",
      "\n",
      "\n",
      "from model_discovery.model.utils.modules import GAUBase\n",
      "\n",
      "\n",
      "class XAU(GAUBase):\n",
      "    \"\"\"Generalized Autoregressive Block Unit\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
      "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "        super().__init__(embed_dim)\n",
      "        self.token_scorer: GAUBase = TokenScoringGAU(embed_dim=embed_dim,\n",
      "            device=device, dtype=dtype, **kwargs)\n",
      "        self.dual_path: GAUBase = DualPathGAU(embed_dim=embed_dim, device=\n",
      "            device, dtype=dtype, **kwargs)\n",
      "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim=\n",
      "            embed_dim, device=device, dtype=dtype, **kwargs)\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        X, Z = self.token_scorer(X, **Z)\n",
      "        Y, Z = self.latent_attention(X, **Z)\n",
      "        X, Z = self.dual_path(X, **Z)\n",
      "        return Y, Z\n",
      "\n",
      "\n",
      "class TokenScoringGAU(GAUBase): \n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "        super().__init__(embed_dim) \n",
      "        \n",
      "    def _forward(self, X, **Z): \n",
      "        return X\n",
      "\n",
      "\n",
      "class LatentAttentionGAU(GAUBase): \n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "        super().__init__(embed_dim) \n",
      "        \n",
      "    def _forward(self, X, **Z): \n",
      "        return X\n",
      "\n",
      "\n",
      "class DualPathGAU(GAUBase): \n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "        super().__init__(embed_dim) \n",
      "        \n",
      "    def _forward(self, X, **Z): \n",
      "        return X\n",
      "\n",
      "\n",
      "gab_config = {}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import astor\n",
    "from typing import List\n",
    "\n",
    "def replace_from_second(text, old, new):\n",
    "    first_part, remaining = text.split(old, 1)\n",
    "    remaining = remaining.replace(old, new)\n",
    "    return first_part + old + remaining\n",
    "\n",
    "class GABComposer:\n",
    "    \n",
    "    def generate_gab_code(self,tree):\n",
    "        root_node = tree.root\n",
    "        generated_code = []\n",
    "        \n",
    "        # Recursively generate code for the root and its children\n",
    "        self.generate_node_code(root_node.name, generated_code, tree.units)\n",
    "        \n",
    "        # Combine all generated code into a single Python file content\n",
    "        gau_code = \"\\n\".join(generated_code)\n",
    "\n",
    "        gathered_args={}\n",
    "        for unit in tree.units.values():\n",
    "            gathered_args.update(unit.args)\n",
    "        gab_code=gab_template.format(ROOT_UNIT_NAME=root_node.name)\n",
    "\n",
    "        cfg_code=f'gab_config = {json.dumps(gathered_args)}'\n",
    "\n",
    "        compoesed_code = f'{gab_code}\\n\\n{gau_code}\\n\\n{cfg_code}'\n",
    "\n",
    "        compoesed_code=replace_from_second(compoesed_code,'import torch\\n','')\n",
    "        compoesed_code=replace_from_second(compoesed_code,'import torch.nn as nn\\n','')\n",
    "        compoesed_code=replace_from_second(compoesed_code,'from model_discovery.model.utils.modules import GAUBase\\n','')\n",
    "\n",
    "        return compoesed_code\n",
    "\n",
    "\n",
    "    # Recursive function to generate code for a node and its children\n",
    "    def generate_node_code(self, unit_name, generated_code: List[str], units):\n",
    "        # Check if the node exists in units\n",
    "        if unit_name not in units:\n",
    "            # If the node does not exist in units, create a placeholder\n",
    "            generated_code.append(self.create_placeholder_class(unit_name))\n",
    "        else:\n",
    "            node = units[unit_name]\n",
    "            generated_code.append(node.code)\n",
    "            \n",
    "            # Recursively generate code for children\n",
    "            children_units=set()\n",
    "            for child_name, child_unit_name in node.children.items():\n",
    "                children_units.add(child_unit_name)\n",
    "            for child_unit in children_units:\n",
    "                self.generate_node_code(child_unit, generated_code, units)\n",
    "\n",
    "    # Function to create a placeholder class for a GAUNode\n",
    "    def create_placeholder_class(self, unit_name) -> str:\n",
    "        class_template = f\"\"\"\n",
    "class {unit_name}(GAUBase): \n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \n",
    "        factory_kwargs = {{\"device\": device, \"dtype\": dtype}} \n",
    "        super().__init__(embed_dim) \n",
    "        \n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\"\"\"\n",
    "        return class_template\n",
    "\n",
    "    # Function to convert the generated code to AST using ast and astor\n",
    "    def convert_code_to_ast(self, code: str):\n",
    "        try:\n",
    "            return ast.parse(code)\n",
    "        except SyntaxError as e:\n",
    "            print(f\"Syntax error in code: {code}\")\n",
    "            raise e\n",
    "\n",
    "    # Function to convert AST back to Python code using astor\n",
    "    def convert_ast_to_code(self, ast_tree: ast.AST) -> str:\n",
    "        return astor.to_source(ast_tree)\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "generated_code = GABComposer().generate_gab_code(test_tree)\n",
    "print(generated_code)  # This will print the final Python code for the entire GAUTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# gau.py\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "from model_discovery.model.utils.modules import GAUBase\n",
      "\n",
      "# Placeholder imports for future GAUs\n",
      "# from gau import RandomizedAttentionUnit, HierarchicalCompositionUnit\n",
      "\n",
      "class HRAB(GAUBase):\n",
      " \"\"\"Hierarchical Randomized Attention Block Unit\n",
      " Input: X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      " Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      " Constraints: Causal, differentiable, parameter number, complexity, parallelizable\n",
      " \"\"\"\n",
      " def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
      " factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      " super().__init__(embed_dim)\n",
      " \n",
      " # Initialize the Randomized Attention Unit\n",
      " self.randomized_attention: GAUBase = RandomizedAttentionUnit(embed_dim, **factory_kwargs, **kwargs)\n",
      " \n",
      " # Initialize the Hierarchical Composition Unit\n",
      " self.hierarchical_composition: GAUBase = HierarchicalCompositionUnit(embed_dim, **factory_kwargs, **kwargs)\n",
      "\n",
      " def _forward(self, X, **Z):\n",
      " # Apply randomized attention to capture local dependencies\n",
      " Y, Z = self.randomized_attention(X, **Z)\n",
      " \n",
      " # Apply hierarchical composition to capture global dependencies\n",
      " Y, Z = self.hierarchical_composition(Y, **Z)\n",
      " \n",
      " return Y, Z\n",
      "\n",
      "# Placeholder classes for future implementation\n",
      "class RandomizedAttentionUnit(GAUBase):\n",
      " def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
      " super().__init__(embed_dim)\n",
      " # Implementation will be added later\n",
      "\n",
      " def _forward(self, X, **Z):\n",
      " # Placeholder forward method\n",
      " return X, {}\n",
      "\n",
      "class HierarchicalCompositionUnit(GAUBase):\n",
      " def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
      " super().__init__(embed_dim)\n",
      " # Implementation will be added later\n",
      "\n",
      " def _forward(self, X, **Z):\n",
      " # Placeholder forward method\n",
      " return X, {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_report=r'''\n",
    "Checking the designed model...\n",
    "Checking code format...\n",
    "Code format is correct and reformatted.\n",
    "\n",
    "\n",
    "Warnings:\n",
    "\n",
    "The super().__init__(embed_dim, block_loc) call in GAB is force overwritten by the reformatter. It may cause error if you modified this line.\n",
    "\n",
    "Error: Model initialization failed with error: Expected size for first two dimensions of batch2 tensor to be: [2, 128] but got: [2, 2048].\n",
    "Full Traceback: \n",
    "Traceback (most recent call last):\n",
    "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\agents\\roles\\checker.py\", line 835, in check\n",
    "    glm(mock_input) # super slow as well, why??? but its only for the first time initialization\n",
    "    ^^^^^^^^^^^^^^^\n",
    "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
    "    return self._call_impl(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
    "    return forward_call(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\gam.py\", line 399, in forward\n",
    "    hidden_states = self.backbone(input_ids, **gab_kwargs)\n",
    "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
    "    return self._call_impl(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
    "    return forward_call(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\gam.py\", line 285, in forward\n",
    "    hidden_states, residual, intermediate_vars = block(\n",
    "                                                 ^^^^^^\n",
    "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
    "    return self._call_impl(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
    "    return forward_call(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\gam.py\", line 106, in forward\n",
    "    hidden_states,intermediate_vars = self.gab(hidden_states, **intermediate_vars)\n",
    "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
    "    return self._call_impl(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
    "    return forward_call(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\utils\\modules.py\", line 29, in forward\n",
    "    Y = self._forward(X, **Z)\n",
    "        ^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"<string>\", line 16, in _forward\n",
    "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
    "    return self._call_impl(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
    "    return forward_call(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\utils\\modules.py\", line 58, in forward\n",
    "    Y = self._forward(X, **_Z)\n",
    "        ^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"<string>\", line 41, in _forward\n",
    "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
    "    return self._call_impl(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
    "    return forward_call(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\utils\\modules.py\", line 58, in forward\n",
    "    Y = self._forward(X, **_Z)\n",
    "        ^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"<string>\", line 77, in _forward\n",
    "RuntimeError: Expected size for first two dimensions of batch2 tensor to be: [2, 128] but got: [2, 2048].\n",
    "\n",
    "Hint: 1. if it is a dtype or device error, check whether the factory kwargs are passed to the layers, and whether you manually designate a type instead of apply the type from factory kwargs or the input's type during conversion or creating of an variable. 2. If it is a shape error, check whether the output shape is equal to the input shape. The output shape of GAB should be the same as the input. 3. Always remember to follow the template and do not implement redundant part like embedding layer.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "gabcode=r'''import torch\n",
    "import torch.nn as nn\n",
    "from model_discovery.model.utils.modules import GABBase\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n",
    "        =None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, block_loc)\n",
    "        self.root = RootGAU(embed_dim=embed_dim, device=device, dtype=dtype,\n",
    "            **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase\n",
    "\n",
    "\n",
    "class RootGAU(GAUBase):\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim)\n",
    "        self.meta_sorting: GAUBase = MetaSortingGAU(embed_dim=embed_dim,\n",
    "            device=device, dtype=dtype, **kwargs)\n",
    "        self.chunked_attention: GAUBase = ChunkedAttentionGAU(embed_dim=\n",
    "            embed_dim, device=device, dtype=dtype, **kwargs)\n",
    "        self.hybrid_attention: GAUBase = HybridAttentionGAU(embed_dim=\n",
    "            embed_dim, device=device, dtype=dtype, **kwargs)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        X, Z = self.meta_sorting(X, **Z)\n",
    "        X, Z = self.chunked_attention(X, **Z)\n",
    "        Y, Z = self.hybrid_attention(X, **Z)\n",
    "        return Y, Z\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def sinkhorn_balancing(log_alpha, n_iters=5):\n",
    "    \"\"\"Applies Sinkhorn balancing to ensure doubly stochastic matrix.\"\"\"\n",
    "    for _ in range(n_iters):\n",
    "        log_alpha = log_alpha - torch.logsumexp(log_alpha, dim=-1, keepdim=True\n",
    "            )\n",
    "        log_alpha = log_alpha - torch.logsumexp(log_alpha, dim=-2, keepdim=True\n",
    "            )\n",
    "    return log_alpha\n",
    "\n",
    "\n",
    "class MetaSortingGAU(GAUBase):\n",
    "    \"\"\"Meta Sorting Generalized Autoregressive Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim)\n",
    "        self.sorting_network = nn.Linear(embed_dim, embed_dim, **factory_kwargs\n",
    "            )\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        latent_permutations = self.sorting_network(X)\n",
    "        balanced_permutations = sinkhorn_balancing(latent_permutations)\n",
    "        balanced_permutations = F.softmax(balanced_permutations, dim=-1)\n",
    "        sorted_sequence = torch.matmul(balanced_permutations, X)\n",
    "        truncated_sequence = sorted_sequence\n",
    "        assert truncated_sequence.shape == X.shape, 'Output shape must match input shape'\n",
    "        return truncated_sequence, Z\n",
    "\n",
    "\n",
    "class HybridAttentionGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "class ChunkedAttentionGAU(GAUBase):\n",
    "\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X\n",
    "\n",
    "\n",
    "gab_config = {}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking the designed model...\n",
      "Checking code format...\n",
      "Code format is correct and reformatted.\n",
      "\n",
      "\n",
      "Warnings:\n",
      "\n",
      "The super().__init__(embed_dim, block_loc) call in GAB is force overwritten by the reformatter. It may cause error if you modified this line.\n",
      "\n",
      "Error: Model initialization failed with error: Expected size for first two dimensions of batch2 tensor to be: [2, 128] but got: [2, 2048].\n",
      "Full Traceback: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\agents\\roles\\checker.py\", line 835, in check\n",
      "    glm(mock_input) # super slow as well, why??? but its only for the first time initialization\n",
      "    ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\gam.py\", line 399, in forward\n",
      "    hidden_states = self.backbone(input_ids, **gab_kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\gam.py\", line 285, in forward\n",
      "    hidden_states, residual, intermediate_vars = block(\n",
      "                                                 ^^^^^^\n",
      "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\gam.py\", line 106, in forward\n",
      "    hidden_states,intermediate_vars = self.gab(hidden_states, **intermediate_vars)\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\utils\\modules.py\", line 29, in forward\n",
      "    Y = self._forward(X, **Z)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"gab.py\", line 16:         X, Z = self.root(X, **Z), in _forward\n",
      "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\utils\\modules.py\", line 58, in forward\n",
      "    Y = self._forward(X, **_Z)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"gab.py\", line 41:         X, Z = self.meta_sorting(X, **Z), in _forward\n",
      "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ChengJunyan1\\Research\\model_discovery\\model_discovery\\model\\utils\\modules.py\", line 58, in forward\n",
      "    Y = self._forward(X, **_Z)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"gab.py\", line 77:         sorted_sequence = torch.matmul(balanced_permutations, X), in _forward\n",
      "RuntimeError: Expected size for first two dimensions of batch2 tensor to be: [2, 128] but got: [2, 2048].\n",
      "\n",
      "Hint: 1. if it is a dtype or device error, check whether the factory kwargs are passed to the layers, and whether you manually designate a type instead of apply the type from factory kwargs or the input's type during conversion or creating of an variable. 2. If it is a shape error, check whether the output shape is equal to the input shape. The output shape of GAB should be the same as the input. 3. Always remember to follow the template and do not implement redundant part like embedding layer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gabcode_lines=gabcode.split('\\n')\n",
    "new_check_report=[]\n",
    "for line in check_report.split('\\n'):\n",
    "    if 'File \"<string>\", line' in line:\n",
    "        line=line.replace('File \"<string>\", line','File \"gab.py\", line')\n",
    "        line_num=int(line.split('File \"gab.py\", line ')[-1].split(',')[0].strip())\n",
    "        line=line.replace(f'line {line_num}',f'line {line_num}: {gabcode_lines[line_num-1]}')\n",
    "    new_check_report.append(line)\n",
    "new_check_report='\\n'.join(new_check_report)\n",
    "print(new_check_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
