{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place for experimenting the progressive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChengJunyan1\\anaconda3\\envs\\modis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ChengJunyan1\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import yaml\n",
    "import inspect\n",
    "import importlib\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import model_discovery.utils as U\n",
    "from model_discovery.configs.gam_config import GAMConfig, GAMConfig_14M\n",
    "from model_discovery.model.composer import GAUTree,GAUBase\n",
    "# from model_discovery.evolution import BuildEvolution\n",
    "from model_discovery.agents.flow.gau_utils import check_and_reformat_gau_code\n",
    "\n",
    "ckpt_dir = os.environ['CKPT_DIR']\n",
    "lib_dir = U.pjoin(ckpt_dir, 'test_composer', 'lib')\n",
    "test_tree = GAUTree('TestTree', None, None, None,None,lib_dir)\n",
    "\n",
    "prompts_dir='../model_discovery/agents/prompts/'\n",
    "gab_py = U.read_file(U.pjoin(prompts_dir,'gab_template.py'))\n",
    "gam_py = U.read_file(U.pjoin(prompts_dir,'gam_prompt.py'))\n",
    "GAU_TEMPLATE = U.read_file(U.pjoin(prompts_dir,'gau_template.py'))\n",
    "GAU_BASE=inspect.getsource(GAUBase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "code='''\n",
    "# gau.py   # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "\n",
    "# YOU CAN IMPORT MORE MODULES HERE #\n",
    "\n",
    "# YOU CAN DEFINE MORE CLASSES OR FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "class GAU(GAUBase): # DO NOT CHANGE THE NAME OF THIS CLASS\n",
    "    \"\"\"Generalized Autoregressive Block Unit\n",
    "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
    "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
    "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS WITH OPTIONAL DEFAULT VALUES, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
    "        # argv: list of hyperparameters\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to all nn layers\n",
    "        super().__init__(embed_dim) # DO NOT CHANGE THIS LINE #\n",
    "\n",
    "        # COMPLETING THE CODE HERE #\n",
    "        self.token_scorer: GAUBase = TokenScoringGAU(embed_dim, **factory_kwargs)\n",
    "        self.dual_path: GAUBase = DualPathGAU(embed_dim, **factory_kwargs)\n",
    "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim, **factory_kwargs)\n",
    "\n",
    "\n",
    "    # YOU CAN ADD MORE FUNCTIONS HERE #\n",
    "\n",
    "\n",
    "    def _forward(self, X, **Z): \n",
    "\n",
    "        # THE CODE HERE MUST BE COMPLETED #\n",
    "        # Step 1: Score tokens\n",
    "        X, Z = self.token_scorer(X, **Z)\n",
    "        # Step 2: Route through dual paths\n",
    "        # Step 3: Apply latent attention\n",
    "        Y, Z = self.latent_attention(X, **Z)\n",
    "        X, Z = self.dual_path(X, **Z)\n",
    "\n",
    "        return Y, Z\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reformatted Code:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from model_discovery.model.utils.modules import GAUBase\n",
      "\n",
      "\n",
      "class XAU(GAUBase):\n",
      "    \"\"\"Generalized Autoregressive Block Unit\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
      "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "        super().__init__(embed_dim)\n",
      "        self.token_scorer: GAUBase = TokenScoringGAU(embed_dim=embed_dim,\n",
      "            device=device, dtype=dtype, **kwargs)\n",
      "        self.dual_path: GAUBase = DualPathGAU(embed_dim=embed_dim, device=\n",
      "            device, dtype=dtype, **kwargs)\n",
      "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim=\n",
      "            embed_dim, device=device, dtype=dtype, **kwargs)\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        X, Z = self.token_scorer(X, **Z)\n",
      "        Y, Z = self.latent_attention(X, **Z)\n",
      "        X, Z = self.dual_path(X, **Z)\n",
      "        return Y, Z\n",
      "\n",
      "Errors:\n",
      " []\n",
      "Warnings:\n",
      " []\n",
      "Children Units:\n",
      " {'token_scorer': 'TokenScoringGAU', 'dual_path': 'DualPathGAU', 'latent_attention': 'LatentAttentionGAU'}\n",
      "New Arguments:\n",
      " {}\n",
      "Called Children:\n",
      " ['token_scorer', 'latent_attention', 'dual_path']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "code2 = \"\"\"\n",
    "# gau.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GAUBase\n",
    "\n",
    "# Placeholder classes for future implementation\n",
    "class MemoryAccessUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, memory_size, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "class DownsamplingUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, downsample_factor, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, {}\n",
    "\n",
    "class XAEU(GAUBase):  # This class will be renamed to the unit_name\n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None):\n",
    "        super().__init__(embed_dim)\n",
    "        self.unit: GAUBase = MemoryAccessUnit(embed_dim=embed_dim, device=device)\n",
    "\n",
    "    def _forward(self, X, **Z):\n",
    "        return X, Z\n",
    "\"\"\"\n",
    "\n",
    "unit_name = \"XAU\"  # Provide the unit_name to rename GAU class\n",
    "reformatted_code, children_units, new_args, called, errors, warnings = check_and_reformat_gau_code(code, unit_name)\n",
    "print(\"Reformatted Code:\\n\" + reformatted_code)\n",
    "print(\"Errors:\\n\", errors)\n",
    "print(\"Warnings:\\n\", warnings)\n",
    "print(\"Children Units:\\n\", children_units)\n",
    "print(\"New Arguments:\\n\", new_args)\n",
    "print(\"Called Children:\\n\", called)\n",
    "\n",
    "\n",
    "\n",
    "test_tree.add_unit(\n",
    "    unit_name,reformatted_code,new_args,None,called,None,None,None,children_units,None\n",
    ")\n",
    "test_tree.root=test_tree.units['XAU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch\n",
      "import torch.nn as nn\n",
      "from model_discovery.model.utils.modules import GAUBase\n",
      "\n",
      "\n",
      "class XAU(GAUBase):\n",
      "    \"\"\"Generalized Autoregressive Block Unit\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
      "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "        super().__init__(embed_dim)\n",
      "        self.token_scorer: GAUBase = TokenScoringGAU(embed_dim=embed_dim,\n",
      "            device=device, dtype=dtype, **kwargs)\n",
      "        self.dual_path: GAUBase = DualPathGAU(embed_dim=embed_dim, device=\n",
      "            device, dtype=dtype, **kwargs)\n",
      "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim=\n",
      "            embed_dim, device=device, dtype=dtype, **kwargs)\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        X, Z = self.token_scorer(X, **Z)\n",
      "        Y, Z = self.latent_attention(X, **Z)\n",
      "        X, Z = self.dual_path(X, **Z)\n",
      "        return Y, Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(reformatted_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gab_template='''\n",
    "# gab.py    # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_discovery.model.utils.modules import GABBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
    "\n",
    "\n",
    "class GAB(GABBase):\n",
    "    def __init__(self,embed_dim: int, block_loc: tuple, device=None,dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
    "        factory_kwargs = {{\"device\": device, \"dtype\": dtype}} # remember to pass it to nn layers\n",
    "        super().__init__(embed_dim, block_loc) # DO NOT CHANGE THIS LINE #\n",
    "        self.root = {ROOT_UNIT_NAME}(embed_dim, embed_dim=embed_dim, device=device, dtype=dtype, **kwargs)\n",
    "\n",
    "    def _forward(self, X, *Z): \n",
    "        X, Z = self.root(X, **Z)\n",
    "        return X, Z\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# gab.py    # DO NOT CHANGE OR REMOVE THE MAKK HERE, KEEP IT ALWAYS THE FIRST LINE #\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "from model_discovery.model.utils.modules import GABBase # DO NOT CHANGE THIS IMPORT STATEMENT #\n",
      "\n",
      "\n",
      "class GAB(GABBase):\n",
      "    def __init__(self,embed_dim: int, block_loc: tuple, device=None,dtype=None,**kwargs): # YOU CAN ADD MORE ARGUMENTS, BUT YOU HAVE TO HAVE embed_dim, device, dtype AS THE ARGUTMENTS #\n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype} # remember to pass it to nn layers\n",
      "        super().__init__(embed_dim, block_loc) # DO NOT CHANGE THIS LINE #\n",
      "        self.root = XAU(embed_dim, embed_dim=embed_dim, device=device, dtype=dtype, **kwargs)\n",
      "\n",
      "    def _forward(self, X, *Z): \n",
      "        X, Z = self.root(X, **Z)\n",
      "        return X, Z\n",
      "\n",
      "\n",
      "from model_discovery.model.utils.modules import GAUBase\n",
      "\n",
      "\n",
      "class XAU(GAUBase):\n",
      "    \"\"\"Generalized Autoregressive Block Unit\n",
      "        Input:        X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      "        Output:       Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      "        Constraints:  Causal, differentiable, parameter number, complexity, parallelizable\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
      "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "        super().__init__(embed_dim)\n",
      "        self.token_scorer: GAUBase = TokenScoringGAU(embed_dim=embed_dim,\n",
      "            device=device, dtype=dtype, **kwargs)\n",
      "        self.dual_path: GAUBase = DualPathGAU(embed_dim=embed_dim, device=\n",
      "            device, dtype=dtype, **kwargs)\n",
      "        self.latent_attention: GAUBase = LatentAttentionGAU(embed_dim=\n",
      "            embed_dim, device=device, dtype=dtype, **kwargs)\n",
      "\n",
      "    def _forward(self, X, **Z):\n",
      "        X, Z = self.token_scorer(X, **Z)\n",
      "        Y, Z = self.latent_attention(X, **Z)\n",
      "        X, Z = self.dual_path(X, **Z)\n",
      "        return Y, Z\n",
      "\n",
      "\n",
      "class TokenScoringGAU(GAUBase): \n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "        super().__init__(embed_dim) \n",
      "        \n",
      "    def _forward(self, X, **Z): \n",
      "        return X\n",
      "\n",
      "\n",
      "class LatentAttentionGAU(GAUBase): \n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "        super().__init__(embed_dim) \n",
      "        \n",
      "    def _forward(self, X, **Z): \n",
      "        return X\n",
      "\n",
      "\n",
      "class DualPathGAU(GAUBase): \n",
      "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \n",
      "        factory_kwargs = {\"device\": device, \"dtype\": dtype} \n",
      "        super().__init__(embed_dim) \n",
      "        \n",
      "    def _forward(self, X, **Z): \n",
      "        return X\n",
      "\n",
      "\n",
      "gab_config = {}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import astor\n",
    "from typing import List\n",
    "\n",
    "def replace_from_second(text, old, new):\n",
    "    first_part, remaining = text.split(old, 1)\n",
    "    remaining = remaining.replace(old, new)\n",
    "    return first_part + old + remaining\n",
    "\n",
    "class GABComposer:\n",
    "    \n",
    "    def generate_gab_code(self,tree):\n",
    "        root_node = tree.root\n",
    "        generated_code = []\n",
    "        \n",
    "        # Recursively generate code for the root and its children\n",
    "        self.generate_node_code(root_node.name, generated_code, tree.units)\n",
    "        \n",
    "        # Combine all generated code into a single Python file content\n",
    "        gau_code = \"\\n\".join(generated_code)\n",
    "\n",
    "        gathered_args={}\n",
    "        for unit in tree.units.values():\n",
    "            gathered_args.update(unit.args)\n",
    "        gab_code=gab_template.format(ROOT_UNIT_NAME=root_node.name)\n",
    "\n",
    "        cfg_code=f'gab_config = {json.dumps(gathered_args)}'\n",
    "\n",
    "        compoesed_code = f'{gab_code}\\n\\n{gau_code}\\n\\n{cfg_code}'\n",
    "\n",
    "        compoesed_code=replace_from_second(compoesed_code,'import torch\\n','')\n",
    "        compoesed_code=replace_from_second(compoesed_code,'import torch.nn as nn\\n','')\n",
    "        compoesed_code=replace_from_second(compoesed_code,'from model_discovery.model.utils.modules import GAUBase\\n','')\n",
    "\n",
    "        return compoesed_code\n",
    "\n",
    "\n",
    "    # Recursive function to generate code for a node and its children\n",
    "    def generate_node_code(self, unit_name, generated_code: List[str], units):\n",
    "        # Check if the node exists in units\n",
    "        if unit_name not in units:\n",
    "            # If the node does not exist in units, create a placeholder\n",
    "            generated_code.append(self.create_placeholder_class(unit_name))\n",
    "        else:\n",
    "            node = units[unit_name]\n",
    "            generated_code.append(node.code)\n",
    "            \n",
    "            # Recursively generate code for children\n",
    "            children_units=set()\n",
    "            for child_name, child_unit_name in node.children.items():\n",
    "                children_units.add(child_unit_name)\n",
    "            for child_unit in children_units:\n",
    "                self.generate_node_code(child_unit, generated_code, units)\n",
    "\n",
    "    # Function to create a placeholder class for a GAUNode\n",
    "    def create_placeholder_class(self, unit_name) -> str:\n",
    "        class_template = f\"\"\"\n",
    "class {unit_name}(GAUBase): \n",
    "    def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs): \n",
    "        factory_kwargs = {{\"device\": device, \"dtype\": dtype}} \n",
    "        super().__init__(embed_dim) \n",
    "        \n",
    "    def _forward(self, X, **Z): \n",
    "        return X\n",
    "\"\"\"\n",
    "        return class_template\n",
    "\n",
    "    # Function to convert the generated code to AST using ast and astor\n",
    "    def convert_code_to_ast(self, code: str):\n",
    "        try:\n",
    "            return ast.parse(code)\n",
    "        except SyntaxError as e:\n",
    "            print(f\"Syntax error in code: {code}\")\n",
    "            raise e\n",
    "\n",
    "    # Function to convert AST back to Python code using astor\n",
    "    def convert_ast_to_code(self, ast_tree: ast.AST) -> str:\n",
    "        return astor.to_source(ast_tree)\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "generated_code = GABComposer().generate_gab_code(test_tree)\n",
    "print(generated_code)  # This will print the final Python code for the entire GAUTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# gau.py\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "from model_discovery.model.utils.modules import GAUBase\n",
      "\n",
      "# Placeholder imports for future GAUs\n",
      "# from gau import RandomizedAttentionUnit, HierarchicalCompositionUnit\n",
      "\n",
      "class HRAB(GAUBase):\n",
      " \"\"\"Hierarchical Randomized Attention Block Unit\n",
      " Input: X: (batch, seqlen, embed_dim), Z: {dict of all current intermediate variables}\n",
      " Output: Y: (batch, seqlen, embed_dim), Z_: Optional, {dict of *new* intermediate variables to update the current Z}\n",
      " Constraints: Causal, differentiable, parameter number, complexity, parallelizable\n",
      " \"\"\"\n",
      " def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
      " factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
      " super().__init__(embed_dim)\n",
      " \n",
      " # Initialize the Randomized Attention Unit\n",
      " self.randomized_attention: GAUBase = RandomizedAttentionUnit(embed_dim, **factory_kwargs, **kwargs)\n",
      " \n",
      " # Initialize the Hierarchical Composition Unit\n",
      " self.hierarchical_composition: GAUBase = HierarchicalCompositionUnit(embed_dim, **factory_kwargs, **kwargs)\n",
      "\n",
      " def _forward(self, X, **Z):\n",
      " # Apply randomized attention to capture local dependencies\n",
      " Y, Z = self.randomized_attention(X, **Z)\n",
      " \n",
      " # Apply hierarchical composition to capture global dependencies\n",
      " Y, Z = self.hierarchical_composition(Y, **Z)\n",
      " \n",
      " return Y, Z\n",
      "\n",
      "# Placeholder classes for future implementation\n",
      "class RandomizedAttentionUnit(GAUBase):\n",
      " def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
      " super().__init__(embed_dim)\n",
      " # Implementation will be added later\n",
      "\n",
      " def _forward(self, X, **Z):\n",
      " # Placeholder forward method\n",
      " return X, {}\n",
      "\n",
      "class HierarchicalCompositionUnit(GAUBase):\n",
      " def __init__(self, embed_dim: int, device=None, dtype=None, **kwargs):\n",
      " super().__init__(embed_dim)\n",
      " # Implementation will be added later\n",
      "\n",
      " def _forward(self, X, **Z):\n",
      " # Placeholder forward method\n",
      " return X, {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
