{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process SmolLM Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import boto3\n",
    "import gzip,os\n",
    "\n",
    "# cv2 = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\")\n",
    "# fed = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\")\n",
    "# pe = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\")\n",
    "\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed4 = fed.filter(lambda x: x['metadata']['int_score'] >= 4, num_proc=120)\n",
    "# fed4=load_dataset(\"chengjunyan1/smollm-10\", \"fineweb-edu-dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe4 = pe.filter(lambda x: x['score'] > 4.16, num_proc=120)\n",
    "pe4['train'].num_rows,pe4['train'].num_rows/pe['train'].num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import dataset_dict\n",
    "\n",
    "cv2_ratio=(fed4['train'].num_rows/fed['train'].num_rows+pe4['train'].num_rows/pe['train'].num_rows)/2\n",
    "cv2_samples=int(cv2.num_rows['train']*cv2_ratio)\n",
    "cv2_10=cv2['train'].shuffle(seed=42).select(range(cv2_samples))\n",
    "cv2_10=dataset_dict.DatasetDict({'train':cv2_10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_all=fed4['train'].num_rows+pe4['train'].num_rows+cv2_10['train'].num_rows\n",
    "len_all_full=fed['train'].num_rows+pe['train'].num_rows+cv2['train'].num_rows\n",
    "print(fed4['train'].num_rows/len_all, pe4['train'].num_rows/len_all, cv2_10['train'].num_rows/len_all)\n",
    "print(fed['train'].num_rows/len_all_full, pe['train'].num_rows/len_all_full, cv2['train'].num_rows/len_all_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe4.push_to_hub(\"chengjunyan1/smollm-10\",\"python-edu\")\n",
    "# fed4.push_to_hub(\"chengjunyan1/smollm-10\",\"fineweb-edu-dedup\")\n",
    "# cv2_10.push_to_hub(\"chengjunyan1/smollm-10\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "import functools as ft\n",
    "\n",
    "\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed10=load_dataset(\"chengjunyan1/smollm-10\",\"fineweb-edu-dedup\")\n",
    "cv10=load_dataset(\"chengjunyan1/smollm-10\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu():\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10\", \"python-edu\", split=\"train\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds\n",
    "\n",
    "pe10=download_python_edu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets,DatasetDict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def resample_dataset(dataset, weight):\n",
    "    num_samples = int(len(dataset) * weight)\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=True)\n",
    "    return dataset.select(indices)\n",
    "\n",
    "def combine_datasets(dataset_dicts, weights:dict=None): # weights e.g. {'train':[1.5,1.0]}\n",
    "    combined_dict = {}\n",
    "    \n",
    "    # Initialize weights if not provided\n",
    "    for dataset_dict in dataset_dicts:\n",
    "        for key, dataset in dataset_dict.items():\n",
    "            if key in combined_dict:\n",
    "                combined_dict[key] = concatenate_datasets([combined_dict[key], dataset])\n",
    "            else:\n",
    "                combined_dict[key] = dataset\n",
    "\n",
    "    # Apply weights by resampling the datasets\n",
    "    for key in combined_dict:\n",
    "        datasets = []\n",
    "        for idx,dataset in enumerate(dataset_dicts):\n",
    "            if key in dataset:\n",
    "                if weights is None or key not in weights or weights[key][idx] == 1.0:\n",
    "                    datasets.append(dataset[key])\n",
    "                else:\n",
    "                    resampled_dataset = resample_dataset(dataset[key], weights[key][idx])\n",
    "                    datasets.append(resampled_dataset)\n",
    "        combined_dict[key] = concatenate_datasets(datasets)\n",
    "    \n",
    "    return DatasetDict(combined_dict)\n",
    "\n",
    "\n",
    "dataset_dicts=[pe10,fed10,cv10]\n",
    "combined=combine_datasets(dataset_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test split based on the processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# login('YOURKEY')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu():\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10\", \"python-edu\", split=\"train\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "fed=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "cv=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "# fed10=load_dataset(\"chengjunyan1/smollm-10\",\"fineweb-edu-dedup\",num_proc=DEFAULT_NUM_PROC//8)\n",
    "# cv10=load_dataset(\"chengjunyan1/smollm-10\",\"cosmopedia-v2\",num_proc=DEFAULT_NUM_PROC//8)\n",
    "# pe10=download_python_edu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample 2 GB of data from each dataset\n",
    "smollm_size=673\n",
    "test_ratio=1/smollm_size\n",
    "pe_test_lines=int(pe['train'].num_rows*test_ratio)\n",
    "fed_test_lines=int(fed['train'].num_rows*test_ratio)\n",
    "cv_test_lines=int(cv['train'].num_rows*test_ratio)\n",
    "pe_shuffle=pe['train'].shuffle(seed=42)\n",
    "fed_shuffle=fed['train'].shuffle(seed=42)\n",
    "cv_shuffle=cv['train'].shuffle(seed=42)\n",
    "pe_test=pe_shuffle.select(range(pe_test_lines))\n",
    "pe_eval=pe_shuffle.select(range(pe_test_lines,pe_test_lines*2))\n",
    "pe_test=pe_test.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "pe_eval=pe_eval.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "fed_test=fed_shuffle.select(range(fed_test_lines))\n",
    "fed_eval=fed_shuffle.select(range(fed_test_lines,fed_test_lines*2))\n",
    "cv_test=cv_shuffle.select(range(cv_test_lines))\n",
    "cv_eval=cv_shuffle.select(range(cv_test_lines,cv_test_lines*2))\n",
    "\n",
    "print('Python-Edu:',pe10.num_rows,pe_eval.num_rows,pe_test.num_rows)\n",
    "print('FineWeb-Edu-Dedup:',fed10.num_rows,fed_eval.num_rows,fed_test.num_rows)\n",
    "print('Cosmopedia-V2:',cv10.num_rows,cv_eval.num_rows,cv_test.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_test,pe_eval,fed_test,fed_eval,cv_test,cv_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove verbatims\n",
    "import multiprocessing as mp\n",
    "def get_text(x):\n",
    "    return x['text']\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    pe_text=pool.map(get_text, pe_test)+pool.map(get_text, pe_eval)\n",
    "    fed_text=pool.map(get_text, fed_test)+pool.map(get_text, fed_eval)\n",
    "    cv_text=pool.map(get_text, cv_test)+pool.map(get_text, cv_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_train=pe10.filter(lambda x: x['text'] not in pe_text, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "cv_train=cv10.filter(lambda x: x['text'] not in cv_text, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "fed_train=fed10.filter(lambda x: x['text'] not in fed_text, num_proc=DEFAULT_NUM_PROC//4-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pe_train['train'].num_rows,pe10['train'].num_rows,pe_train['train'].num_rows/pe10['train'].num_rows)\n",
    "print(cv_train['train'].num_rows,cv10['train'].num_rows,cv_train['train'].num_rows/cv10['train'].num_rows)\n",
    "print(fed_train['train'].num_rows,fed10['train'].num_rows,fed_train['train'].num_rows/fed10['train'].num_rows)\n",
    "total_rows_train=pe_train['train'].num_rows+cv_train['train'].num_rows+fed_train['train'].num_rows\n",
    "total_rows=pe10['train'].num_rows+cv10['train'].num_rows+fed10['train'].num_rows\n",
    "print(pe_train['train'].num_rows/total_rows_train,cv_train['train'].num_rows/total_rows_train,fed_train['train'].num_rows/total_rows_train)\n",
    "print(pe10['train'].num_rows/total_rows,cv10['train'].num_rows/total_rows,fed10['train'].num_rows/total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus=DatasetDict({'train':pe10['train'],'eval':pe_eval,'test':pe_test})\n",
    "fed10_corpus=DatasetDict({'train':fed10['train'],'eval':fed_eval,'test':fed_test})\n",
    "cv10_corpus=DatasetDict({'train':cv10['train'],'eval':cv_eval,'test':cv_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus,fed10_corpus,cv10_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")\n",
    "# fed10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"fineweb-edu-dedup\")\n",
    "# cv10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the datasets\n",
    "from datasets import load_dataset\n",
    "pe10_corpus=load_dataset(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")\n",
    "fed10_corpus=load_dataset(\"chengjunyan1/smollm-10-corpus\",\"fineweb-edu-dedup\")\n",
    "cv10_corpus=load_dataset(\"chengjunyan1/smollm-10-corpus\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus,fed10_corpus,cv10_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_python_edu(split):\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10-corpus\", \"python-edu\", split=split, num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds\n",
    "\n",
    "pe10_train=download_python_edu(\"train\")\n",
    "pe10_eval=download_python_edu(\"eval\")\n",
    "pe10_test=download_python_edu(\"test\")\n",
    "pe10_corpus=DatasetDict({'train':pe10_train['train'],'eval':pe10_eval['train'],'test':pe10_test['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe10_corpus,fed10_corpus,cv10_corpus\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "\n",
    "\n",
    "splits = ['train','eval','test']\n",
    "\n",
    "# for split in splits:\n",
    "#     print('Counting',split)\n",
    "#     fed10_token_lengths=np.sum(fed10_corpus[split]['metadata']['token_count'])\n",
    "#     print(f'{fed10_token_lengths/1000/1000/1000:.2f}','B')\n",
    "\n",
    "# for split in splits:\n",
    "#     print('Counting',split)\n",
    "#     cv10_token_lengths=np.sum(cv10_corpus[split]['token_length'])\n",
    "#     print(f'{cv10_token_lengths/1000/1000/1000:.2f}','B')\n",
    "\n",
    "for split in splits:\n",
    "    print('Counting',split)\n",
    "    pe10_length=pe10_corpus[split].map(lambda x: {'token_length':tokenizer(x['text'],return_tensors='pt')['input_ids'].shape[1]},num_proc=cpu_count())\n",
    "    pe10_token_lengths=np.sum(pe10_length['token_length'])\n",
    "    print(f'{pe10_token_lengths/1000/1000/1000:.3f}','B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove text column from pe10\n",
    "\n",
    "# pe10_corpus['train']=pe10_corpus['train'].remove_columns('text')\n",
    "# pe10_corpus['eval']=pe10_corpus['eval'].remove_columns('text')\n",
    "# pe10_corpus['test']=pe10_corpus['test'].remove_columns('text')\n",
    "\n",
    "# pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make train and test splits from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "from huggingface_hub import login\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu():\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10\", \"python-edu\", split=\"train\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "fed=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "cv=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", num_proc=DEFAULT_NUM_PROC//8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample 2 GB of data from each dataset\n",
    "smollm_size=673\n",
    "test_ratio=1/smollm_size\n",
    "pe_test_lines=int(pe['train'].num_rows*test_ratio)\n",
    "fed_test_lines=int(fed['train'].num_rows*test_ratio)\n",
    "cv_test_lines=int(cv['train'].num_rows*test_ratio)\n",
    "pe_shuffle=pe['train'].shuffle(seed=42)\n",
    "fed_shuffle=fed['train'].shuffle(seed=42)\n",
    "cv_shuffle=cv['train'].shuffle(seed=42)\n",
    "pe_test=pe_shuffle.select(range(pe_test_lines))\n",
    "pe_eval=pe_shuffle.select(range(pe_test_lines,pe_test_lines*2))\n",
    "pe_train=pe_shuffle.select(range(pe_test_lines*2,pe_shuffle.num_rows))\n",
    "fed_test=fed_shuffle.select(range(fed_test_lines))\n",
    "fed_eval=fed_shuffle.select(range(fed_test_lines,fed_test_lines*2))\n",
    "fed_train=fed_shuffle.select(range(fed_test_lines*2,fed_shuffle.num_rows))\n",
    "cv_test=cv_shuffle.select(range(cv_test_lines))\n",
    "cv_eval=cv_shuffle.select(range(cv_test_lines,cv_test_lines*2))\n",
    "cv_train=cv_shuffle.select(range(cv_test_lines*2,cv_shuffle.num_rows))\n",
    "\n",
    "print('Python-Edu:',pe_train.num_rows,pe_eval.num_rows,pe_test.num_rows)\n",
    "print('FineWeb-Edu-Dedup:',fed_train.num_rows,fed_eval.num_rows,fed_test.num_rows)\n",
    "print('Cosmopedia-V2:',cv_train.num_rows,cv_eval.num_rows,cv_test.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10 = pe_train.filter(lambda x: x['score'] > 4.16, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "fed10 = fed_train.filter(lambda x: x['metadata']['int_score'] >= 4, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "cv2_ratio=(fed10['train'].num_rows/fed_train['train'].num_rows+pe10['train'].num_rows/pe_train['train'].num_rows)/2\n",
    "cv2_samples=int(cv_train.num_rows['train']*cv2_ratio)\n",
    "cv10=cv_train['train'].shuffle(seed=42).select(range(cv2_samples))\n",
    "cv10=dataset_dict.DatasetDict({'train':cv10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus=DatasetDict({'train':pe10['train'],'eval':pe_eval,'test':pe_test})\n",
    "fed10_corpus=DatasetDict({'train':fed10['train'],'eval':fed_eval,'test':fed_test})\n",
    "cv10_corpus=DatasetDict({'train':cv10['train'],'eval':cv_eval,'test':cv_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")\n",
    "pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"fineweb-edu-dedup\")\n",
    "pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Make based on SmolLM Mixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "\n",
    "# login('YOURKEY')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YK\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YK\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu(path,split):\n",
    "    ds = load_dataset(path, \"python-edu\", split=split, num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({split: ds})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "fed=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "cv=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", num_proc=DEFAULT_NUM_PROC//8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed4 = fed.filter(lambda x: x['metadata']['score'] >= 4, num_proc=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "fed4_token_lengths=np.sum(fed4['train']['metadata']['token_count'])\n",
    "# bar=tqdm(fed4['train'])\n",
    "# for i in bar:\n",
    "#     fed4_token_lengths+=i['metadata']['token_count']\n",
    "#     current_count=f'{fed4_token_lengths/1000/1000/1000:.3f}B'\n",
    "#     bar.set_description(current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process DeepMind Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "def download_contents_py(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=\"softwareheritage\", Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "\n",
    "DEFAULT_NUM_PROC_LOAD=os.cpu_count()*4\n",
    "\n",
    "ds = load_dataset(\"chengjunyan1/smollm-12.5-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUM_PROC_LOAD=os.cpu_count()*8\n",
    "from datasets import DatasetDict    \n",
    "\n",
    "ds_train = ds['train'].map(download_contents_py, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC_LOAD)\n",
    "ds_test = ds['test'].map(download_contents_py, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC_LOAD)\n",
    "ds_eval = ds['eval'].map(download_contents_py, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC_LOAD)\n",
    "ds = DatasetDict({'train':ds_train, 'test':ds_test, 'eval':ds_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 224.47ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 220.52ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 216.23ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 220.11ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 218.15ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 215.97ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 218.69ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 213.65ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 217.63ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 210.88ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 210.66ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 221.14ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 218.60ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 219.57ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 210.89ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 215.25ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 220.15ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 211.61ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 293/293 [00:01<00:00, 219.80ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 19/19 [02:49<00:00,  8.90s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 70/70 [00:00<00:00, 208.41ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 70/70 [00:00<00:00, 208.11ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/chengjunyan1/smollm-12.5-corpus/commit/0193632a61d04658f93c6041e0429e451a026f75', commit_message='Upload dataset', commit_description='', oid='0193632a61d04658f93c6041e0429e451a026f75', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ds.push_to_hub(\"chengjunyan1/smollm-12.5-corpus\",\"python-edu\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
