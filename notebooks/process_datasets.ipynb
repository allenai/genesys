{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process SmolLM Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import boto3\n",
    "import gzip,os\n",
    "\n",
    "# cv2 = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\")\n",
    "# fed = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\")\n",
    "# pe = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\")\n",
    "\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed4 = fed.filter(lambda x: x['metadata']['int_score'] >= 4, num_proc=120)\n",
    "# fed4=load_dataset(\"chengjunyan1/smollm-10\", \"fineweb-edu-dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe4 = pe.filter(lambda x: x['score'] > 4.16, num_proc=120)\n",
    "pe4['train'].num_rows,pe4['train'].num_rows/pe['train'].num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import dataset_dict\n",
    "\n",
    "cv2_ratio=(fed4['train'].num_rows/fed['train'].num_rows+pe4['train'].num_rows/pe['train'].num_rows)/2\n",
    "cv2_samples=int(cv2.num_rows['train']*cv2_ratio)\n",
    "cv2_10=cv2['train'].shuffle(seed=42).select(range(cv2_samples))\n",
    "cv2_10=dataset_dict.DatasetDict({'train':cv2_10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_all=fed4['train'].num_rows+pe4['train'].num_rows+cv2_10['train'].num_rows\n",
    "len_all_full=fed['train'].num_rows+pe['train'].num_rows+cv2['train'].num_rows\n",
    "print(fed4['train'].num_rows/len_all, pe4['train'].num_rows/len_all, cv2_10['train'].num_rows/len_all)\n",
    "print(fed['train'].num_rows/len_all_full, pe['train'].num_rows/len_all_full, cv2['train'].num_rows/len_all_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe4.push_to_hub(\"chengjunyan1/smollm-10\",\"python-edu\")\n",
    "# fed4.push_to_hub(\"chengjunyan1/smollm-10\",\"fineweb-edu-dedup\")\n",
    "# cv2_10.push_to_hub(\"chengjunyan1/smollm-10\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "import functools as ft\n",
    "\n",
    "\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed10=load_dataset(\"chengjunyan1/smollm-10\",\"fineweb-edu-dedup\")\n",
    "cv10=load_dataset(\"chengjunyan1/smollm-10\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu():\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10\", \"python-edu\", split=\"train\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds\n",
    "\n",
    "pe10=download_python_edu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets,DatasetDict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def resample_dataset(dataset, weight):\n",
    "    num_samples = int(len(dataset) * weight)\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=True)\n",
    "    return dataset.select(indices)\n",
    "\n",
    "def combine_datasets(dataset_dicts, weights:dict=None): # weights e.g. {'train':[1.5,1.0]}\n",
    "    combined_dict = {}\n",
    "    \n",
    "    # Initialize weights if not provided\n",
    "    for dataset_dict in dataset_dicts:\n",
    "        for key, dataset in dataset_dict.items():\n",
    "            if key in combined_dict:\n",
    "                combined_dict[key] = concatenate_datasets([combined_dict[key], dataset])\n",
    "            else:\n",
    "                combined_dict[key] = dataset\n",
    "\n",
    "    # Apply weights by resampling the datasets\n",
    "    for key in combined_dict:\n",
    "        datasets = []\n",
    "        for idx,dataset in enumerate(dataset_dicts):\n",
    "            if key in dataset:\n",
    "                if weights is None or key not in weights or weights[key][idx] == 1.0:\n",
    "                    datasets.append(dataset[key])\n",
    "                else:\n",
    "                    resampled_dataset = resample_dataset(dataset[key], weights[key][idx])\n",
    "                    datasets.append(resampled_dataset)\n",
    "        combined_dict[key] = concatenate_datasets(datasets)\n",
    "    \n",
    "    return DatasetDict(combined_dict)\n",
    "\n",
    "\n",
    "dataset_dicts=[pe10,fed10,cv10]\n",
    "combined=combine_datasets(dataset_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test split based on the processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# login('YOURKEY')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu():\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10\", \"python-edu\", split=\"train\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "fed=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "cv=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "# fed10=load_dataset(\"chengjunyan1/smollm-10\",\"fineweb-edu-dedup\",num_proc=DEFAULT_NUM_PROC//8)\n",
    "# cv10=load_dataset(\"chengjunyan1/smollm-10\",\"cosmopedia-v2\",num_proc=DEFAULT_NUM_PROC//8)\n",
    "# pe10=download_python_edu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample 2 GB of data from each dataset\n",
    "smollm_size=673\n",
    "test_ratio=1/smollm_size\n",
    "pe_test_lines=int(pe['train'].num_rows*test_ratio)\n",
    "fed_test_lines=int(fed['train'].num_rows*test_ratio)\n",
    "cv_test_lines=int(cv['train'].num_rows*test_ratio)\n",
    "pe_shuffle=pe['train'].shuffle(seed=42)\n",
    "fed_shuffle=fed['train'].shuffle(seed=42)\n",
    "cv_shuffle=cv['train'].shuffle(seed=42)\n",
    "pe_test=pe_shuffle.select(range(pe_test_lines))\n",
    "pe_eval=pe_shuffle.select(range(pe_test_lines,pe_test_lines*2))\n",
    "pe_test=pe_test.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "pe_eval=pe_eval.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "fed_test=fed_shuffle.select(range(fed_test_lines))\n",
    "fed_eval=fed_shuffle.select(range(fed_test_lines,fed_test_lines*2))\n",
    "cv_test=cv_shuffle.select(range(cv_test_lines))\n",
    "cv_eval=cv_shuffle.select(range(cv_test_lines,cv_test_lines*2))\n",
    "\n",
    "print('Python-Edu:',pe10.num_rows,pe_eval.num_rows,pe_test.num_rows)\n",
    "print('FineWeb-Edu-Dedup:',fed10.num_rows,fed_eval.num_rows,fed_test.num_rows)\n",
    "print('Cosmopedia-V2:',cv10.num_rows,cv_eval.num_rows,cv_test.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_test,pe_eval,fed_test,fed_eval,cv_test,cv_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove verbatims\n",
    "import multiprocessing as mp\n",
    "def get_text(x):\n",
    "    return x['text']\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    pe_text=pool.map(get_text, pe_test)+pool.map(get_text, pe_eval)\n",
    "    fed_text=pool.map(get_text, fed_test)+pool.map(get_text, fed_eval)\n",
    "    cv_text=pool.map(get_text, cv_test)+pool.map(get_text, cv_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_train=pe10.filter(lambda x: x['text'] not in pe_text, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "cv_train=cv10.filter(lambda x: x['text'] not in cv_text, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "fed_train=fed10.filter(lambda x: x['text'] not in fed_text, num_proc=DEFAULT_NUM_PROC//4-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pe_train['train'].num_rows,pe10['train'].num_rows,pe_train['train'].num_rows/pe10['train'].num_rows)\n",
    "print(cv_train['train'].num_rows,cv10['train'].num_rows,cv_train['train'].num_rows/cv10['train'].num_rows)\n",
    "print(fed_train['train'].num_rows,fed10['train'].num_rows,fed_train['train'].num_rows/fed10['train'].num_rows)\n",
    "total_rows_train=pe_train['train'].num_rows+cv_train['train'].num_rows+fed_train['train'].num_rows\n",
    "total_rows=pe10['train'].num_rows+cv10['train'].num_rows+fed10['train'].num_rows\n",
    "print(pe_train['train'].num_rows/total_rows_train,cv_train['train'].num_rows/total_rows_train,fed_train['train'].num_rows/total_rows_train)\n",
    "print(pe10['train'].num_rows/total_rows,cv10['train'].num_rows/total_rows,fed10['train'].num_rows/total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus=DatasetDict({'train':pe10['train'],'eval':pe_eval,'test':pe_test})\n",
    "fed10_corpus=DatasetDict({'train':fed10['train'],'eval':fed_eval,'test':fed_test})\n",
    "cv10_corpus=DatasetDict({'train':cv10['train'],'eval':cv_eval,'test':cv_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus,fed10_corpus,cv10_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")\n",
    "# fed10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"fineweb-edu-dedup\")\n",
    "# cv10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the datasets\n",
    "from datasets import load_dataset\n",
    "pe10_corpus=load_dataset(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")\n",
    "fed10_corpus=load_dataset(\"chengjunyan1/smollm-10-corpus\",\"fineweb-edu-dedup\")\n",
    "cv10_corpus=load_dataset(\"chengjunyan1/smollm-10-corpus\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus,fed10_corpus,cv10_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_python_edu(split):\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10-corpus\", \"python-edu\", split=split, num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds\n",
    "\n",
    "pe10_train=download_python_edu(\"train\")\n",
    "pe10_eval=download_python_edu(\"eval\")\n",
    "pe10_test=download_python_edu(\"test\")\n",
    "pe10_corpus=DatasetDict({'train':pe10_train['train'],'eval':pe10_eval['train'],'test':pe10_test['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe10_corpus,fed10_corpus,cv10_corpus\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "\n",
    "\n",
    "splits = ['train','eval','test']\n",
    "\n",
    "# for split in splits:\n",
    "#     print('Counting',split)\n",
    "#     fed10_token_lengths=np.sum(fed10_corpus[split]['metadata']['token_count'])\n",
    "#     print(f'{fed10_token_lengths/1000/1000/1000:.2f}','B')\n",
    "\n",
    "# for split in splits:\n",
    "#     print('Counting',split)\n",
    "#     cv10_token_lengths=np.sum(cv10_corpus[split]['token_length'])\n",
    "#     print(f'{cv10_token_lengths/1000/1000/1000:.2f}','B')\n",
    "\n",
    "for split in splits:\n",
    "    print('Counting',split)\n",
    "    pe10_length=pe10_corpus[split].map(lambda x: {'token_length':tokenizer(x['text'],return_tensors='pt')['input_ids'].shape[1]},num_proc=cpu_count())\n",
    "    pe10_token_lengths=np.sum(pe10_length['token_length'])\n",
    "    print(f'{pe10_token_lengths/1000/1000/1000:.3f}','B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove text column from pe10\n",
    "\n",
    "# pe10_corpus['train']=pe10_corpus['train'].remove_columns('text')\n",
    "# pe10_corpus['eval']=pe10_corpus['eval'].remove_columns('text')\n",
    "# pe10_corpus['test']=pe10_corpus['test'].remove_columns('text')\n",
    "\n",
    "# pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make train and test splits from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "from huggingface_hub import login\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu():\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10\", \"python-edu\", split=\"train\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "fed=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "cv=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", num_proc=DEFAULT_NUM_PROC//8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample 2 GB of data from each dataset\n",
    "smollm_size=673\n",
    "test_ratio=1/smollm_size\n",
    "pe_test_lines=int(pe['train'].num_rows*test_ratio)\n",
    "fed_test_lines=int(fed['train'].num_rows*test_ratio)\n",
    "cv_test_lines=int(cv['train'].num_rows*test_ratio)\n",
    "pe_shuffle=pe['train'].shuffle(seed=42)\n",
    "fed_shuffle=fed['train'].shuffle(seed=42)\n",
    "cv_shuffle=cv['train'].shuffle(seed=42)\n",
    "pe_test=pe_shuffle.select(range(pe_test_lines))\n",
    "pe_eval=pe_shuffle.select(range(pe_test_lines,pe_test_lines*2))\n",
    "pe_train=pe_shuffle.select(range(pe_test_lines*2,pe_shuffle.num_rows))\n",
    "fed_test=fed_shuffle.select(range(fed_test_lines))\n",
    "fed_eval=fed_shuffle.select(range(fed_test_lines,fed_test_lines*2))\n",
    "fed_train=fed_shuffle.select(range(fed_test_lines*2,fed_shuffle.num_rows))\n",
    "cv_test=cv_shuffle.select(range(cv_test_lines))\n",
    "cv_eval=cv_shuffle.select(range(cv_test_lines,cv_test_lines*2))\n",
    "cv_train=cv_shuffle.select(range(cv_test_lines*2,cv_shuffle.num_rows))\n",
    "\n",
    "print('Python-Edu:',pe_train.num_rows,pe_eval.num_rows,pe_test.num_rows)\n",
    "print('FineWeb-Edu-Dedup:',fed_train.num_rows,fed_eval.num_rows,fed_test.num_rows)\n",
    "print('Cosmopedia-V2:',cv_train.num_rows,cv_eval.num_rows,cv_test.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10 = pe_train.filter(lambda x: x['score'] > 4.16, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "fed10 = fed_train.filter(lambda x: x['metadata']['int_score'] >= 4, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "cv2_ratio=(fed10['train'].num_rows/fed_train['train'].num_rows+pe10['train'].num_rows/pe_train['train'].num_rows)/2\n",
    "cv2_samples=int(cv_train.num_rows['train']*cv2_ratio)\n",
    "cv10=cv_train['train'].shuffle(seed=42).select(range(cv2_samples))\n",
    "cv10=dataset_dict.DatasetDict({'train':cv10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus=DatasetDict({'train':pe10['train'],'eval':pe_eval,'test':pe_test})\n",
    "fed10_corpus=DatasetDict({'train':fed10['train'],'eval':fed_eval,'test':fed_test})\n",
    "cv10_corpus=DatasetDict({'train':cv10['train'],'eval':cv_eval,'test':cv_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")\n",
    "pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"fineweb-edu-dedup\")\n",
    "pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Make based on SmolLM Mixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "\n",
    "# login('YOURKEY')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YK\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YK\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu(path,split):\n",
    "    ds = load_dataset(path, \"python-edu\", split=split, num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({split: ds})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "fed=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "cv=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", num_proc=DEFAULT_NUM_PROC//8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed4 = fed.filter(lambda x: x['metadata']['score'] >= 4, num_proc=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "fed4_token_lengths=np.sum(fed4['train']['metadata']['token_count'])\n",
    "# bar=tqdm(fed4['train'])\n",
    "# for i in bar:\n",
    "#     fed4_token_lengths+=i['metadata']['token_count']\n",
    "#     current_count=f'{fed4_token_lengths/1000/1000/1000:.3f}B'\n",
    "#     bar.set_description(current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "def download_contents_py(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=\"softwareheritage\", Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "\n",
    "DEFAULT_NUM_PROC_LOAD=os.cpu_count()*4\n",
    "\n",
    "ds = load_dataset(\"chengjunyan1/smollm-12.5-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUM_PROC_LOAD=os.cpu_count()*8\n",
    "from datasets import DatasetDict    \n",
    "\n",
    "ds_train = ds['train'].map(download_contents_py, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC_LOAD)\n",
    "ds_test = ds['test'].map(download_contents_py, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC_LOAD)\n",
    "ds_eval = ds['eval'].map(download_contents_py, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC_LOAD)\n",
    "ds = DatasetDict({'train':ds_train, 'test':ds_test, 'eval':ds_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ds.push_to_hub(\"chengjunyan1/smollm-12.5-corpus\",\"python-edu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process DeepMind Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "\n",
    "# login('')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"chengjunyan1/smollm-12.5-corpus\", \"deepmind-math\", num_proc=DEFAULT_NUM_PROC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle\n",
    "ds['train']=ds['train'].shuffle(seed=42)\n",
    "ds['test']=ds['test'].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# around 195,167,286\n",
    "rows=50000\n",
    "print(round(sum(ds['test'][:rows]['token_counts'])/1000/1000/1000,5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_new=DatasetDict({\n",
    "    'train':ds['train'].select(range(3880000)),\n",
    "    'test':ds['test'].select(range(rows)),\n",
    "    'eval':ds['test'].select(range(rows,rows*2))\n",
    "})\n",
    "ds_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_new.push_to_hub(\"chengjunyan1/smollm-12.5-corpus\",\"deepmind-math-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process StackOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "\n",
    "# login('')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "\n",
    "ds = load_dataset(\"bigcode/stackoverflow-clean\",num_proc=DEFAULT_NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=ds.shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows_train=395000\n",
    "\n",
    "ds_train=ds['train'].select(range(n_rows_train))\n",
    "nb_train=round(sum(ds_train['nb_tokens'])/1000/1000/1000,5)\n",
    "print(nb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows_test=6100\n",
    "n_rows_eval=n_rows_test\n",
    "\n",
    "ds_test=ds['train'].select(range(n_rows_train,n_rows_train+n_rows_test))\n",
    "nb_test=round(sum(ds_test['nb_tokens'])/1000/1000/1000,5)\n",
    "print(nb_test)\n",
    "ds_eval=ds['train'].select(range(n_rows_train+n_rows_test,n_rows_train+n_rows_test+n_rows_eval))\n",
    "nb_eval=round(sum(ds_eval['nb_tokens'])/1000/1000/1000,5)\n",
    "print(nb_eval)\n",
    "\n",
    "print(nb_train+nb_test+nb_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_new=DatasetDict({\n",
    "    'train':ds_train,\n",
    "    'test':ds_test,\n",
    "    'eval':ds_eval\n",
    "})\n",
    "ds_new.push_to_hub(\"chengjunyan1/smollm-12.5-corpus\",\"stackoverflow-clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnose Cosmopedia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "\n",
    "# login('')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "\n",
    "ds = load_dataset(\"chengjunyan1/smollm-12.5-corpus\",\"cosmopedia-v2\",num_proc=DEFAULT_NUM_PROC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_text=ds['train'].map(lambda x: {'text':f\"Audience: {x['audience']}\\n\\nPrompt: {x['prompt']}\\n\\nResponse: {x['text']}\"},num_proc=DEFAULT_NUM_PROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test loading all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from model_discovery.configs.gam_config import ( \n",
    "    GAMConfig,GAMConfig_14M,GAMConfig_31M,GAMConfig_70M,GAMConfig_125M,GAMConfig_350M,GAMConfig_760M,\n",
    "    GAMConfig_1300M,GAMConfig_2700M,GAMConfig_6700M,GAMConfig_13B,GAMConfig_175B,GAMConfig_1T,GAMConfig_debug\n",
    ")\n",
    "\n",
    "from model_discovery.ve.data_loader import load_datasets\n",
    "\n",
    "\n",
    "cfg=GAMConfig_14M()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e975066e544723826b7f412a98c2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d868b433e0942aaadb6f5d7ccb51efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8005284f1ce464585aa0e7aac4c6d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cc38ab5a0e47cf80b81906c905d0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cda29a6998427eb836f78c7a766e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "\n",
    "# login('')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "def process_cosmopedia_v2(examples):\n",
    "    return {\n",
    "        'text': [\n",
    "            f\"Audience: {audience}\\n\\nPrompt: {prompt}\\n\\nResponse: {text}\"\n",
    "            for audience, prompt, text in zip(examples['audience'], examples['prompt'], examples['text'])\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def process_deepmind_math_small(examples):\n",
    "    return {\n",
    "        'text': [\n",
    "            f\"Question: {question}\\n\\nAnswer: {answer}\"\n",
    "            for question, answer in zip(examples['question'], examples['answer'])\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "python_edu=load_dataset(\"chengjunyan1/smollm-12.5-corpus\",\"python-edu\",split='test',num_proc=DEFAULT_NUM_PROC)\n",
    "fineweb_edu_dedup=load_dataset(\"chengjunyan1/smollm-12.5-corpus\",\"fineweb-edu-dedup\",split='test',num_proc=DEFAULT_NUM_PROC)\n",
    "open_web_math=load_dataset(\"chengjunyan1/smollm-12.5-corpus\",\"open-web-math\",split='test',num_proc=DEFAULT_NUM_PROC)\n",
    "cosmopedia = load_dataset(\"chengjunyan1/smollm-12.5-corpus\",\"cosmopedia-v2\",split='test',num_proc=DEFAULT_NUM_PROC)\n",
    "deepmind_math = load_dataset(\"chengjunyan1/smollm-12.5-corpus\",\"deepmind-math-small\",split='test',num_proc=DEFAULT_NUM_PROC)\n",
    "stackoverflow = load_dataset(\"chengjunyan1/smollm-12.5-corpus\",\"stackoverflow-clean\",split='test',num_proc=DEFAULT_NUM_PROC)\n",
    "\n",
    "cosmopedia_processed = cosmopedia.map(process_cosmopedia_v2, batched=True, batch_size=1000, num_proc=DEFAULT_NUM_PROC)\n",
    "deepmind_math_processed = deepmind_math.map(process_deepmind_math_small, batched=True, batch_size=1000, num_proc=DEFAULT_NUM_PROC)\n",
    "stackoverflow_processed = stackoverflow.rename_column('content', 'text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blob_id',\n",
       " 'int_score',\n",
       " 'length_bytes',\n",
       " 'path',\n",
       " 'repo_name',\n",
       " 'score',\n",
       " 'token_counts'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_edu=python_edu.remove_columns(set(python_edu.column_names)-{'text'})\n",
    "fineweb_edu_dedup=fineweb_edu_dedup.remove_columns(set(fineweb_edu_dedup.column_names)-{'text'})\n",
    "open_web_math=open_web_math.remove_columns(set(open_web_math.column_names)-{'text'})\n",
    "cosmopedia_processed=cosmopedia_processed.remove_columns(set(cosmopedia_processed.column_names)-{'text'})\n",
    "deepmind_math_processed=deepmind_math_processed.remove_columns(set(deepmind_math_processed.column_names)-{'text'})\n",
    "stackoverflow_processed=stackoverflow_processed.remove_columns(set(stackoverflow_processed.column_names)-{'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_edu=DatasetDict({'test':python_edu})\n",
    "fineweb_edu_dedup=DatasetDict({'test':fineweb_edu_dedup})\n",
    "open_web_math=DatasetDict({'test':open_web_math})\n",
    "cosmopedia_processed=DatasetDict({'test':cosmopedia_processed})\n",
    "deepmind_math_processed=DatasetDict({'test':deepmind_math_processed})\n",
    "stackoverflow_processed=DatasetDict({'test':stackoverflow_processed})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e4a2adc5924c6896d03c649b6c164f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c08d631fdc642fa8203088458e3a7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/70 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6665140736243988c979e4697049d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18599da18d724ee98ab1aa6af972c9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/95 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5323b8f0672e48eb8b4efdec4796495d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/95 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9e619bb2464d5090e9b624f0d57685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/95 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc94b6e02ecf4bd29f75dab4d4a328f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/312 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793c1f8152054daf9b5eff2447f31b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c20c0303554783beb486a7109f18ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e143491e8d6d454aa5f76e118e3e0a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/618 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4645651640f7439789039265133a0192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6eb18c083fd4ba8b7bc7b28d8e649a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/79 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603d36002a6c446ab9a5c223621c3efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/906 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1b5fecefdb42579b551649672d7b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a437ba97734b59807149792c65fe13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc84c450fcd34522a02eef402429f98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a2757199c24b7199357be72e49f10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3b0b9de49d4ff28d40fbdc616b4ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ee3e0f30b248d4b1b952bd73860900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/chengjunyan1/smollm-12.5-test/commit/fc8ddcd756a70a88180e1c03a3e9cfb35178a2a1', commit_message='Upload dataset', commit_description='', oid='fc8ddcd756a70a88180e1c03a3e9cfb35178a2a1', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_edu.push_to_hub(\"chengjunyan1/smollm-12.5-test\",\"python-edu\")\n",
    "fineweb_edu_dedup.push_to_hub(\"chengjunyan1/smollm-12.5-test\",\"fineweb-edu-dedup\")\n",
    "open_web_math.push_to_hub(\"chengjunyan1/smollm-12.5-test\",\"open-web-math\")\n",
    "cosmopedia_processed.push_to_hub(\"chengjunyan1/smollm-12.5-test\",\"cosmopedia-v2\")\n",
    "deepmind_math_processed.push_to_hub(\"chengjunyan1/smollm-12.5-test\",\"deepmind-math-small\")\n",
    "stackoverflow_processed.push_to_hub(\"chengjunyan1/smollm-12.5-test\",\"stackoverflow-clean\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
