{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process SmolLM Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import boto3\n",
    "import gzip,os\n",
    "\n",
    "# cv2 = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\")\n",
    "# fed = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\")\n",
    "# pe = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\")\n",
    "\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed4 = fed.filter(lambda x: x['metadata']['int_score'] >= 4, num_proc=120)\n",
    "# fed4=load_dataset(\"chengjunyan1/smollm-10\", \"fineweb-edu-dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe4 = pe.filter(lambda x: x['score'] > 4.16, num_proc=120)\n",
    "pe4['train'].num_rows,pe4['train'].num_rows/pe['train'].num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import dataset_dict\n",
    "\n",
    "cv2_ratio=(fed4['train'].num_rows/fed['train'].num_rows+pe4['train'].num_rows/pe['train'].num_rows)/2\n",
    "cv2_samples=int(cv2.num_rows['train']*cv2_ratio)\n",
    "cv2_10=cv2['train'].shuffle(seed=42).select(range(cv2_samples))\n",
    "cv2_10=dataset_dict.DatasetDict({'train':cv2_10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_all=fed4['train'].num_rows+pe4['train'].num_rows+cv2_10['train'].num_rows\n",
    "len_all_full=fed['train'].num_rows+pe['train'].num_rows+cv2['train'].num_rows\n",
    "print(fed4['train'].num_rows/len_all, pe4['train'].num_rows/len_all, cv2_10['train'].num_rows/len_all)\n",
    "print(fed['train'].num_rows/len_all_full, pe['train'].num_rows/len_all_full, cv2['train'].num_rows/len_all_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe4.push_to_hub(\"chengjunyan1/smollm-10\",\"python-edu\")\n",
    "# fed4.push_to_hub(\"chengjunyan1/smollm-10\",\"fineweb-edu-dedup\")\n",
    "# cv2_10.push_to_hub(\"chengjunyan1/smollm-10\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "import functools as ft\n",
    "\n",
    "\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed10=load_dataset(\"chengjunyan1/smollm-10\",\"fineweb-edu-dedup\")\n",
    "cv10=load_dataset(\"chengjunyan1/smollm-10\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu():\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10\", \"python-edu\", split=\"train\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds\n",
    "\n",
    "pe10=download_python_edu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets,DatasetDict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def resample_dataset(dataset, weight):\n",
    "    num_samples = int(len(dataset) * weight)\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=True)\n",
    "    return dataset.select(indices)\n",
    "\n",
    "def combine_datasets(dataset_dicts, weights:dict=None): # weights e.g. {'train':[1.5,1.0]}\n",
    "    combined_dict = {}\n",
    "    \n",
    "    # Initialize weights if not provided\n",
    "    for dataset_dict in dataset_dicts:\n",
    "        for key, dataset in dataset_dict.items():\n",
    "            if key in combined_dict:\n",
    "                combined_dict[key] = concatenate_datasets([combined_dict[key], dataset])\n",
    "            else:\n",
    "                combined_dict[key] = dataset\n",
    "\n",
    "    # Apply weights by resampling the datasets\n",
    "    for key in combined_dict:\n",
    "        datasets = []\n",
    "        for idx,dataset in enumerate(dataset_dicts):\n",
    "            if key in dataset:\n",
    "                if weights is None or key not in weights or weights[key][idx] == 1.0:\n",
    "                    datasets.append(dataset[key])\n",
    "                else:\n",
    "                    resampled_dataset = resample_dataset(dataset[key], weights[key][idx])\n",
    "                    datasets.append(resampled_dataset)\n",
    "        combined_dict[key] = concatenate_datasets(datasets)\n",
    "    \n",
    "    return DatasetDict(combined_dict)\n",
    "\n",
    "\n",
    "dataset_dicts=[pe10,fed10,cv10]\n",
    "combined=combine_datasets(dataset_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test split based on the processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# login('YOURKEY')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu():\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10\", \"python-edu\", split=\"train\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "fed=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "cv=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "# fed10=load_dataset(\"chengjunyan1/smollm-10\",\"fineweb-edu-dedup\",num_proc=DEFAULT_NUM_PROC//8)\n",
    "# cv10=load_dataset(\"chengjunyan1/smollm-10\",\"cosmopedia-v2\",num_proc=DEFAULT_NUM_PROC//8)\n",
    "# pe10=download_python_edu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample 2 GB of data from each dataset\n",
    "smollm_size=673\n",
    "test_ratio=1/smollm_size\n",
    "pe_test_lines=int(pe['train'].num_rows*test_ratio)\n",
    "fed_test_lines=int(fed['train'].num_rows*test_ratio)\n",
    "cv_test_lines=int(cv['train'].num_rows*test_ratio)\n",
    "pe_shuffle=pe['train'].shuffle(seed=42)\n",
    "fed_shuffle=fed['train'].shuffle(seed=42)\n",
    "cv_shuffle=cv['train'].shuffle(seed=42)\n",
    "pe_test=pe_shuffle.select(range(pe_test_lines))\n",
    "pe_eval=pe_shuffle.select(range(pe_test_lines,pe_test_lines*2))\n",
    "pe_test=pe_test.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "pe_eval=pe_eval.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "fed_test=fed_shuffle.select(range(fed_test_lines))\n",
    "fed_eval=fed_shuffle.select(range(fed_test_lines,fed_test_lines*2))\n",
    "cv_test=cv_shuffle.select(range(cv_test_lines))\n",
    "cv_eval=cv_shuffle.select(range(cv_test_lines,cv_test_lines*2))\n",
    "\n",
    "print('Python-Edu:',pe10.num_rows,pe_eval.num_rows,pe_test.num_rows)\n",
    "print('FineWeb-Edu-Dedup:',fed10.num_rows,fed_eval.num_rows,fed_test.num_rows)\n",
    "print('Cosmopedia-V2:',cv10.num_rows,cv_eval.num_rows,cv_test.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_test,pe_eval,fed_test,fed_eval,cv_test,cv_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove verbatims\n",
    "import multiprocessing as mp\n",
    "def get_text(x):\n",
    "    return x['text']\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    pe_text=pool.map(get_text, pe_test)+pool.map(get_text, pe_eval)\n",
    "    fed_text=pool.map(get_text, fed_test)+pool.map(get_text, fed_eval)\n",
    "    cv_text=pool.map(get_text, cv_test)+pool.map(get_text, cv_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_train=pe10.filter(lambda x: x['text'] not in pe_text, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "cv_train=cv10.filter(lambda x: x['text'] not in cv_text, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "fed_train=fed10.filter(lambda x: x['text'] not in fed_text, num_proc=DEFAULT_NUM_PROC//4-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pe_train['train'].num_rows,pe10['train'].num_rows,pe_train['train'].num_rows/pe10['train'].num_rows)\n",
    "print(cv_train['train'].num_rows,cv10['train'].num_rows,cv_train['train'].num_rows/cv10['train'].num_rows)\n",
    "print(fed_train['train'].num_rows,fed10['train'].num_rows,fed_train['train'].num_rows/fed10['train'].num_rows)\n",
    "total_rows_train=pe_train['train'].num_rows+cv_train['train'].num_rows+fed_train['train'].num_rows\n",
    "total_rows=pe10['train'].num_rows+cv10['train'].num_rows+fed10['train'].num_rows\n",
    "print(pe_train['train'].num_rows/total_rows_train,cv_train['train'].num_rows/total_rows_train,fed_train['train'].num_rows/total_rows_train)\n",
    "print(pe10['train'].num_rows/total_rows,cv10['train'].num_rows/total_rows,fed10['train'].num_rows/total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus=DatasetDict({'train':pe10['train'],'eval':pe_eval,'test':pe_test})\n",
    "fed10_corpus=DatasetDict({'train':fed10['train'],'eval':fed_eval,'test':fed_test})\n",
    "cv10_corpus=DatasetDict({'train':cv10['train'],'eval':cv_eval,'test':cv_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus,fed10_corpus,cv10_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")\n",
    "# fed10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"fineweb-edu-dedup\")\n",
    "# cv10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the datasets\n",
    "from datasets import load_dataset\n",
    "pe10_corpus=load_dataset(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")\n",
    "fed10_corpus=load_dataset(\"chengjunyan1/smollm-10-corpus\",\"fineweb-edu-dedup\")\n",
    "cv10_corpus=load_dataset(\"chengjunyan1/smollm-10-corpus\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus,fed10_corpus,cv10_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_python_edu(split):\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10-corpus\", \"python-edu\", split=split, num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds\n",
    "\n",
    "pe10_train=download_python_edu(\"train\")\n",
    "pe10_eval=download_python_edu(\"eval\")\n",
    "pe10_test=download_python_edu(\"test\")\n",
    "pe10_corpus=DatasetDict({'train':pe10_train['train'],'eval':pe10_eval['train'],'test':pe10_test['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe10_corpus,fed10_corpus,cv10_corpus\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "\n",
    "\n",
    "splits = ['train','eval','test']\n",
    "\n",
    "# for split in splits:\n",
    "#     print('Counting',split)\n",
    "#     fed10_token_lengths=np.sum(fed10_corpus[split]['metadata']['token_count'])\n",
    "#     print(f'{fed10_token_lengths/1000/1000/1000:.2f}','B')\n",
    "\n",
    "# for split in splits:\n",
    "#     print('Counting',split)\n",
    "#     cv10_token_lengths=np.sum(cv10_corpus[split]['token_length'])\n",
    "#     print(f'{cv10_token_lengths/1000/1000/1000:.2f}','B')\n",
    "\n",
    "for split in splits:\n",
    "    print('Counting',split)\n",
    "    pe10_length=pe10_corpus[split].map(lambda x: {'token_length':tokenizer(x['text'],return_tensors='pt')['input_ids'].shape[1]},num_proc=cpu_count())\n",
    "    pe10_token_lengths=np.sum(pe10_length['token_length'])\n",
    "    print(f'{pe10_token_lengths/1000/1000/1000:.3f}','B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove text column from pe10\n",
    "\n",
    "# pe10_corpus['train']=pe10_corpus['train'].remove_columns('text')\n",
    "# pe10_corpus['eval']=pe10_corpus['eval'].remove_columns('text')\n",
    "# pe10_corpus['test']=pe10_corpus['test'].remove_columns('text')\n",
    "\n",
    "# pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make train and test splits from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "from huggingface_hub import login\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu():\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10\", \"python-edu\", split=\"train\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "fed=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "cv=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", num_proc=DEFAULT_NUM_PROC//8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample 2 GB of data from each dataset\n",
    "smollm_size=673\n",
    "test_ratio=1/smollm_size\n",
    "pe_test_lines=int(pe['train'].num_rows*test_ratio)\n",
    "fed_test_lines=int(fed['train'].num_rows*test_ratio)\n",
    "cv_test_lines=int(cv['train'].num_rows*test_ratio)\n",
    "pe_shuffle=pe['train'].shuffle(seed=42)\n",
    "fed_shuffle=fed['train'].shuffle(seed=42)\n",
    "cv_shuffle=cv['train'].shuffle(seed=42)\n",
    "pe_test=pe_shuffle.select(range(pe_test_lines))\n",
    "pe_eval=pe_shuffle.select(range(pe_test_lines,pe_test_lines*2))\n",
    "pe_train=pe_shuffle.select(range(pe_test_lines*2,pe_shuffle.num_rows))\n",
    "fed_test=fed_shuffle.select(range(fed_test_lines))\n",
    "fed_eval=fed_shuffle.select(range(fed_test_lines,fed_test_lines*2))\n",
    "fed_train=fed_shuffle.select(range(fed_test_lines*2,fed_shuffle.num_rows))\n",
    "cv_test=cv_shuffle.select(range(cv_test_lines))\n",
    "cv_eval=cv_shuffle.select(range(cv_test_lines,cv_test_lines*2))\n",
    "cv_train=cv_shuffle.select(range(cv_test_lines*2,cv_shuffle.num_rows))\n",
    "\n",
    "print('Python-Edu:',pe_train.num_rows,pe_eval.num_rows,pe_test.num_rows)\n",
    "print('FineWeb-Edu-Dedup:',fed_train.num_rows,fed_eval.num_rows,fed_test.num_rows)\n",
    "print('Cosmopedia-V2:',cv_train.num_rows,cv_eval.num_rows,cv_test.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10 = pe_train.filter(lambda x: x['score'] > 4.16, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "fed10 = fed_train.filter(lambda x: x['metadata']['int_score'] >= 4, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "cv2_ratio=(fed10['train'].num_rows/fed_train['train'].num_rows+pe10['train'].num_rows/pe_train['train'].num_rows)/2\n",
    "cv2_samples=int(cv_train.num_rows['train']*cv2_ratio)\n",
    "cv10=cv_train['train'].shuffle(seed=42).select(range(cv2_samples))\n",
    "cv10=dataset_dict.DatasetDict({'train':cv10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus=DatasetDict({'train':pe10['train'],'eval':pe_eval,'test':pe_test})\n",
    "fed10_corpus=DatasetDict({'train':fed10['train'],'eval':fed_eval,'test':fed_test})\n",
    "cv10_corpus=DatasetDict({'train':cv10['train'],'eval':cv_eval,'test':cv_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")\n",
    "pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"fineweb-edu-dedup\")\n",
    "pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Make based on SmolLM Mixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "\n",
    "# login('YOURKEY')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YK\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YK\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu(path,split):\n",
    "    ds = load_dataset(path, \"python-edu\", split=split, num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({split: ds})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "fed=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "cv=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", num_proc=DEFAULT_NUM_PROC//8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed4 = fed.filter(lambda x: x['metadata']['score'] >= 4, num_proc=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "fed4_token_lengths=np.sum(fed4['train']['metadata']['token_count'])\n",
    "# bar=tqdm(fed4['train'])\n",
    "# for i in bar:\n",
    "#     fed4_token_lengths+=i['metadata']['token_count']\n",
    "#     current_count=f'{fed4_token_lengths/1000/1000/1000:.3f}B'\n",
    "#     bar.set_description(current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "def download_contents_py(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=\"softwareheritage\", Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "\n",
    "DEFAULT_NUM_PROC_LOAD=os.cpu_count()*4\n",
    "\n",
    "ds = load_dataset(\"chengjunyan1/smollm-12.5-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUM_PROC_LOAD=os.cpu_count()*8\n",
    "from datasets import DatasetDict    \n",
    "\n",
    "ds_train = ds['train'].map(download_contents_py, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC_LOAD)\n",
    "ds_test = ds['test'].map(download_contents_py, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC_LOAD)\n",
    "ds_eval = ds['eval'].map(download_contents_py, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC_LOAD)\n",
    "ds = DatasetDict({'train':ds_train, 'test':ds_test, 'eval':ds_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ds.push_to_hub(\"chengjunyan1/smollm-12.5-corpus\",\"python-edu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process DeepMind Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "\n",
    "# login('')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"chengjunyan1/smollm-12.5-corpus\", \"deepmind-math\", num_proc=DEFAULT_NUM_PROC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle\n",
    "ds['train']=ds['train'].shuffle(seed=42)\n",
    "ds['test']=ds['test'].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# around 195,167,286\n",
    "rows=50000\n",
    "print(round(sum(ds['test'][:rows]['token_counts'])/1000/1000/1000,5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'token_counts'],\n",
       "        num_rows: 111999888\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'token_counts'],\n",
       "        num_rows: 560000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'token_counts'],\n",
       "        num_rows: 3880000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'token_counts'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['question', 'answer', 'token_counts'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_new=DatasetDict({\n",
    "    'train':ds['train'].select(range(3880000)),\n",
    "    'test':ds['test'].select(range(rows)),\n",
    "    'eval':ds['test'].select(range(rows,rows*2))\n",
    "})\n",
    "ds_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cc899a4548492c9347b20819754f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f3c223f289486db1796810e54a4c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3880 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f37f64cad741889b3ed5d490739514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec75273d4e5b4655a733065328c43122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb78625887524c81ad991dc930b2f9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae86748fd41b4da8ae4caf957822d23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cb32980ba5413bb68548c1811d1921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/chengjunyan1/smollm-12.5-corpus/commit/36a310d5b132f916919bfec88227ce2752905c5e', commit_message='Upload dataset', commit_description='', oid='36a310d5b132f916919bfec88227ce2752905c5e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_new.push_to_hub(\"chengjunyan1/smollm-12.5-corpus\",\"deepmind-math-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process StackOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cfa116a1384aaa8eb6dd8e44951d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cc38b969fb484aa17c7d333d52d9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 512 to 72 for the train split as it only contains 72 shards.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de22894631504231bbf983758ec04cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10404628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9748189a90244a09be5a981b8f3d3470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "\n",
    "# login('')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "\n",
    "ds = load_dataset(\"bigcode/stackoverflow-clean\",num_proc=DEFAULT_NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=ds.shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38889\n"
     ]
    }
   ],
   "source": [
    "n_rows_train=395000\n",
    "\n",
    "ds_train=ds['train'].select(range(n_rows_train))\n",
    "nb_train=round(sum(ds_train['nb_tokens'])/1000/1000/1000,5)\n",
    "print(nb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006\n",
      "0.00601\n",
      "0.40090000000000003\n"
     ]
    }
   ],
   "source": [
    "n_rows_test=6100\n",
    "n_rows_eval=n_rows_test\n",
    "\n",
    "ds_test=ds['train'].select(range(n_rows_train,n_rows_train+n_rows_test))\n",
    "nb_test=round(sum(ds_test['nb_tokens'])/1000/1000/1000,5)\n",
    "print(nb_test)\n",
    "ds_eval=ds['train'].select(range(n_rows_train+n_rows_test,n_rows_train+n_rows_test+n_rows_eval))\n",
    "nb_eval=round(sum(ds_eval['nb_tokens'])/1000/1000/1000,5)\n",
    "print(nb_eval)\n",
    "\n",
    "print(nb_train+nb_test+nb_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742d564229f9448a9db09a718e6ce927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3cc76dd9a843a4b2d32813364a8f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/132 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e159e59d1140a98a3211fb50bf1b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/132 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60b28ce29cc49efabaa14d19274967a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/132 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f7ec4bc14249f99f982cf8d53e2572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6248d99fc24089a5ebe485f7e00f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2066a86c47394ad6aa26235aa4573656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7779e498a66347ffa380b055deaf757c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a4575615804fa9afbfd6b374b3f347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/chengjunyan1/smollm-12.5-corpus/commit/a350b714921de0b87eea04dd43d414563ed910dc', commit_message='Upload dataset', commit_description='', oid='a350b714921de0b87eea04dd43d414563ed910dc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_new=DatasetDict({\n",
    "    'train':ds_train,\n",
    "    'test':ds_test,\n",
    "    'eval':ds_eval\n",
    "})\n",
    "ds_new.push_to_hub(\"chengjunyan1/smollm-12.5-corpus\",\"stackoverflow-clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnose Cosmopedia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d9b17dbbe64a19a065e8d66cd2211c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3e63486f3c4ea4a3713a8314304f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abb203b1b3f42a7945b61ad9d958552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb15e49a84ca44dea4d91c591a8e43c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/69 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef694f6fdfb4996a634a8e602f26894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a585e0236fc456d93936781746221fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c83b24156124530a3814ce64b3f9822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6315863 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc5e5e0b84f4535974fa8cc6320023b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split:   0%|          | 0/78715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a456f603284f3dbf5ee4a43121920c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/78715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4c7c1539e5421ebc359ad94c5c4216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "\n",
    "# login('')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "\n",
    "ds = load_dataset(\"chengjunyan1/smollm-12.5-corpus\",\"cosmopedia-v2\",num_proc=DEFAULT_NUM_PROC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1760940b3b460189aee9f901069456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=512):   0%|          | 0/6315863 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_text=ds['train'].map(lambda x: {'text':f\"Audience: {x['audience']}\\n\\nPrompt: {x['prompt']}\\n\\nResponse: {x['text']}\"},num_proc=DEFAULT_NUM_PROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test loading all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'anthropic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_discovery\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgam_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ( \n\u001b[1;32m      5\u001b[0m     GAMConfig,GAMConfig_14M,GAMConfig_31M,GAMConfig_70M,GAMConfig_125M,GAMConfig_350M,GAMConfig_760M,\n\u001b[1;32m      6\u001b[0m     GAMConfig_1300M,GAMConfig_2700M,GAMConfig_6700M,GAMConfig_13B,GAMConfig_175B,GAMConfig_1T,GAMConfig_debug\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_discovery\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mve\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_datasets\n\u001b[1;32m     12\u001b[0m cfg\u001b[38;5;241m=\u001b[39mGAMConfig_14M\n",
      "File \u001b[0;32m/media/8TBNVME/home/junyanc/model_discovery/notebooks/../model_discovery/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevolution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msystem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/media/8TBNVME/home/junyanc/model_discovery/notebooks/../model_discovery/evolution.py:48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnetwork\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Network\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_discovery\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlibrary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtester\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_tune\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModuleType\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m     Type,\n\u001b[1;32m     54\u001b[0m     List,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     Union\n\u001b[1;32m     61\u001b[0m )\n",
      "File \u001b[0;32m/media/8TBNVME/home/junyanc/model_discovery/notebooks/../model_discovery/model/library/tester.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_discovery\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroles\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchecker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_discovery\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlibrary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MODEL2CODE\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_discovery\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgam_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ( \n\u001b[1;32m      9\u001b[0m     GAMConfig,GAMConfig_14M,GAMConfig_31M,GAMConfig_70M,GAMConfig_125M,GAMConfig_350M,GAMConfig_760M,\n\u001b[1;32m     10\u001b[0m     GAMConfig_1300M,GAMConfig_2700M,GAMConfig_6700M,GAMConfig_13B,GAMConfig_175B,GAMConfig_1T,GAMConfig_debug\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m/media/8TBNVME/home/junyanc/model_discovery/notebooks/../model_discovery/agents/roles/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from .reviewer import *\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdesigner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchecker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclaude\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/media/8TBNVME/home/junyanc/model_discovery/notebooks/../model_discovery/agents/roles/designer.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mexec_utils\u001b[39;00m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structured__call__,ModelOutputPlus\n\u001b[1;32m     16\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDesignerAgent\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m ]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@exec_utils\u001b[39m\u001b[38;5;241m.\u001b[39mRegistry(\n\u001b[1;32m     21\u001b[0m     resource_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_model_type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesigner_agent\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDesignerAgent\u001b[39;00m(exec_utils\u001b[38;5;241m.\u001b[39mSimpleLMAgent):\n",
      "File \u001b[0;32m/media/8TBNVME/home/junyanc/model_discovery/notebooks/../model_discovery/agents/agent_utils.py:244\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ModelOutputPlus(\n\u001b[1;32m    228\u001b[0m         text\u001b[38;5;241m=\u001b[39moutput,\n\u001b[1;32m    229\u001b[0m         cost\u001b[38;5;241m=\u001b[39mcost,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m         usage\u001b[38;5;241m=\u001b[39musage\n\u001b[1;32m    234\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m#################################################################################################################################\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m## Patch for Claude Agent and possibly structured outputs with LangChain\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m## https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/#anthropic \u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m#################################################################################################################################\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01manthropic\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_anthropic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatAnthropic\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage, SystemMessage, AIMessage\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'anthropic'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from model_discovery.configs.gam_config import ( \n",
    "    GAMConfig,GAMConfig_14M,GAMConfig_31M,GAMConfig_70M,GAMConfig_125M,GAMConfig_350M,GAMConfig_760M,\n",
    "    GAMConfig_1300M,GAMConfig_2700M,GAMConfig_6700M,GAMConfig_13B,GAMConfig_175B,GAMConfig_1T,GAMConfig_debug\n",
    ")\n",
    "\n",
    "from model_discovery.ve.data_loader import load_datasets\n",
    "\n",
    "\n",
    "cfg=GAMConfig_14M()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
