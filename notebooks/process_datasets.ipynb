{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process SmolLM Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import boto3\n",
    "import gzip,os\n",
    "\n",
    "# cv2 = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\")\n",
    "# fed = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\")\n",
    "# pe = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\")\n",
    "\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed4 = fed.filter(lambda x: x['metadata']['int_score'] >= 4, num_proc=120)\n",
    "# fed4=load_dataset(\"chengjunyan1/smollm-10\", \"fineweb-edu-dedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe4 = pe.filter(lambda x: x['score'] > 4.16, num_proc=120)\n",
    "pe4['train'].num_rows,pe4['train'].num_rows/pe['train'].num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import dataset_dict\n",
    "\n",
    "cv2_ratio=(fed4['train'].num_rows/fed['train'].num_rows+pe4['train'].num_rows/pe['train'].num_rows)/2\n",
    "cv2_samples=int(cv2.num_rows['train']*cv2_ratio)\n",
    "cv2_10=cv2['train'].shuffle(seed=42).select(range(cv2_samples))\n",
    "cv2_10=dataset_dict.DatasetDict({'train':cv2_10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_all=fed4['train'].num_rows+pe4['train'].num_rows+cv2_10['train'].num_rows\n",
    "len_all_full=fed['train'].num_rows+pe['train'].num_rows+cv2['train'].num_rows\n",
    "print(fed4['train'].num_rows/len_all, pe4['train'].num_rows/len_all, cv2_10['train'].num_rows/len_all)\n",
    "print(fed['train'].num_rows/len_all_full, pe['train'].num_rows/len_all_full, cv2['train'].num_rows/len_all_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe4.push_to_hub(\"chengjunyan1/smollm-10\",\"python-edu\")\n",
    "# fed4.push_to_hub(\"chengjunyan1/smollm-10\",\"fineweb-edu-dedup\")\n",
    "# cv2_10.push_to_hub(\"chengjunyan1/smollm-10\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "import functools as ft\n",
    "\n",
    "\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed10=load_dataset(\"chengjunyan1/smollm-10\",\"fineweb-edu-dedup\")\n",
    "cv10=load_dataset(\"chengjunyan1/smollm-10\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu():\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10\", \"python-edu\", split=\"train\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds\n",
    "\n",
    "pe10=download_python_edu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets,DatasetDict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def resample_dataset(dataset, weight):\n",
    "    num_samples = int(len(dataset) * weight)\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=True)\n",
    "    return dataset.select(indices)\n",
    "\n",
    "def combine_datasets(dataset_dicts, weights:dict=None): # weights e.g. {'train':[1.5,1.0]}\n",
    "    combined_dict = {}\n",
    "    \n",
    "    # Initialize weights if not provided\n",
    "    for dataset_dict in dataset_dicts:\n",
    "        for key, dataset in dataset_dict.items():\n",
    "            if key in combined_dict:\n",
    "                combined_dict[key] = concatenate_datasets([combined_dict[key], dataset])\n",
    "            else:\n",
    "                combined_dict[key] = dataset\n",
    "\n",
    "    # Apply weights by resampling the datasets\n",
    "    for key in combined_dict:\n",
    "        datasets = []\n",
    "        for idx,dataset in enumerate(dataset_dicts):\n",
    "            if key in dataset:\n",
    "                if weights is None or key not in weights or weights[key][idx] == 1.0:\n",
    "                    datasets.append(dataset[key])\n",
    "                else:\n",
    "                    resampled_dataset = resample_dataset(dataset[key], weights[key][idx])\n",
    "                    datasets.append(resampled_dataset)\n",
    "        combined_dict[key] = concatenate_datasets(datasets)\n",
    "    \n",
    "    return DatasetDict(combined_dict)\n",
    "\n",
    "\n",
    "dataset_dicts=[pe10,fed10,cv10]\n",
    "combined=combine_datasets(dataset_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test split based on the processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# login('YOURKEY')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu():\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10\", \"python-edu\", split=\"train\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6642c6ad6fa242b2adf312e2a62467dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f801fafe98aa47289e41a119c90c564f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1937598afd4b47b61192d54e04e0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9586ff82201f41d8be45d0b4e8011839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/1934 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1229ef96258420ab8d7247ce46b661c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1745b94221247c28f2377c3ade62dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223ecd3ea530479da6e11531f0f2a440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pe=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "fed=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "cv=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "# fed10=load_dataset(\"chengjunyan1/smollm-10\",\"fineweb-edu-dedup\",num_proc=DEFAULT_NUM_PROC//8)\n",
    "# cv10=load_dataset(\"chengjunyan1/smollm-10\",\"cosmopedia-v2\",num_proc=DEFAULT_NUM_PROC//8)\n",
    "# pe10=download_python_edu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample 2 GB of data from each dataset\n",
    "smollm_size=673\n",
    "test_ratio=1/smollm_size\n",
    "pe_test_lines=int(pe['train'].num_rows*test_ratio)\n",
    "fed_test_lines=int(fed['train'].num_rows*test_ratio)\n",
    "cv_test_lines=int(cv['train'].num_rows*test_ratio)\n",
    "pe_shuffle=pe['train'].shuffle(seed=42)\n",
    "fed_shuffle=fed['train'].shuffle(seed=42)\n",
    "cv_shuffle=cv['train'].shuffle(seed=42)\n",
    "pe_test=pe_shuffle.select(range(pe_test_lines))\n",
    "pe_eval=pe_shuffle.select(range(pe_test_lines,pe_test_lines*2))\n",
    "pe_test=pe_test.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "pe_eval=pe_eval.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "fed_test=fed_shuffle.select(range(fed_test_lines))\n",
    "fed_eval=fed_shuffle.select(range(fed_test_lines,fed_test_lines*2))\n",
    "cv_test=cv_shuffle.select(range(cv_test_lines))\n",
    "cv_eval=cv_shuffle.select(range(cv_test_lines,cv_test_lines*2))\n",
    "\n",
    "print('Python-Edu:',pe10.num_rows,pe_eval.num_rows,pe_test.num_rows)\n",
    "print('FineWeb-Edu-Dedup:',fed10.num_rows,fed_eval.num_rows,fed_test.num_rows)\n",
    "print('Cosmopedia-V2:',cv10.num_rows,cv_eval.num_rows,cv_test.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_test,pe_eval,fed_test,fed_eval,cv_test,cv_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove verbatims\n",
    "import multiprocessing as mp\n",
    "def get_text(x):\n",
    "    return x['text']\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    pe_text=pool.map(get_text, pe_test)+pool.map(get_text, pe_eval)\n",
    "    fed_text=pool.map(get_text, fed_test)+pool.map(get_text, fed_eval)\n",
    "    cv_text=pool.map(get_text, cv_test)+pool.map(get_text, cv_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_train=pe10.filter(lambda x: x['text'] not in pe_text, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "cv_train=cv10.filter(lambda x: x['text'] not in cv_text, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "fed_train=fed10.filter(lambda x: x['text'] not in fed_text, num_proc=DEFAULT_NUM_PROC//4-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pe_train['train'].num_rows,pe10['train'].num_rows,pe_train['train'].num_rows/pe10['train'].num_rows)\n",
    "print(cv_train['train'].num_rows,cv10['train'].num_rows,cv_train['train'].num_rows/cv10['train'].num_rows)\n",
    "print(fed_train['train'].num_rows,fed10['train'].num_rows,fed_train['train'].num_rows/fed10['train'].num_rows)\n",
    "total_rows_train=pe_train['train'].num_rows+cv_train['train'].num_rows+fed_train['train'].num_rows\n",
    "total_rows=pe10['train'].num_rows+cv10['train'].num_rows+fed10['train'].num_rows\n",
    "print(pe_train['train'].num_rows/total_rows_train,cv_train['train'].num_rows/total_rows_train,fed_train['train'].num_rows/total_rows_train)\n",
    "print(pe10['train'].num_rows/total_rows,cv10['train'].num_rows/total_rows,fed10['train'].num_rows/total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus=DatasetDict({'train':pe10['train'],'eval':pe_eval,'test':pe_test})\n",
    "fed10_corpus=DatasetDict({'train':fed10['train'],'eval':fed_eval,'test':fed_test})\n",
    "cv10_corpus=DatasetDict({'train':cv10['train'],'eval':cv_eval,'test':cv_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus,fed10_corpus,cv10_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")\n",
    "# fed10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"fineweb-edu-dedup\")\n",
    "# cv10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the datasets\n",
    "from datasets import load_dataset\n",
    "pe10_corpus=load_dataset(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")\n",
    "fed10_corpus=load_dataset(\"chengjunyan1/smollm-10-corpus\",\"fineweb-edu-dedup\")\n",
    "cv10_corpus=load_dataset(\"chengjunyan1/smollm-10-corpus\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus,fed10_corpus,cv10_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f7be79bf3f474a94687172a3504722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function download_contents at 0x7f92d81e19e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d5653102f74e5882e28cb5a4125295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=512):   0%|          | 0/864074 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0c7a0aee3349aea43b99d8bdff8151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d686bfb94b4e60ab424d1882aa0116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=512):   0%|          | 0/11409 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329b6f1633b0418e915a78e283124267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cccf930f9249c8a99dbcaee63f5ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=512):   0%|          | 0/11409 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def download_python_edu(split):\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10-corpus\", \"python-edu\", split=split, num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds\n",
    "\n",
    "pe10_train=download_python_edu(\"train\")\n",
    "pe10_eval=download_python_edu(\"eval\")\n",
    "pe10_test=download_python_edu(\"test\")\n",
    "pe10_corpus=DatasetDict({'train':pe10_train['train'],'eval':pe10_eval['train'],'test':pe10_test['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting train\n",
      "0.513 B\n",
      "Counting eval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4190cdb347ca43bba1d9776607639e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/11409 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006 B\n",
      "Counting test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ea0cbae0fd4679b4ee44594df04067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/11409 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007 B\n"
     ]
    }
   ],
   "source": [
    "# pe10_corpus,fed10_corpus,cv10_corpus\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "\n",
    "\n",
    "splits = ['train','eval','test']\n",
    "\n",
    "# for split in splits:\n",
    "#     print('Counting',split)\n",
    "#     fed10_token_lengths=np.sum(fed10_corpus[split]['metadata']['token_count'])\n",
    "#     print(f'{fed10_token_lengths/1000/1000/1000:.2f}','B')\n",
    "\n",
    "# for split in splits:\n",
    "#     print('Counting',split)\n",
    "#     cv10_token_lengths=np.sum(cv10_corpus[split]['token_length'])\n",
    "#     print(f'{cv10_token_lengths/1000/1000/1000:.2f}','B')\n",
    "\n",
    "for split in splits:\n",
    "    print('Counting',split)\n",
    "    pe10_length=pe10_corpus[split].map(lambda x: {'token_length':tokenizer(x['text'],return_tensors='pt')['input_ids'].shape[1]},num_proc=cpu_count())\n",
    "    pe10_token_lengths=np.sum(pe10_length['token_length'])\n",
    "    print(f'{pe10_token_lengths/1000/1000/1000:.3f}','B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove text column from pe10\n",
    "\n",
    "# pe10_corpus['train']=pe10_corpus['train'].remove_columns('text')\n",
    "# pe10_corpus['eval']=pe10_corpus['eval'].remove_columns('text')\n",
    "# pe10_corpus['test']=pe10_corpus['test'].remove_columns('text')\n",
    "\n",
    "# pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make train and test splits from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "from huggingface_hub import login\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YOURKEY\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YOURKEY\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu():\n",
    "    ds = load_dataset(\"chengjunyan1/smollm-10\", \"python-edu\", split=\"train\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({\"train\": ds})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "fed=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "cv=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", num_proc=DEFAULT_NUM_PROC//8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample 2 GB of data from each dataset\n",
    "smollm_size=673\n",
    "test_ratio=1/smollm_size\n",
    "pe_test_lines=int(pe['train'].num_rows*test_ratio)\n",
    "fed_test_lines=int(fed['train'].num_rows*test_ratio)\n",
    "cv_test_lines=int(cv['train'].num_rows*test_ratio)\n",
    "pe_shuffle=pe['train'].shuffle(seed=42)\n",
    "fed_shuffle=fed['train'].shuffle(seed=42)\n",
    "cv_shuffle=cv['train'].shuffle(seed=42)\n",
    "pe_test=pe_shuffle.select(range(pe_test_lines))\n",
    "pe_eval=pe_shuffle.select(range(pe_test_lines,pe_test_lines*2))\n",
    "pe_train=pe_shuffle.select(range(pe_test_lines*2,pe_shuffle.num_rows))\n",
    "fed_test=fed_shuffle.select(range(fed_test_lines))\n",
    "fed_eval=fed_shuffle.select(range(fed_test_lines,fed_test_lines*2))\n",
    "fed_train=fed_shuffle.select(range(fed_test_lines*2,fed_shuffle.num_rows))\n",
    "cv_test=cv_shuffle.select(range(cv_test_lines))\n",
    "cv_eval=cv_shuffle.select(range(cv_test_lines,cv_test_lines*2))\n",
    "cv_train=cv_shuffle.select(range(cv_test_lines*2,cv_shuffle.num_rows))\n",
    "\n",
    "print('Python-Edu:',pe_train.num_rows,pe_eval.num_rows,pe_test.num_rows)\n",
    "print('FineWeb-Edu-Dedup:',fed_train.num_rows,fed_eval.num_rows,fed_test.num_rows)\n",
    "print('Cosmopedia-V2:',cv_train.num_rows,cv_eval.num_rows,cv_test.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10 = pe_train.filter(lambda x: x['score'] > 4.16, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "fed10 = fed_train.filter(lambda x: x['metadata']['int_score'] >= 4, num_proc=DEFAULT_NUM_PROC//4-8)\n",
    "cv2_ratio=(fed10['train'].num_rows/fed_train['train'].num_rows+pe10['train'].num_rows/pe_train['train'].num_rows)/2\n",
    "cv2_samples=int(cv_train.num_rows['train']*cv2_ratio)\n",
    "cv10=cv_train['train'].shuffle(seed=42).select(range(cv2_samples))\n",
    "cv10=dataset_dict.DatasetDict({'train':cv10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus=DatasetDict({'train':pe10['train'],'eval':pe_eval,'test':pe_test})\n",
    "fed10_corpus=DatasetDict({'train':fed10['train'],'eval':fed_eval,'test':fed_test})\n",
    "cv10_corpus=DatasetDict({'train':cv10['train'],'eval':cv_eval,'test':cv_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"python-edu\")\n",
    "pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"fineweb-edu-dedup\")\n",
    "pe10_corpus.push_to_hub(\"chengjunyan1/smollm-10-corpus\",\"cosmopedia-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Make based on SmolLM Mixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import boto3\n",
    "import gzip,os\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "\n",
    "# login('YOURKEY')\n",
    "\n",
    "DEFAULT_NUM_PROC =  os.cpu_count()*4 # Configure it based on your system, it can significantly speed up the download\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"YK\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"YK\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "s3 = session.client(\"s3\")\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "        content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return {\"text\": content}\n",
    "\n",
    "def download_python_edu(path,split):\n",
    "    ds = load_dataset(path, \"python-edu\", split=split, num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=DEFAULT_NUM_PROC)\n",
    "    ds = DatasetDict({split: ds})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c55305f5864795b6315e675da0c786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff37e0abedd4ed4a3fd8831cc9bccba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f0fdf6b6e7426ea1bf4cdb751e8f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a801ddae86374e249934241d368deeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/1934 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aebfbe6a1c04563bd5620dec40ac0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9cb6fb6b744ef2b6c185a71178c855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1311e9ad35414ceea4e649621a2b536b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pe=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "fed=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\", num_proc=DEFAULT_NUM_PROC//8)\n",
    "cv=load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", num_proc=DEFAULT_NUM_PROC//8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed4 = fed.filter(lambda x: x['metadata']['score'] >= 4, num_proc=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'id', 'metadata'],\n",
       "        num_rows: 3564328\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.015B:   0%|          | 14679/3564328 [00:08<33:08, 1785.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m fed4_token_lengths\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mi[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m current_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfed4_token_lengths\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_description\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_count\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/modis/lib/python3.12/site-packages/tqdm/std.py:1394\u001b[0m, in \u001b[0;36mtqdm.set_description\u001b[0;34m(self, desc, refresh)\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesc \u001b[38;5;241m=\u001b[39m desc \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m desc \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refresh:\n\u001b[0;32m-> 1394\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/modis/lib/python3.12/site-packages/tqdm/std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m-> 1347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/modis/lib/python3.12/site-packages/tqdm/std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> 1495\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[0;32m~/.conda/envs/modis/lib/python3.12/site-packages/tqdm/std.py:459\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[1;32m    458\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[0;32m--> 459\u001b[0m     \u001b[43mfp_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_len\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlen_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[0;32m~/.conda/envs/modis/lib/python3.12/site-packages/tqdm/std.py:453\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[1;32m    452\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[0;32m--> 453\u001b[0m     \u001b[43mfp_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/modis/lib/python3.12/site-packages/tqdm/utils.py:196\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/modis/lib/python3.12/site-packages/ipykernel/iostream.py:609\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/modis/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.conda/envs/modis/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "fed4_token_lengths=np.sum(fed4['train']['metadata']['token_count'])\n",
    "# bar=tqdm(fed4['train'])\n",
    "# for i in bar:\n",
    "#     fed4_token_lengths+=i['metadata']['token_count']\n",
    "#     current_count=f'{fed4_token_lengths/1000/1000/1000:.3f}B'\n",
    "#     bar.set_description(current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process DeepMind Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
