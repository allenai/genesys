def forward(self,
    hidden_states: Tensor,
    position_ids: Tensor) -> Tensor:
  o_proj = self.o_proj
  post_norm = self.post_norm
  ttt_norm_bias = self.ttt_norm_bias
  ttt_norm_weight = self.ttt_norm_weight
  b1 = self.b1
  W1 = self.W1
  learnable_token_idx = self.learnable_token_idx
  token_idx = self.token_idx
  learnable_ttt_lr_bias = self.learnable_ttt_lr_bias
  learnable_ttt_lr_weight = self.learnable_ttt_lr_weight
  rotary_emb = self.rotary_emb
  v_proj = self.v_proj
  k_proj = self.k_proj
  q_proj = self.q_proj
  B = ops.prim.NumToTensor(torch.size(hidden_states, 0))
  _0 = int(B)
  _1 = int(B)
  _2 = int(B)
  L = ops.prim.NumToTensor(torch.size(hidden_states, 1))
  _3 = int(L)
  _4 = int(L)
  _5 = int(L)
  reminder_len = torch.remainder(L, 16)
  _6 = int(reminder_len)
  _7 = int(reminder_len)
  _8 = int(reminder_len)
  _9 = int(reminder_len)
  _10 = int(reminder_len)
  _11 = int(reminder_len)
  _12 = int(reminder_len)
  _13 = int(reminder_len)
  num_mini_batch = torch.floor_divide(L, CONSTANTS.c0)
  _14 = (q_proj).forward(hidden_states, )
  _15 = (k_proj).forward(hidden_states, )
  _16 = (v_proj).forward(hidden_states, )
  q = torch.transpose(torch.reshape(_14, [_2, _5, 4, 32]), 1, 2)
  k = torch.transpose(torch.reshape(_15, [_1, _4, 4, 32]), 1, 2)
  XV = torch.transpose(torch.reshape(_16, [_0, _3, 4, 32]), 1, 2)
  position_ids0 = torch.remainder(position_ids, 16)
  _17, _18, = (rotary_emb).forward(position_ids0, )
  bsz = ops.prim.NumToTensor(torch.size(q, 0))
  _19 = int(bsz)
  _20 = int(bsz)
  _21 = int(bsz)
  _22 = int(bsz)
  num_head = ops.prim.NumToTensor(torch.size(q, 1))
  _23 = int(num_head)
  _24 = int(num_head)
  _25 = int(num_head)
  _26 = int(num_head)
  seq_len = ops.prim.NumToTensor(torch.size(q, 2))
  _27 = int(seq_len)
  _28 = int(seq_len)
  _29 = int(seq_len)
  _30 = int(seq_len)
  head_dim = ops.prim.NumToTensor(torch.size(q, 3))
  _31 = int(head_dim)
  _32 = int(head_dim)
  _33 = torch.floor_divide(head_dim, CONSTANTS.c1)
  _34 = torch.reshape(q, [_22, _26, _30, int(_33), 2])
  q0 = torch.reshape(torch.transpose(_34, 3, 4), [_21, _25, _29, _32])
  _35 = torch.floor_divide(head_dim, CONSTANTS.c1)
  _36 = torch.reshape(k, [_20, _24, _28, int(_35), 2])
  k0 = torch.reshape(torch.transpose(_36, 3, 4), [_19, _23, _27, _31])
  cos = torch.unsqueeze(_17, 1)
  sin = torch.unsqueeze(_18, 1)
  _37 = torch.mul(q0, cos)
  _38 = ops.prim.NumToTensor(torch.size(q0, 3))
  _39 = int(torch.floor_divide(_38, CONSTANTS.c1))
  x1 = torch.slice(q0, 3, 0, _39)
  _40 = ops.prim.NumToTensor(torch.size(q0, 3))
  _41 = int(torch.floor_divide(_40, CONSTANTS.c1))
  x2 = torch.slice(q0, 3, _41, 9223372036854775807)
  _42 = torch.mul(torch.cat([torch.neg(x2), x1], -1), sin)
  q1 = torch.add(_37, _42)
  _43 = torch.mul(k0, cos)
  _44 = ops.prim.NumToTensor(torch.size(k0, 3))
  _45 = int(torch.floor_divide(_44, CONSTANTS.c1))
  x10 = torch.slice(k0, 3, 0, _45)
  _46 = ops.prim.NumToTensor(torch.size(k0, 3))
  _47 = int(torch.floor_divide(_46, CONSTANTS.c1))
  x20 = torch.slice(k0, 3, _47, 9223372036854775807)
  _48 = torch.mul(torch.cat([torch.neg(x20), x10], -1), sin)
  k1 = torch.add(_43, _48)
  bsz0 = ops.prim.NumToTensor(torch.size(q1, 0))
  _49 = int(bsz0)
  _50 = int(bsz0)
  _51 = int(bsz0)
  _52 = int(bsz0)
  num_head0 = ops.prim.NumToTensor(torch.size(q1, 1))
  _53 = int(num_head0)
  _54 = int(num_head0)
  _55 = int(num_head0)
  _56 = int(num_head0)
  seq_len0 = ops.prim.NumToTensor(torch.size(q1, 2))
  _57 = int(seq_len0)
  _58 = int(seq_len0)
  _59 = int(seq_len0)
  _60 = int(seq_len0)
  head_dim0 = ops.prim.NumToTensor(torch.size(q1, 3))
  _61 = int(head_dim0)
  _62 = int(head_dim0)
  _63 = torch.floor_divide(head_dim0, CONSTANTS.c1)
  _64 = torch.reshape(q1, [_52, _56, _60, 2, int(_63)])
  XQ = torch.reshape(torch.transpose(_64, 3, 4), [_51, _55, _59, _62])
  _65 = torch.floor_divide(head_dim0, CONSTANTS.c1)
  _66 = torch.reshape(k1, [_50, _54, _58, 2, int(_65)])
  XK = torch.reshape(torch.transpose(_66, 3, 4), [_49, _53, _57, _61])
  _67 = torch.mul(num_mini_batch, CONSTANTS.c0)
  _68 = int(_67)
  _69 = torch.slice(XQ, 0, 0, 9223372036854775807)
  _70 = torch.slice(_69, 1, 0, 9223372036854775807)
  XQ0 = torch.slice(_70, 2, 0, _68)
  _71 = torch.mul(num_mini_batch, CONSTANTS.c0)
  _72 = int(_71)
  _73 = torch.slice(XK, 0, 0, 9223372036854775807)
  _74 = torch.slice(_73, 1, 0, 9223372036854775807)
  XK0 = torch.slice(_74, 2, 0, _72)
  _75 = torch.mul(num_mini_batch, CONSTANTS.c0)
  _76 = int(_75)
  _77 = torch.slice(XV, 0, 0, 9223372036854775807)
  _78 = torch.slice(_77, 1, 0, 9223372036854775807)
  XV0 = torch.slice(_78, 2, 0, _76)
  _79 = torch.mul(num_mini_batch, CONSTANTS.c0)
  _80 = int(_79)
  _81 = torch.slice(hidden_states, 0, 0, 9223372036854775807)
  X = torch.slice(_81, 1, 0, _80)
  B0 = ops.prim.NumToTensor(torch.size(X, 0))
  _82 = int(B0)
  _83 = int(B0)
  _84 = int(B0)
  _85 = int(B0)
  _86 = ops.prim.NumToTensor(torch.size(X, 1))
  num_mini_batch0 = torch.floor_divide(_86, CONSTANTS.c0)
  X0 = torch.reshape(X, [_85, int(num_mini_batch0), 16, 128])
  _87 = int(torch.floor_divide(_86, CONSTANTS.c0))
  x = torch.reshape(XQ0, [_84, 4, _87, 16, 32])
  _88 = int(torch.floor_divide(_86, CONSTANTS.c0))
  x0 = torch.reshape(XK0, [_83, 4, _88, 16, 32])
  _89 = int(torch.floor_divide(_86, CONSTANTS.c0))
  x3 = torch.reshape(XV0, [_82, 4, _89, 16, 32])
  _90 = torch.einsum("bnkc,hdc->bhnkd", [X0, learnable_ttt_lr_weight])
  _91 = torch.reshape(learnable_ttt_lr_bias, [1, -1, 1, 1, 1])
  input = torch.add(_90, _91)
  ttt_lr = torch.sigmoid(input)
  ttt_lr0 = torch.permute(ttt_lr, [0, 1, 2, 4, 3])
  ttt_lr_eta = torch.div(torch.mul(ttt_lr0, CONSTANTS.c2), CONSTANTS.c3)
  token_idx0 = torch.add(token_idx, learnable_token_idx)
  token_idx1 = torch.slice(token_idx0, 0, 0, 16)
  token_idx2 = torch.clamp_min(token_idx1, 0.)
  _92 = torch.reshape(token_idx2, [1, 1, 1, 16, 1])
  _93 = ops.prim.NumToTensor(torch.size(X0, 0))
  _94 = int(_93)
  _95 = ops.prim.NumToTensor(torch.size(X0, 1))
  token_eta = torch.broadcast_to(_92, [_94, 4, int(_95), 16, 1])
  x4 = torch.mul(token_eta, ttt_lr_eta)
  B1 = ops.prim.NumToTensor(torch.size(x3, 0))
  _96 = int(B1)
  _97 = int(B1)
  _98 = int(B1)
  _99 = int(B1)
  num_mini_batch1 = ops.prim.NumToTensor(torch.size(x3, 2))
  _100 = int(num_mini_batch1)
  _101 = ops.prim.NumToTensor(torch.size(x3, 2))
  _102 = ops.prim.NumToTensor(torch.size(x3, 3))
  L0 = torch.mul(_101, _102)
  _103 = int(L0)
  W1_init = torch.tile(torch.unsqueeze(W1, 0), [_99, 1, 1, 1])
  b1_init = torch.tile(torch.unsqueeze(b1, 0), [_98, 1, 1, 1])
  tensor = torch.permute(x, [2, 0, 1, 3, 4])
  tensor0 = torch.permute(x0, [2, 0, 1, 3, 4])
  tensor1 = torch.permute(x3, [2, 0, 1, 3, 4])
  tensor2 = torch.permute(x4, [2, 0, 1, 3, 4])
  out = torch.empty([_100, _97, 4, 16, 32], dtype=6, layout=None, device=torch.device("cpu"), pin_memory=False)
  XQ_mini_batch = torch.select(tensor, 0, 0)
  XK_mini_batch = torch.select(tensor0, 0, 0)
  XV_mini_batch = torch.select(tensor1, 0, 0)
  eta_mini_batch = torch.select(tensor2, 0, 0)
  x5 = torch.add(torch.matmul(XK_mini_batch, W1_init), b1_init)
  l2_target = torch.sub(XV_mini_batch, XK_mini_batch)
  gamma = torch.reshape(ttt_norm_weight, [4, 1, 32])
  beta = torch.reshape(ttt_norm_bias, [4, 1, 32])
  D = ops.prim.NumToTensor(torch.size(x5, 3))
  mu = torch.mean(x5, [-1], True)
  var = torch.var(x5, [-1], False, True)
  std = torch.sqrt(torch.add(var, CONSTANTS.c4))
  x_hat = torch.div(torch.sub(x5, mu), std)
  y = torch.add(torch.mul(gamma, x_hat), beta)
  grad_output = torch.sub(y, l2_target)
  grad_x_hat = torch.mul(grad_output, gamma)
  _104 = torch.mul(torch.reciprocal(D), CONSTANTS.c2)
  _105 = torch.sub(torch.mul(D, grad_x_hat), torch.sum(grad_x_hat, [-1], True))
  _106 = torch.sum(torch.mul(grad_x_hat, x_hat), [-1], True)
  _107 = torch.sub(_105, torch.mul(x_hat, _106))
  grad_l_wrt_Z1 = torch.div(torch.mul(_104, _107), std)
  _108 = torch.transpose(XK_mini_batch, -2, -1)
  Attn1 = torch.tril(torch.matmul(XQ_mini_batch, _108))
  _109 = torch.matmul(torch.tril(eta_mini_batch), grad_l_wrt_Z1)
  b1_bar = torch.sub(b1_init, _109)
  _110 = torch.matmul(XQ_mini_batch, W1_init)
  _111 = torch.matmul(torch.mul(eta_mini_batch, Attn1), grad_l_wrt_Z1)
  x6 = torch.add(torch.sub(_110, _111), b1_bar)
  _112 = torch.slice(eta_mini_batch, 0, 0, 9223372036854775807)
  _113 = torch.slice(_112, 1, 0, 9223372036854775807)
  _114 = torch.slice(torch.select(_113, 2, -1), 2, 0, 9223372036854775807)
  last_eta_mini_batch = torch.unsqueeze(_114, 3)
  _115 = torch.mul(last_eta_mini_batch, XK_mini_batch)
  _116 = torch.matmul(torch.transpose(_115, -1, -2), grad_l_wrt_Z1)
  W1_last = torch.sub(W1_init, _116)
  _117 = torch.mul(last_eta_mini_batch, grad_l_wrt_Z1)
  b1_last = torch.sub(b1_init, torch.sum(_117, [-2], True))
  mu0 = torch.mean(x6, [-1], True)
  var0 = torch.var(x6, [-1], False, True)
  std0 = torch.sqrt(torch.add(var0, CONSTANTS.c4))
  x_hat0 = torch.div(torch.sub(x6, mu0), std0)
  Z1_bar = torch.add(torch.mul(gamma, x_hat0), beta)
  y0 = torch.add(XQ_mini_batch, Z1_bar)
  _118 = torch.copy_(torch.select(out, 0, 0), y0)
  XQ_mini_batch0 = torch.select(tensor, 0, 1)
  XK_mini_batch0 = torch.select(tensor0, 0, 1)
  XV_mini_batch0 = torch.select(tensor1, 0, 1)
  eta_mini_batch0 = torch.select(tensor2, 0, 1)
  x7 = torch.add(torch.matmul(XK_mini_batch0, W1_last), b1_last)
  l2_target0 = torch.sub(XV_mini_batch0, XK_mini_batch0)
  gamma0 = torch.reshape(ttt_norm_weight, [4, 1, 32])
  beta0 = torch.reshape(ttt_norm_bias, [4, 1, 32])
  D0 = ops.prim.NumToTensor(torch.size(x7, 3))
  mu1 = torch.mean(x7, [-1], True)
  var1 = torch.var(x7, [-1], False, True)
  std1 = torch.sqrt(torch.add(var1, CONSTANTS.c4))
  x_hat1 = torch.div(torch.sub(x7, mu1), std1)
  y1 = torch.add(torch.mul(gamma0, x_hat1), beta0)
  grad_output0 = torch.sub(y1, l2_target0)
  grad_x_hat0 = torch.mul(grad_output0, gamma0)
  _119 = torch.mul(torch.reciprocal(D0), CONSTANTS.c2)
  _120 = torch.sub(torch.mul(D0, grad_x_hat0), torch.sum(grad_x_hat0, [-1], True))
  _121 = torch.sum(torch.mul(grad_x_hat0, x_hat1), [-1], True)
  _122 = torch.sub(_120, torch.mul(x_hat1, _121))
  grad_l_wrt_Z10 = torch.div(torch.mul(_119, _122), std1)
  _123 = torch.transpose(XK_mini_batch0, -2, -1)
  Attn10 = torch.tril(torch.matmul(XQ_mini_batch0, _123))
  _124 = torch.matmul(torch.tril(eta_mini_batch0), grad_l_wrt_Z10)
  b1_bar0 = torch.sub(b1_last, _124)
  _125 = torch.matmul(XQ_mini_batch0, W1_last)
  _126 = torch.matmul(torch.mul(eta_mini_batch0, Attn10), grad_l_wrt_Z10)
  x8 = torch.add(torch.sub(_125, _126), b1_bar0)
  _127 = torch.slice(eta_mini_batch0, 0, 0, 9223372036854775807)
  _128 = torch.slice(_127, 1, 0, 9223372036854775807)
  _129 = torch.slice(torch.select(_128, 2, -1), 2, 0, 9223372036854775807)
  last_eta_mini_batch0 = torch.unsqueeze(_129, 3)
  _130 = torch.mul(last_eta_mini_batch0, XK_mini_batch0)
  _131 = torch.matmul(torch.transpose(_130, -1, -2), grad_l_wrt_Z10)
  W1_last0 = torch.sub(W1_last, _131)
  _132 = torch.mul(last_eta_mini_batch0, grad_l_wrt_Z10)
  b1_last0 = torch.sub(b1_last, torch.sum(_132, [-2], True))
  mu2 = torch.mean(x8, [-1], True)
  var2 = torch.var(x8, [-1], False, True)
  std2 = torch.sqrt(torch.add(var2, CONSTANTS.c4))
  x_hat2 = torch.div(torch.sub(x8, mu2), std2)
  Z1_bar0 = torch.add(torch.mul(gamma0, x_hat2), beta0)
  y2 = torch.add(XQ_mini_batch0, Z1_bar0)
  _133 = torch.copy_(torch.select(out, 0, 1), y2)
  XQ_mini_batch1 = torch.select(tensor, 0, 2)
  XK_mini_batch1 = torch.select(tensor0, 0, 2)
  XV_mini_batch1 = torch.select(tensor1, 0, 2)
  eta_mini_batch1 = torch.select(tensor2, 0, 2)
  _134 = torch.matmul(XK_mini_batch1, W1_last0)
  x9 = torch.add(_134, b1_last0)
  l2_target1 = torch.sub(XV_mini_batch1, XK_mini_batch1)
  gamma1 = torch.reshape(ttt_norm_weight, [4, 1, 32])
  beta1 = torch.reshape(ttt_norm_bias, [4, 1, 32])
  D1 = ops.prim.NumToTensor(torch.size(x9, 3))
  mu3 = torch.mean(x9, [-1], True)
  var3 = torch.var(x9, [-1], False, True)
  std3 = torch.sqrt(torch.add(var3, CONSTANTS.c4))
  x_hat3 = torch.div(torch.sub(x9, mu3), std3)
  y3 = torch.add(torch.mul(gamma1, x_hat3), beta1)
  grad_output1 = torch.sub(y3, l2_target1)
  grad_x_hat1 = torch.mul(grad_output1, gamma1)
  _135 = torch.mul(torch.reciprocal(D1), CONSTANTS.c2)
  _136 = torch.sub(torch.mul(D1, grad_x_hat1), torch.sum(grad_x_hat1, [-1], True))
  _137 = torch.sum(torch.mul(grad_x_hat1, x_hat3), [-1], True)
  _138 = torch.sub(_136, torch.mul(x_hat3, _137))
  grad_l_wrt_Z11 = torch.div(torch.mul(_135, _138), std3)
  _139 = torch.transpose(XK_mini_batch1, -2, -1)
  Attn11 = torch.tril(torch.matmul(XQ_mini_batch1, _139))
  _140 = torch.matmul(torch.tril(eta_mini_batch1), grad_l_wrt_Z11)
  b1_bar1 = torch.sub(b1_last0, _140)
  _141 = torch.matmul(XQ_mini_batch1, W1_last0)
  _142 = torch.matmul(torch.mul(eta_mini_batch1, Attn11), grad_l_wrt_Z11)
  x11 = torch.add(torch.sub(_141, _142), b1_bar1)
  _143 = torch.slice(eta_mini_batch1, 0, 0, 9223372036854775807)
  _144 = torch.slice(_143, 1, 0, 9223372036854775807)
  _145 = torch.slice(torch.select(_144, 2, -1), 2, 0, 9223372036854775807)
  last_eta_mini_batch1 = torch.unsqueeze(_145, 3)
  _146 = torch.mul(last_eta_mini_batch1, XK_mini_batch1)
  _147 = torch.matmul(torch.transpose(_146, -1, -2), grad_l_wrt_Z11)
  W1_last1 = torch.sub(W1_last0, _147)
  _148 = torch.mul(last_eta_mini_batch1, grad_l_wrt_Z11)
  b1_last1 = torch.sub(b1_last0, torch.sum(_148, [-2], True))
  mu4 = torch.mean(x11, [-1], True)
  var4 = torch.var(x11, [-1], False, True)
  std4 = torch.sqrt(torch.add(var4, CONSTANTS.c4))
  x_hat4 = torch.div(torch.sub(x11, mu4), std4)
  Z1_bar1 = torch.add(torch.mul(gamma1, x_hat4), beta1)
  y4 = torch.add(XQ_mini_batch1, Z1_bar1)
  _149 = torch.copy_(torch.select(out, 0, 2), y4)
  XQ_mini_batch2 = torch.select(tensor, 0, 3)
  XK_mini_batch2 = torch.select(tensor0, 0, 3)
  XV_mini_batch2 = torch.select(tensor1, 0, 3)
  eta_mini_batch2 = torch.select(tensor2, 0, 3)
  _150 = torch.matmul(XK_mini_batch2, W1_last1)
  x12 = torch.add(_150, b1_last1)
  l2_target2 = torch.sub(XV_mini_batch2, XK_mini_batch2)
  gamma2 = torch.reshape(ttt_norm_weight, [4, 1, 32])
  beta2 = torch.reshape(ttt_norm_bias, [4, 1, 32])
  D2 = ops.prim.NumToTensor(torch.size(x12, 3))
  mu5 = torch.mean(x12, [-1], True)
  var5 = torch.var(x12, [-1], False, True)
  std5 = torch.sqrt(torch.add(var5, CONSTANTS.c4))
  x_hat5 = torch.div(torch.sub(x12, mu5), std5)
  y5 = torch.add(torch.mul(gamma2, x_hat5), beta2)
  grad_output2 = torch.sub(y5, l2_target2)
  grad_x_hat2 = torch.mul(grad_output2, gamma2)
  _151 = torch.mul(torch.reciprocal(D2), CONSTANTS.c2)
  _152 = torch.sub(torch.mul(D2, grad_x_hat2), torch.sum(grad_x_hat2, [-1], True))
  _153 = torch.sum(torch.mul(grad_x_hat2, x_hat5), [-1], True)
  _154 = torch.sub(_152, torch.mul(x_hat5, _153))
  grad_l_wrt_Z12 = torch.div(torch.mul(_151, _154), std5)
  _155 = torch.transpose(XK_mini_batch2, -2, -1)
  Attn12 = torch.tril(torch.matmul(XQ_mini_batch2, _155))
  _156 = torch.matmul(torch.tril(eta_mini_batch2), grad_l_wrt_Z12)
  b1_bar2 = torch.sub(b1_last1, _156)
  _157 = torch.matmul(XQ_mini_batch2, W1_last1)
  _158 = torch.matmul(torch.mul(eta_mini_batch2, Attn12), grad_l_wrt_Z12)
  x13 = torch.add(torch.sub(_157, _158), b1_bar2)
  _159 = torch.slice(eta_mini_batch2, 0, 0, 9223372036854775807)
  _160 = torch.slice(_159, 1, 0, 9223372036854775807)
  _161 = torch.slice(torch.select(_160, 2, -1), 2, 0, 9223372036854775807)
  last_eta_mini_batch2 = torch.unsqueeze(_161, 3)
  _162 = torch.mul(last_eta_mini_batch2, XK_mini_batch2)
  _163 = torch.matmul(torch.transpose(_162, -1, -2), grad_l_wrt_Z12)
  W1_last2 = torch.sub(W1_last1, _163)
  _164 = torch.mul(last_eta_mini_batch2, grad_l_wrt_Z12)
  b1_last2 = torch.sub(b1_last1, torch.sum(_164, [-2], True))
  mu6 = torch.mean(x13, [-1], True)
  var6 = torch.var(x13, [-1], False, True)
  std6 = torch.sqrt(torch.add(var6, CONSTANTS.c4))
  x_hat6 = torch.div(torch.sub(x13, mu6), std6)
  Z1_bar2 = torch.add(torch.mul(gamma2, x_hat6), beta2)
  y6 = torch.add(XQ_mini_batch2, Z1_bar2)
  _165 = torch.copy_(torch.select(out, 0, 3), y6)
  XQ_mini_batch3 = torch.select(tensor, 0, 4)
  XK_mini_batch3 = torch.select(tensor0, 0, 4)
  XV_mini_batch3 = torch.select(tensor1, 0, 4)
  eta_mini_batch3 = torch.select(tensor2, 0, 4)
  _166 = torch.matmul(XK_mini_batch3, W1_last2)
  x14 = torch.add(_166, b1_last2)
  l2_target3 = torch.sub(XV_mini_batch3, XK_mini_batch3)
  gamma3 = torch.reshape(ttt_norm_weight, [4, 1, 32])
  beta3 = torch.reshape(ttt_norm_bias, [4, 1, 32])
  D3 = ops.prim.NumToTensor(torch.size(x14, 3))
  mu7 = torch.mean(x14, [-1], True)
  var7 = torch.var(x14, [-1], False, True)
  std7 = torch.sqrt(torch.add(var7, CONSTANTS.c4))
  x_hat7 = torch.div(torch.sub(x14, mu7), std7)
  y7 = torch.add(torch.mul(gamma3, x_hat7), beta3)
  grad_output3 = torch.sub(y7, l2_target3)
  grad_x_hat3 = torch.mul(grad_output3, gamma3)
  _167 = torch.mul(torch.reciprocal(D3), CONSTANTS.c2)
  _168 = torch.sub(torch.mul(D3, grad_x_hat3), torch.sum(grad_x_hat3, [-1], True))
  _169 = torch.sum(torch.mul(grad_x_hat3, x_hat7), [-1], True)
  _170 = torch.sub(_168, torch.mul(x_hat7, _169))
  grad_l_wrt_Z13 = torch.div(torch.mul(_167, _170), std7)
  _171 = torch.transpose(XK_mini_batch3, -2, -1)
  Attn13 = torch.tril(torch.matmul(XQ_mini_batch3, _171))
  _172 = torch.matmul(torch.tril(eta_mini_batch3), grad_l_wrt_Z13)
  b1_bar3 = torch.sub(b1_last2, _172)
  _173 = torch.matmul(XQ_mini_batch3, W1_last2)
  _174 = torch.matmul(torch.mul(eta_mini_batch3, Attn13), grad_l_wrt_Z13)
  x15 = torch.add(torch.sub(_173, _174), b1_bar3)
  _175 = torch.slice(eta_mini_batch3, 0, 0, 9223372036854775807)
  _176 = torch.slice(_175, 1, 0, 9223372036854775807)
  _177 = torch.slice(torch.select(_176, 2, -1), 2, 0, 9223372036854775807)
  last_eta_mini_batch3 = torch.unsqueeze(_177, 3)
  _178 = torch.mul(last_eta_mini_batch3, XK_mini_batch3)
  _179 = torch.matmul(torch.transpose(_178, -1, -2), grad_l_wrt_Z13)
  W1_last3 = torch.sub(W1_last2, _179)
  _180 = torch.mul(last_eta_mini_batch3, grad_l_wrt_Z13)
  b1_last3 = torch.sub(b1_last2, torch.sum(_180, [-2], True))
  mu8 = torch.mean(x15, [-1], True)
  var8 = torch.var(x15, [-1], False, True)
  std8 = torch.sqrt(torch.add(var8, CONSTANTS.c4))
  x_hat8 = torch.div(torch.sub(x15, mu8), std8)
  Z1_bar3 = torch.add(torch.mul(gamma3, x_hat8), beta3)
  y8 = torch.add(XQ_mini_batch3, Z1_bar3)
  _181 = torch.copy_(torch.select(out, 0, 4), y8)
  XQ_mini_batch4 = torch.select(tensor, 0, 5)
  XK_mini_batch4 = torch.select(tensor0, 0, 5)
  XV_mini_batch4 = torch.select(tensor1, 0, 5)
  eta_mini_batch4 = torch.select(tensor2, 0, 5)
  _182 = torch.matmul(XK_mini_batch4, W1_last3)
  x16 = torch.add(_182, b1_last3)
  l2_target4 = torch.sub(XV_mini_batch4, XK_mini_batch4)
  gamma4 = torch.reshape(ttt_norm_weight, [4, 1, 32])
  beta4 = torch.reshape(ttt_norm_bias, [4, 1, 32])
  D4 = ops.prim.NumToTensor(torch.size(x16, 3))
  mu9 = torch.mean(x16, [-1], True)
  var9 = torch.var(x16, [-1], False, True)
  std9 = torch.sqrt(torch.add(var9, CONSTANTS.c4))
  x_hat9 = torch.div(torch.sub(x16, mu9), std9)
  y9 = torch.add(torch.mul(gamma4, x_hat9), beta4)
  grad_output4 = torch.sub(y9, l2_target4)
  grad_x_hat4 = torch.mul(grad_output4, gamma4)
  _183 = torch.mul(torch.reciprocal(D4), CONSTANTS.c2)
  _184 = torch.sub(torch.mul(D4, grad_x_hat4), torch.sum(grad_x_hat4, [-1], True))
  _185 = torch.sum(torch.mul(grad_x_hat4, x_hat9), [-1], True)
  _186 = torch.sub(_184, torch.mul(x_hat9, _185))
  grad_l_wrt_Z14 = torch.div(torch.mul(_183, _186), std9)
  _187 = torch.transpose(XK_mini_batch4, -2, -1)
  Attn14 = torch.tril(torch.matmul(XQ_mini_batch4, _187))
  _188 = torch.matmul(torch.tril(eta_mini_batch4), grad_l_wrt_Z14)
  b1_bar4 = torch.sub(b1_last3, _188)
  _189 = torch.matmul(XQ_mini_batch4, W1_last3)
  _190 = torch.matmul(torch.mul(eta_mini_batch4, Attn14), grad_l_wrt_Z14)
  x17 = torch.add(torch.sub(_189, _190), b1_bar4)
  _191 = torch.slice(eta_mini_batch4, 0, 0, 9223372036854775807)
  _192 = torch.slice(_191, 1, 0, 9223372036854775807)
  _193 = torch.slice(torch.select(_192, 2, -1), 2, 0, 9223372036854775807)
  last_eta_mini_batch4 = torch.unsqueeze(_193, 3)
  _194 = torch.mul(last_eta_mini_batch4, XK_mini_batch4)
  _195 = torch.matmul(torch.transpose(_194, -1, -2), grad_l_wrt_Z14)
  W1_last4 = torch.sub(W1_last3, _195)
  _196 = torch.mul(last_eta_mini_batch4, grad_l_wrt_Z14)
  b1_last4 = torch.sub(b1_last3, torch.sum(_196, [-2], True))
  mu10 = torch.mean(x17, [-1], True)
  var10 = torch.var(x17, [-1], False, True)
  std10 = torch.sqrt(torch.add(var10, CONSTANTS.c4))
  x_hat10 = torch.div(torch.sub(x17, mu10), std10)
  Z1_bar4 = torch.add(torch.mul(gamma4, x_hat10), beta4)
  y10 = torch.add(XQ_mini_batch4, Z1_bar4)
  _197 = torch.copy_(torch.select(out, 0, 5), y10)
  XQW_batch = torch.permute(out, [1, 0, 3, 2, 4])
  output_mod = torch.reshape(XQW_batch, [_96, _103, 128])
  _198 = int(torch.neg(reminder_len))
  _199 = torch.slice(XQ, 0, 0, 9223372036854775807)
  _200 = torch.slice(_199, 1, 0, 9223372036854775807)
  XQ1 = torch.slice(_200, 2, _198, 9223372036854775807)
  _201 = int(torch.neg(reminder_len))
  _202 = torch.slice(XK, 0, 0, 9223372036854775807)
  _203 = torch.slice(_202, 1, 0, 9223372036854775807)
  XK1 = torch.slice(_203, 2, _201, 9223372036854775807)
  _204 = int(torch.neg(reminder_len))
  _205 = torch.slice(XV, 0, 0, 9223372036854775807)
  _206 = torch.slice(_205, 1, 0, 9223372036854775807)
  XV1 = torch.slice(_206, 2, _204, 9223372036854775807)
  _207 = int(torch.neg(reminder_len))
  _208 = torch.slice(hidden_states, 0, 0, 9223372036854775807)
  X1 = torch.slice(_208, 1, _207, 9223372036854775807)
  B2 = ops.prim.NumToTensor(torch.size(X1, 0))
  _209 = int(B2)
  _210 = int(B2)
  _211 = int(B2)
  _212 = int(B2)
  _213 = ops.prim.NumToTensor(torch.size(X1, 1))
  num_mini_batch2 = torch.floor_divide(_213, reminder_len)
  _214 = [_212, int(num_mini_batch2), _13, 128]
  X2 = torch.reshape(X1, _214)
  _215 = torch.floor_divide(_213, reminder_len)
  x18 = torch.reshape(XQ1, [_211, 4, int(_215), _12, 32])
  _216 = torch.floor_divide(_213, reminder_len)
  x19 = torch.reshape(XK1, [_210, 4, int(_216), _11, 32])
  _217 = torch.floor_divide(_213, reminder_len)
  x21 = torch.reshape(XV1, [_209, 4, int(_217), _10, 32])
  _218 = torch.einsum("bnkc,hdc->bhnkd", [X2, learnable_ttt_lr_weight])
  _219 = torch.reshape(learnable_ttt_lr_bias, [1, -1, 1, 1, 1])
  input0 = torch.add(_218, _219)
  ttt_lr1 = torch.sigmoid(input0)
  ttt_lr2 = torch.permute(ttt_lr1, [0, 1, 2, 4, 3])
  ttt_lr_eta0 = torch.div(torch.mul(ttt_lr2, CONSTANTS.c2), CONSTANTS.c3)
  token_idx3 = torch.add(token_idx, learnable_token_idx)
  token_idx4 = torch.slice(token_idx3, 0, 0, _9)
  token_idx5 = torch.clamp_min(token_idx4, 0.)
  _220 = torch.reshape(token_idx5, [1, 1, 1, _8, 1])
  _221 = ops.prim.NumToTensor(torch.size(X2, 0))
  _222 = int(_221)
  _223 = ops.prim.NumToTensor(torch.size(X2, 1))
  token_eta0 = torch.broadcast_to(_220, [_222, 4, int(_223), _7, 1])
  x22 = torch.mul(token_eta0, ttt_lr_eta0)
  B3 = ops.prim.NumToTensor(torch.size(x21, 0))
  _224 = int(B3)
  _225 = int(B3)
  num_mini_batch3 = ops.prim.NumToTensor(torch.size(x21, 2))
  _226 = int(num_mini_batch3)
  _227 = ops.prim.NumToTensor(torch.size(x21, 2))
  _228 = ops.prim.NumToTensor(torch.size(x21, 3))
  L1 = torch.mul(_227, _228)
  _229 = int(L1)
  tensor3 = torch.permute(x18, [2, 0, 1, 3, 4])
  tensor4 = torch.permute(x19, [2, 0, 1, 3, 4])
  tensor5 = torch.permute(x21, [2, 0, 1, 3, 4])
  tensor6 = torch.permute(x22, [2, 0, 1, 3, 4])
  out0 = torch.empty([_226, _225, 4, _6, 32], dtype=6, layout=None, device=torch.device("cpu"), pin_memory=False)
  XQ_mini_batch5 = torch.select(tensor3, 0, 0)
  XK_mini_batch5 = torch.select(tensor4, 0, 0)
  XV_mini_batch5 = torch.select(tensor5, 0, 0)
  eta_mini_batch5 = torch.select(tensor6, 0, 0)
  _230 = torch.matmul(XK_mini_batch5, W1_last4)
  x23 = torch.add(_230, b1_last4)
  l2_target5 = torch.sub(XV_mini_batch5, XK_mini_batch5)
  gamma5 = torch.reshape(ttt_norm_weight, [4, 1, 32])
  beta5 = torch.reshape(ttt_norm_bias, [4, 1, 32])
  D5 = ops.prim.NumToTensor(torch.size(x23, 3))
  mu11 = torch.mean(x23, [-1], True)
  var11 = torch.var(x23, [-1], False, True)
  std11 = torch.sqrt(torch.add(var11, CONSTANTS.c4))
  x_hat11 = torch.div(torch.sub(x23, mu11), std11)
  y11 = torch.add(torch.mul(gamma5, x_hat11), beta5)
  grad_output5 = torch.sub(y11, l2_target5)
  grad_x_hat5 = torch.mul(grad_output5, gamma5)
  _231 = torch.mul(torch.reciprocal(D5), CONSTANTS.c2)
  _232 = torch.sub(torch.mul(D5, grad_x_hat5), torch.sum(grad_x_hat5, [-1], True))
  _233 = torch.sum(torch.mul(grad_x_hat5, x_hat11), [-1], True)
  _234 = torch.sub(_232, torch.mul(x_hat11, _233))
  grad_l_wrt_Z15 = torch.div(torch.mul(_231, _234), std11)
  _235 = torch.transpose(XK_mini_batch5, -2, -1)
  Attn15 = torch.tril(torch.matmul(XQ_mini_batch5, _235))
  _236 = torch.matmul(torch.tril(eta_mini_batch5), grad_l_wrt_Z15)
  b1_bar5 = torch.sub(b1_last4, _236)
  _237 = torch.matmul(XQ_mini_batch5, W1_last4)
  _238 = torch.matmul(torch.mul(eta_mini_batch5, Attn15), grad_l_wrt_Z15)
  x24 = torch.add(torch.sub(_237, _238), b1_bar5)
  mu12 = torch.mean(x24, [-1], True)
  var12 = torch.var(x24, [-1], False, True)
  std12 = torch.sqrt(torch.add(var12, CONSTANTS.c4))
  x_hat12 = torch.div(torch.sub(x24, mu12), std12)
  Z1_bar5 = torch.add(torch.mul(gamma5, x_hat12), beta5)
  y12 = torch.add(XQ_mini_batch5, Z1_bar5)
  _239 = torch.copy_(torch.select(out0, 0, 0), y12)
  XQW_batch0 = torch.permute(out0, [1, 0, 3, 2, 4])
  output_reminder = torch.reshape(XQW_batch0, [_224, _229, 128])
  input1 = torch.cat([output_mod, output_reminder], 1)
  _240 = (o_proj).forward((post_norm).forward(input1, ), )
  return _240
